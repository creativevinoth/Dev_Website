---
title: Bank_Churn_Data
author: ''
date: '2021-11-30'
slug: bank-churn-data
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><strong>A report to predict the churning of bank customers</strong></p>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<ol style="list-style-type: decimal">
<li><p>Executive Summary</p></li>
<li><p>Issues and Challenges</p></li>
<li><p>Descriptive Statistics and Pre-processing</p></li>
<li><p>Univariate Analysis on Metric Data</p></li>
<li><p>Univariate Analysis on Categorical Values</p></li>
<li><p>Bivariate Analysis</p></li>
<li><p>Data Re-sampling and Feature Engineering</p></li>
<li><p>Logistic Regression Model</p></li>
<li><p>Modelling with the Tidy Model Ecosystem</p></li>
<li><p>Support Vector Machine Model</p></li>
<li><p>Logistic Regression Model (Tidy Workflow Version)</p></li>
<li><p>Random Forest Models</p></li>
<li><p>Creating <em>k</em>-Fold Cross Validation</p></li>
<li><p>Hyper Parameter Tuning</p></li>
<li><p>Best Model Selection with Parameters</p></li>
<li><p>Model Comparison</p></li>
<li><p>Observations and Comments</p></li>
</ol>
</div>
<div id="executive-summary" class="section level1">
<h1>1. Executive summary</h1>
<p>A bank was interested to find out variables that was significant to predict the churning of the customers so as to make sure that customers are satisfied with all aspects and convinced to stay with a bank. The bank was shocked to see that the churning rate of customer have increased dramatically. To stop this churning trend, bank has to first find out the determinants of churning rate and initiate some steps to avoid such highest churning rate in future. It is hard to get a new client, so it is better to retain the existing customers.</p>
<p>There are total 12 independent variable such as <em>RowNumber</em>, <em>Customerid</em>, <em>CreditScore</em>, <em>Geography</em>, <em>Gender</em>, <em>Age</em>, <em>Tenure</em>, <em>Balance</em>, <em>NumOfProucts</em>, <em>HasCrCard</em>, <em>IsActiveMember</em> and <em>EstimatedSalary</em> and 1 dependent variable, <em>Exited</em>. It is clear that variable <em>RowNumber</em> pertains to the counting of the data row wise and <em>CustomerId</em> was to represent the individual customer details. These 2 variables will not have any influence upon the prediction of <em>Exited</em>. Hence, we removed those 2 variables from our analysis. Individual data were analyzed in two categories, namely numerical and categorical data. We found outliers in some of the numerical observations, such as <em>Age</em>, <em>CreditScore</em>, <em>NumOfProducts</em> and removed them using subset function. Categorical variables were analyzed using frequency distribution.</p>
<p>We have used bootstrapping and stepAIC backward regression to find out the best predicting model. Variables <em>Geography</em> and <em>Tenure</em> resulted with non-significant coefficients, and were removed from our model to increase the prediction power and its significance to 99.9%. Our final model turned out to be <em>Gender</em>, <em>Age</em>, <em>Balance</em>, <em>NumOfProducts</em> and <em>IsActiveMember</em> to predict <em>Exited</em> at 99.9% significance level.</p>
<p>We have done data re-sampling using 80:20 split as training and test data. We did feature engineering to preprocess the data. Then, we built different prediction models using Support Vector Machine (SVM), Logistic Regression (LR), and Random Forest (RF). After out-of-sample validation, RF turned out to be the best predicting model. Thus, we did cross validation and hyper parameter tuning on the latter model to further enhance the predicting power. Our best selected RF parameter set was mtry (4), trees (839), and min_n (37).</p>
<p>It was a evident that, from all trained models, around 80% of customers churned. Therefore, the Bank should encourage customers to be actively transacting the account by offering cash back opportunities per transaction. When the customers use the service and get satisfied, they will remain loyal, get more products from the bank, and maintain more balance, which eventually will lead to increase in customers’ retention rate and reduction in customers’ churning rate.</p>
</div>
<div id="issues-and-challenges" class="section level1">
<h1>2. Issues and challenges</h1>
<p>The greatest challenge in this project was to figure out which variables play a role in predicting the response variable.
There were a number of issues. For example, variables <em>Exited</em>, <em>HasCrCard</em> and <em>IsActiveMember</em> were treated automatically as integers although they were categorical values. We converted them to factors before their processing in the model. Also, we noticed that variables like <em>Balance</em> and <em>Salary</em> were high in numbers, whereas others were small. To fix this, we normalized <em>Balance</em> and <em>Salary</em> before processing to get a better prediction. Furthermore, when we hyper tuned and cross validated RF model, the time processing was longer.</p>
</div>
<div id="descriptive-statistics-and-preprocessing" class="section level1">
<h1>3. Descriptive Statistics and Preprocessing</h1>
<div id="a-summary" class="section level4">
<h4>A) Summary</h4>
<pre class="r"><code>summary(Bank_Churn_Data)</code></pre>
<pre><code>##    RowNumber       CustomerId        CreditScore     Geography        
##  Min.   :    1   Min.   :15565701   Min.   :350.0   Length:10000      
##  1st Qu.: 2501   1st Qu.:15628528   1st Qu.:584.0   Class :character  
##  Median : 5000   Median :15690738   Median :652.0   Mode  :character  
##  Mean   : 5000   Mean   :15690941   Mean   :650.5                     
##  3rd Qu.: 7500   3rd Qu.:15753234   3rd Qu.:718.0                     
##  Max.   :10000   Max.   :15815690   Max.   :850.0                     
##     Gender               Age            Tenure          Balance      
##  Length:10000       Min.   :18.00   Min.   : 0.000   Min.   :     0  
##  Class :character   1st Qu.:32.00   1st Qu.: 3.000   1st Qu.:     0  
##  Mode  :character   Median :37.00   Median : 5.000   Median : 97199  
##                     Mean   :38.92   Mean   : 5.013   Mean   : 76486  
##                     3rd Qu.:44.00   3rd Qu.: 7.000   3rd Qu.:127644  
##                     Max.   :92.00   Max.   :10.000   Max.   :250898  
##  NumOfProducts    HasCrCard      IsActiveMember   EstimatedSalary 
##  Min.   :1.00   Min.   :0.0000   Min.   :0.0000   Min.   :    12  
##  1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 51002  
##  Median :1.00   Median :1.0000   Median :1.0000   Median :100194  
##  Mean   :1.53   Mean   :0.7055   Mean   :0.5151   Mean   :100090  
##  3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:149388  
##  Max.   :4.00   Max.   :1.0000   Max.   :1.0000   Max.   :199992  
##      Exited      
##  Min.   :0.0000  
##  1st Qu.:0.0000  
##  Median :0.0000  
##  Mean   :0.2037  
##  3rd Qu.:0.0000  
##  Max.   :1.0000</code></pre>
<p>We removed variables <em>RowNumber</em> and <em>CustomerId</em> as they are related to individuals and will not have influence to predict the churning of customers:</p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, select = - RowNumber)
Bank_Churn_Data &lt;- subset (Bank_Churn_Data, select = - CustomerId)</code></pre>
</div>
<div id="b-structure-of-data" class="section level4">
<h4>B) Structure of data</h4>
<pre class="r"><code>str(Bank_Churn_Data)</code></pre>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  11 variables:
##  $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...
##  $ Geography      : chr  &quot;France&quot; &quot;Spain&quot; &quot;France&quot; &quot;France&quot; ...
##  $ Gender         : chr  &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ...
##  $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...
##  $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...
##  $ Balance        : int  0 83808 159661 0 125511 113756 0 115047 142051 134604 ...
##  $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...
##  $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...
##  $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...
##  $ EstimatedSalary: int  101349 112543 113932 93827 79084 149757 10063 119347 74941 71726 ...
##  $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...</code></pre>
<p>The variables <em>Exited</em>, <em>HasCrCard</em>, and <em>IsActiveMember</em> are categorical values, and hence they were converted to characters.</p>
<p><strong>Changing the variable from int to character</strong></p>
<pre class="r"><code>Bank_Churn_Data$HasCrCard = factor(Bank_Churn_Data$HasCrCard)
Bank_Churn_Data$IsActiveMember = factor(Bank_Churn_Data$IsActiveMember)
Bank_Churn_Data$Exited  = factor(Bank_Churn_Data$Exited )</code></pre>
</div>
<div id="c-descriptive-statistics" class="section level4">
<h4>C) Descriptive statistics</h4>
<pre class="r"><code>descr(Bank_Churn_Data)</code></pre>
<pre><code>## Non-numerical variable(s) ignored: Geography, Gender, HasCrCard, IsActiveMember, Exited</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data  
## N: 10000  
## 
##                          Age     Balance   CreditScore   EstimatedSalary   NumOfProducts     Tenure
## ----------------- ---------- ----------- ------------- ----------------- --------------- ----------
##              Mean      38.92    76485.90        650.53         100090.24            1.53       5.01
##           Std.Dev      10.49    62397.41         96.65          57510.49            0.58       2.89
##               Min      18.00        0.00        350.00             12.00            1.00       0.00
##                Q1      32.00        0.00        584.00          50993.00            1.00       3.00
##            Median      37.00    97198.50        652.00         100193.50            1.00       5.00
##                Q3      44.00   127646.00        718.00         149392.00            2.00       7.00
##               Max      92.00   250898.00        850.00         199992.00            4.00      10.00
##               MAD       8.90    69336.01         99.33          72941.70            0.00       2.97
##               IQR      12.00   127644.00        134.00          98386.00            1.00       4.00
##                CV       0.27        0.82          0.15              0.57            0.38       0.58
##          Skewness       1.01       -0.14         -0.07              0.00            0.75       0.01
##       SE.Skewness       0.02        0.02          0.02              0.02            0.02       0.02
##          Kurtosis       1.39       -1.49         -0.43             -1.18            0.58      -1.17
##           N.Valid   10000.00    10000.00      10000.00          10000.00        10000.00   10000.00
##         Pct.Valid     100.00      100.00        100.00            100.00          100.00     100.00</code></pre>
<p>The above are the descriptive statistics for numerical values. We can find a huge standard deviation for some of the variables such as <em>Balance</em> and <em>EstimatedSalary</em>.</p>
</div>
<div id="d-revised-structure-of-data" class="section level4">
<h4>D) Revised structure of data</h4>
<pre class="r"><code>str(Bank_Churn_Data)</code></pre>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  11 variables:
##  $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...
##  $ Geography      : chr  &quot;France&quot; &quot;Spain&quot; &quot;France&quot; &quot;France&quot; ...
##  $ Gender         : chr  &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ...
##  $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...
##  $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...
##  $ Balance        : int  0 83808 159661 0 125511 113756 0 115047 142051 134604 ...
##  $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...
##  $ HasCrCard      : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 2 2 2 1 2 ...
##  $ IsActiveMember : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 1 1 2 1 2 1 2 2 ...
##  $ EstimatedSalary: int  101349 112543 113932 93827 79084 149757 10063 119347 74941 71726 ...
##  $ Exited         : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 1 2 1 2 1 1 ...</code></pre>
<p>The above are the revised structure of the data after some of the variables have been converted as a factor from the wrongly assigned integer values initially.</p>
</div>
</div>
<div id="univariate-analysis-on-numerical-data" class="section level1">
<h1>4. Univariate Analysis on Numerical Data</h1>
<p>We analyse here all the numerical data individually to find out potential outliers and remove those values.</p>
<div id="a-univariate-analysis-on-age" class="section level4">
<h4>A) Univariate analysis on <em>Age</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$Age)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Age  
## N: 10000  
## 
##                          Age
## ----------------- ----------
##              Mean      38.92
##           Std.Dev      10.49
##               Min      18.00
##                Q1      32.00
##            Median      37.00
##                Q3      44.00
##               Max      92.00
##               MAD       8.90
##               IQR      12.00
##                CV       0.27
##          Skewness       1.01
##       SE.Skewness       0.02
##          Kurtosis       1.39
##           N.Valid   10000.00
##         Pct.Valid     100.00</code></pre>
<p>Variable <em>Age</em> ranges from 18 to 92, with a median of 37. The average is 38, with a standard deviation of 10.49. This variable exhibits a positive skewness of 1.01.</p>
<p><strong>Boxplot of age and its outliers</strong></p>
<pre class="r"><code>Age_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Age)$out
boxplot(Bank_Churn_Data$Age, main = &quot;Box plot of age&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Age_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>Age_outliers_churn</code></pre>
<pre><code>##   [1] 66 75 65 73 65 72 67 67 79 80 68 75 66 66 70 63 72 64 64 70 67 82 63 69 65
##  [26] 69 64 65 74 67 66 67 63 70 71 72 67 74 76 66 63 66 68 67 63 71 66 69 73 65
##  [51] 66 64 69 64 77 74 65 70 67 69 67 74 69 74 74 64 63 63 70 74 65 72 77 66 65
##  [76] 74 88 63 71 63 64 67 70 68 72 71 66 75 67 73 69 76 63 85 67 74 76 66 69 66
## [101] 72 63 71 63 74 67 72 72 66 84 71 66 63 74 69 84 67 64 68 66 77 70 67 79 67
## [126] 76 73 66 67 64 73 76 72 64 71 63 70 65 66 65 80 66 63 63 63 63 66 74 69 63
## [151] 64 76 75 68 69 77 64 66 74 71 67 68 64 68 70 64 75 66 64 78 65 74 64 64 71
## [176] 77 79 70 81 64 68 68 63 79 66 64 70 69 71 72 66 68 63 71 72 72 64 78 75 65
## [201] 65 67 63 68 71 73 64 66 71 69 71 66 76 69 73 64 64 75 73 71 72 63 67 68 73
## [226] 67 64 63 92 65 75 67 71 64 66 64 66 67 77 92 67 63 66 66 68 65 72 71 76 63
## [251] 67 67 66 67 63 65 70 72 77 74 72 73 77 67 71 64 72 81 76 69 68 74 64 64 71
## [276] 68 63 67 63 64 76 63 63 68 67 72 70 81 67 73 66 68 71 66 63 75 69 64 69 70
## [301] 71 71 66 70 63 64 65 63 67 71 67 65 66 63 73 66 64 72 71 69 67 64 81 73 63
## [326] 67 74 83 69 71 78 63 70 69 72 70 63 74 80 69 72 67 76 71 67 71 78 63 63 68
## [351] 64 70 78 69 68 64 64 77 77</code></pre>
<p>The above box plot shows several <em>Age</em> values (&gt; 63) above the the upper quartile, which can be conseidered as outliers. Hence, we removed those values from the dataset</p>
<p><strong>Removal of outliers of age</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, Age &lt; 63)</code></pre>
</div>
<div id="b-univariate-analysis-on-balance" class="section level4">
<h4>B) Univariate analysis on <em>Balance</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$Balance)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Balance  
## N: 9641  
## 
##                       Balance
## ----------------- -----------
##              Mean    76560.68
##           Std.Dev    62401.70
##               Min        0.00
##                Q1        0.00
##            Median    97318.00
##                Q3   127660.00
##               Max   250898.00
##               MAD    69044.68
##               IQR   127660.00
##                CV        0.82
##          Skewness       -0.14
##       SE.Skewness        0.02
##          Kurtosis       -1.49
##           N.Valid     9641.00
##         Pct.Valid      100.00</code></pre>
<p>The variable <em>Balance</em> ranges from $0 to $250,898, with a median of $97,318. Its average is $76,560, with a huge standard deviation of $6,2401. There is a negative skewness of -0.14, which is almost equal to 0.</p>
<p><strong>Boxplot of Balance and its outliers</strong></p>
<pre class="r"><code>Balance_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Balance)$out
boxplot(Bank_Churn_Data$Balance, main = &quot;Box plot of Balance&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Balance_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>Balance_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>No outliers were detected in th boxplot.</p>
</div>
<div id="c-univariate-analysis-on-creditscore" class="section level4">
<h4>C) Univariate analysis on <em>CreditScore</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$CreditScore)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$CreditScore  
## N: 9641  
## 
##                     CreditScore
## ----------------- -------------
##              Mean        650.29
##           Std.Dev         96.69
##               Min        350.00
##                Q1        583.00
##            Median        652.00
##                Q3        717.00
##               Max        850.00
##               MAD         99.33
##               IQR        134.00
##                CV          0.15
##          Skewness         -0.07
##       SE.Skewness          0.02
##          Kurtosis         -0.43
##           N.Valid       9641.00
##         Pct.Valid        100.00</code></pre>
<p>The average of <em>CreditScore</em> is 650, with a standard deviation of 96. It ranges between 350 and 850, with a median of 652. There is a negative skewness of -0.07, which is almost equal to 0.</p>
<p><strong>Boxplot of CreditScore and its outliers</strong></p>
<pre class="r"><code>CreditScore_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$CreditScore)$out
boxplot(Bank_Churn_Data$CreditScore, main = &quot;Box plot of CreditScore&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(CreditScore_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>CreditScore_outliers_churn</code></pre>
<pre><code>##  [1] 376 376 363 359 350 350 358 351 365 367 350 350 373 350</code></pre>
<p>From the above boxplot, it can be noticed that values less than 376 of correspond to outliers. Hence, we removed those values from the data set.</p>
<p><strong>Removal of outliers of CreditScore</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, CreditScore &gt; 376)</code></pre>
</div>
<div id="d-univariate-analysis-on-estimatedsalary" class="section level4">
<h4>D) Univariate analysis on <em>EstimatedSalary</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$EstimatedSalary)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$EstimatedSalary  
## N: 9627  
## 
##                     EstimatedSalary
## ----------------- -----------------
##              Mean         100121.62
##           Std.Dev          57524.28
##               Min             12.00
##                Q1          51011.00
##            Median         100187.00
##                Q3         149458.00
##               Max         199992.00
##               MAD          72964.68
##               IQR          98424.50
##                CV              0.57
##          Skewness              0.00
##       SE.Skewness              0.02
##          Kurtosis             -1.18
##           N.Valid           9627.00
##         Pct.Valid            100.00</code></pre>
<p>The average of <em>EstimatedSalary</em> is $100,121, with a standard deviation of $57524. This variable ranges from $11.58 to $199,002, with a median of $100,187. There is no skewness.</p>
<p><strong>Boxplot of EstimatedSalary and its outliers</strong></p>
<pre class="r"><code>EstimatedSalary_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$EstimatedSalary)$out
boxplot(Bank_Churn_Data$EstimatedSalary, main = &quot;Box plot of EstimatedSalary&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(EstimatedSalary_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>EstimatedSalary_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>No outliers were detected in variable <em>EstimatedSalary</em></p>
</div>
<div id="e-univariate-analysis-on-numofproducts" class="section level4">
<h4>E) Univariate analysis on <em>NumOfProducts</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$NumOfProducts)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$NumOfProducts  
## N: 9627  
## 
##                     NumOfProducts
## ----------------- ---------------
##              Mean            1.53
##           Std.Dev            0.58
##               Min            1.00
##                Q1            1.00
##            Median            1.00
##                Q3            2.00
##               Max            4.00
##               MAD            0.00
##               IQR            1.00
##                CV            0.38
##          Skewness            0.74
##       SE.Skewness            0.02
##          Kurtosis            0.58
##           N.Valid         9627.00
##         Pct.Valid          100.00</code></pre>
<p>The average number of products held by customers is 1.53. There is a standard deviation of 0.58. This variable ranges between 1 and 4, with a median of 1. There is a positive skewness of 0.74.</p>
<p><strong>Boxplot of <em>NumOfProducts</em> and its outliers</strong></p>
<pre class="r"><code>NumOfProducts_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$NumOfProducts)$out
boxplot(Bank_Churn_Data$NumOfProducts, main = &quot;Box plot of NumOfProducts&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(NumOfProducts_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>NumOfProducts_outliers_churn</code></pre>
<pre><code>##  [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
## [39] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4</code></pre>
<p>Observe that there are few people having 4 products and, therefore, can be considered as outliers. We removed them from the data set.</p>
<p><strong>Removal of outliers of NumOfProducts</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, NumOfProducts &lt; 4)</code></pre>
</div>
<div id="f-univariate-analysis-on-tenure" class="section level4">
<h4>F) Univariate analysis on <em>Tenure</em></h4>
<pre class="r"><code>descr(Bank_Churn_Data$Tenure)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Tenure  
## N: 9569  
## 
##                      Tenure
## ----------------- ---------
##              Mean      5.01
##           Std.Dev      2.89
##               Min      0.00
##                Q1      3.00
##            Median      5.00
##                Q3      7.00
##               Max     10.00
##               MAD      2.97
##               IQR      4.00
##                CV      0.58
##          Skewness      0.01
##       SE.Skewness      0.03
##          Kurtosis     -1.16
##           N.Valid   9569.00
##         Pct.Valid    100.00</code></pre>
<p>The variable ranges from 0 to 10, with a median of 5. The average value of <em>Tenure</em> is 5.01 and its standard deviation is 2.89. The skewness is almost 0.</p>
<p><strong>Boxplot of Tenure and its outliers</strong></p>
<pre class="r"><code>Tenure_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Tenure)$out
boxplot(Bank_Churn_Data$Tenure, main = &quot;Box plot of Tenure&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Tenure_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>Tenure_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>No outliers were detected in this variable.</p>
</div>
</div>
<div id="univariate-analysis-on-categorical-values" class="section level1">
<h1>5. Univariate Analysis on Categorical Values</h1>
<p>We analyzed the categorical values using frequency distribution to figure out the distribution among their factors.</p>
<div id="a-frequency-distribution-of-geography" class="section level4">
<h4>A) Frequency distribution of <em>Geography</em></h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Geography, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Geography&quot;, xlab =&quot;Geography&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Geography : 
##         Frequency Percent Cum. percent
## Spain        2373    24.8         24.8
## Germany      2398    25.1         49.9
## France       4798    50.1        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Almost half of the customers (4798) are from France. The remaining customers are equally distributed between Spain and Germany.</p>
</div>
<div id="b-frequency-distribution-of-gender" class="section level4">
<h4>B) Frequency distribution of <em>Gender</em></h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Gender, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Gender&quot;, xlab =&quot;Gender&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Gender : 
##         Frequency Percent Cum. percent
## Female       4332    45.3         45.3
## Male         5237    54.7        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>There more male customers (54%) having account with bank compared to female members.</p>
</div>
<div id="c-frequency-distribution-of-hascrcard" class="section level4">
<h4>C) Frequency distribution of <em>HasCrCard</em></h4>
<pre class="r"><code>tab1(Bank_Churn_Data$HasCrCard, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of HasCrCard&quot;, xlab =&quot;HasCrCard&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$HasCrCard : 
##         Frequency Percent Cum. percent
## 0            2821    29.5         29.5
## 1            6748    70.5        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Around 70% of customers have a credit card with the bank and only 30% of people without credit card.</p>
</div>
<div id="d-frequency-distribution-of-isactivemember" class="section level4">
<h4>D) Frequency distribution of <em>IsActiveMember</em></h4>
<pre class="r"><code>tab1(Bank_Churn_Data$IsActiveMember, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of IsActiveMember&quot;, xlab =&quot;IsActiveMember&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$IsActiveMember : 
##         Frequency Percent Cum. percent
## 0            4749    49.6         49.6
## 1            4820    50.4        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Almost equal number of customers are both active and not active customers in bank.</p>
</div>
<div id="e-frequency-distribution-of-exited" class="section level4">
<h4>E) Frequency distribution of Exited</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Exited , sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Exited &quot;, xlab =&quot;Exited &quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Exited : 
##         Frequency Percent Cum. percent
## 1            1892    19.8         19.8
## 0            7677    80.2        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Notice that, from the above frequency distribution, only 20% of customers remained loyal to the bank, whereas approximately 80% of customers closed the bank account.</p>
</div>
</div>
<div id="bivariate-analysis" class="section level1">
<h1>6. Bivariate Analysis</h1>
<p>Here, we will analyze the relationship between response and explanatory variables.</p>
<div id="a-relationship-between-age-and-exited" class="section level4">
<h4>A) Relationship between <em>Age</em> and <em>Exited</em></h4>
<pre class="r"><code>Age_Exited = glm(Exited~ Age, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Age_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Age, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5677  -0.6369  -0.4780  -0.3213   2.7153  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -5.519707   0.137290  -40.20   &lt;2e-16 ***
## Age          0.103265   0.003233   31.94   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 8334.7  on 9567  degrees of freedom
## AIC: 8338.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>There is a 99.999% significant relationship between <em>Age</em> and response variable <em>Exited</em>.</p>
<pre class="r"><code>discretized.Age = cut(Bank_Churn_Data$Age, c(0, 10, 20, 30, 40, 50, 60,70))
ggplot(Bank_Churn_Data, aes(x = discretized.Age , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>From the above plot, it can be observed that most of the churn customers are between 30 and 40 years old.</p>
</div>
<div id="b-relationship-between-balance-and-exited" class="section level4">
<h4>B) Relationship between <em>Balance</em> and <em>Exited</em></h4>
<pre class="r"><code>Balance_Exited = glm(Exited~ Balance, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Balance_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Balance, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8838  -0.7175  -0.5558  -0.5558   1.9718  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.790e+00  4.535e-02  -39.46   &lt;2e-16 ***
## Balance      4.745e-06  4.260e-07   11.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9387.9  on 9567  degrees of freedom
## AIC: 9391.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.999% significant relationship between <em>Balance</em> and <em>Exited</em>.</p>
<pre class="r"><code>discretized.Balance = cut(Bank_Churn_Data$Balance, c(0, 50000, 100000, 150000, 200000, 250898),dig.lab = 1000)</code></pre>
<pre><code>## Warning in formatC(0 + breaks, digits = dig, width = 1L): &#39;digits&#39; reduced to 50</code></pre>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = discretized.Balance , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="c-relationship-between-creditscore-and-exited" class="section level4">
<h4>C) Relationship between <em>CreditScore</em> and <em>Exited</em></h4>
<pre class="r"><code>CreditScore_Exited = glm(Exited~ CreditScore, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Balance_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Balance, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8838  -0.7175  -0.5558  -0.5558   1.9718  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.790e+00  4.535e-02  -39.46   &lt;2e-16 ***
## Balance      4.745e-06  4.260e-07   11.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9387.9  on 9567  degrees of freedom
## AIC: 9391.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Variable <em>CreditScore</em> is 99.999% significant to predict <em>Exited</em>.</p>
<pre class="r"><code>discretized.CreditScore = cut(Bank_Churn_Data$CreditScore, c(0,400, 500, 600, 700, 800, 900))
ggplot(Bank_Churn_Data, aes(x = discretized.CreditScore , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>From the above bar chart, we can infer that that more people are churned when <em>CreditScore</em> is between 600 and 700.</p>
</div>
<div id="d-relationship-between-estimatedsalary-and-exited" class="section level4">
<h4>D) Relationship between <em>EstimatedSalary</em> and <em>Exited</em></h4>
<pre class="r"><code>EstimatedSalary_Exited = glm(Exited~ EstimatedSalary, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(EstimatedSalary_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ EstimatedSalary, family = &quot;binomial&quot;, 
##     data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6753  -0.6680  -0.6610  -0.6537   1.8178  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.440e+00  5.182e-02 -27.779   &lt;2e-16 ***
## EstimatedSalary  3.875e-07  4.463e-07   0.868    0.385    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9515.2  on 9567  degrees of freedom
## AIC: 9519.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficient of the <em>EstimatedSalary</em> is not significant to predict <em>Exited</em>.</p>
<pre class="r"><code>discretized.EstimatedSalary = cut(Bank_Churn_Data$EstimatedSalary, c(0,40000, 80000, 120000, 160000, 200000),dig.lab = 1000)
ggplot(Bank_Churn_Data, aes(x = discretized.EstimatedSalary , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="e-relationship-between-numofproducts-and-exited" class="section level4">
<h4>E) Relationship between <em>NumOfProducts</em> and <em>Exited</em></h4>
<pre class="r"><code>NumOfProducts_Exited = glm(Exited~ NumOfProducts, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(NumOfProducts_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ NumOfProducts, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7401  -0.7401  -0.5848  -0.5848   2.1487  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -0.63043    0.07476  -8.433   &lt;2e-16 ***
## NumOfProducts -0.52446    0.04935 -10.628   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9398.0  on 9567  degrees of freedom
## AIC: 9402
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The number of bank products held by customers has a 99.999% significant relationship to predict the customers exited.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = NumOfProducts , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>The percentage of churn rate for customers holding 1 or 2 accounts tend to have more churn rate, compared to those customers having 3 products.</p>
</div>
<div id="f-relationship-between-tenure-and-exited" class="section level4">
<h4>F) Relationship between <em>Tenure</em> and <em>Exited</em></h4>
<pre class="r"><code>Tenure_Exited = glm(Exited~ Tenure, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Tenure_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Tenure, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6826  -0.6712  -0.6600  -0.6489   1.8285  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.337962   0.050962 -26.254   &lt;2e-16 ***
## Tenure      -0.012569   0.008891  -1.414    0.157    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9513.9  on 9567  degrees of freedom
## AIC: 9517.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficient of <em>Tenure</em> is not significant to predict <em>Exited</em>.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Tenure , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>It is surprising to see that 828 customers, after having 7 years of relationship with bank, was the highest churn number among different tenures.</p>
</div>
<div id="g-relationship-between-geography-and-exited" class="section level4">
<h4>G) Relationship between <em>Geography</em> and <em>Exited</em></h4>
<pre class="r"><code>Geography_Exited = glm(Exited~ Geography, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Geography_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Geography, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8717  -0.5950  -0.5826  -0.5826   1.9273  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -1.68749    0.03978  -42.42   &lt;2e-16 ***
## GeographyGermany  0.91572    0.05925   15.45   &lt;2e-16 ***
## GeographySpain    0.04585    0.06843    0.67    0.503    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9253.0  on 9566  degrees of freedom
## AIC: 9259
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is no significant relationship between factor <em>Geography$Spain</em> and <em>Exited</em> since its coefficient is not significant.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Geography , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>From the plot above, It can be noticed that France has the highest churn rate of customers, with almost 85% of the customers that closed the account.</p>
</div>
<div id="h-relationship-between-gender-and-exited" class="section level4">
<h4>H) Relationship between <em>Gender</em> and <em>Exited</em></h4>
<pre class="r"><code>Gender_Exited = glm(Exited~ Gender, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Gender_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Gender, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7467  -0.7467  -0.5905  -0.5905   1.9144  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.13464    0.03541  -32.04   &lt;2e-16 ***
## GenderMale  -0.52347    0.05172  -10.12   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9412.7  on 9567  degrees of freedom
## AIC: 9416.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.9% significant relationship between factor <em>Gender$male</em> and <em>Exited</em>.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Gender , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>Observe that male customers have a higher churn rate (84%), than female customers (75% approximately).</p>
</div>
<div id="i-relationship-between-hascrcard-and-exited" class="section level4">
<h4>I) Relationship between <em>HasCrCard</em> and <em>Exited</em></h4>
<pre class="r"><code>HasCrCard_Exited = glm(Exited~ HasCrCard, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(HasCrCard_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ HasCrCard, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6725  -0.6601  -0.6601  -0.6601   1.8060  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.37130    0.04686 -29.264   &lt;2e-16 ***
## HasCrCard1  -0.04170    0.05601  -0.745    0.457    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9515.4  on 9567  degrees of freedom
## AIC: 9519.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is no significant relationship between a customer having credit card and variable <em>Exited</em>.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = HasCrCard , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Notice that almost 80% of customers closed the account irrespective of customers having credit card or not.</p>
</div>
<div id="j-relationship-between-isactivemember-and-exited" class="section level4">
<h4>J) Relationship between <em>IsActiveMember</em> and <em>Exited</em></h4>
<pre class="r"><code>IsActiveMember_Exited = glm(Exited~ IsActiveMember, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(IsActiveMember_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7688  -0.7688  -0.5502  -0.5502   1.9813  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.06769    0.03326   -32.1   &lt;2e-16 ***
## IsActiveMember1 -0.74382    0.05315   -14.0   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9313.0  on 9567  degrees of freedom
## AIC: 9317
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.999% significant relationship between <em>IsActiveMember</em> and <em>Exited</em>.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = IsActiveMember , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>It is surprising to notice that actively transacted customers (86%) have a higher churn rate than customers who have not transacted actively (75%).</p>
</div>
</div>
<div id="data-resampling" class="section level1">
<h1>7. Data Resampling</h1>
<p>The bank_churn dataset was used for the training and testing of the predictive models. The dataset was randomly partitioned into two subsets, the training set, which contains 80% of the observations, and the testing set, which contains the remaining 20%.</p>
<pre class="r"><code>bank_churn_split &lt;- initial_split (Bank_Churn_Data, prop = 0.80, strata = Exited)
bank_churn_training &lt;- bank_churn_split %&gt;%
  training()
bank_churn_test &lt;- bank_churn_split %&gt;%
  testing()</code></pre>
<p><strong>Checking number of rows in training and test data</strong></p>
<pre class="r"><code>nrow(bank_churn_training)</code></pre>
<pre><code>## [1] 7654</code></pre>
<pre class="r"><code>nrow(bank_churn_test)</code></pre>
<pre><code>## [1] 1915</code></pre>
<p>In total, there are 7654 observations in the training data set and 1915 observations in test data.</p>
<p><strong>Checking multicollinearity between numerical values in a training bank churn data set</strong></p>
<pre class="r"><code>bank_churn_training %&gt;%
  select_if(is.numeric) %&gt;%
  cor()</code></pre>
<pre><code>##                  CreditScore           Age       Tenure      Balance
## CreditScore      1.000000000 -1.409731e-02  0.006624589  0.002980709
## Age             -0.014097308  1.000000e+00 -0.011427309  0.036411308
## Tenure           0.006624589 -1.142731e-02  1.000000000 -0.020880106
## Balance          0.002980709  3.641131e-02 -0.020880106  1.000000000
## NumOfProducts    0.014931772 -5.620306e-02  0.023404263 -0.335245584
## EstimatedSalary  0.006783452  7.698704e-05  0.007358602  0.014579815
##                 NumOfProducts EstimatedSalary
## CreditScore        0.01493177    6.783452e-03
## Age               -0.05620306    7.698704e-05
## Tenure             0.02340426    7.358602e-03
## Balance           -0.33524558    1.457982e-02
## NumOfProducts      1.00000000    1.732889e-02
## EstimatedSalary    0.01732889    1.000000e+00</code></pre>
<p>From the correlation matrix above, it can be noticed that there is no multicollinearity between the independent numeric variables. Just variables <em>Balance</em> and <em>NumOfProducts</em> presented a small negative correlation (-0-3258), but not large enough to drop one of the variables from the model.</p>
</div>
<div id="logistic-regression-model" class="section level1">
<h1>8. Logistic Regression Model</h1>
<p>In this section, we build our logistic regression model using the training dataset. This first approach considers all independent variables.</p>
<pre class="r"><code>full_model_bank_churn = glm(Exited ~ ., family = &quot;binomial&quot;, data = bank_churn_training)
summary(full_model_bank_churn)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ ., family = &quot;binomial&quot;, data = bank_churn_training)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2052  -0.6210  -0.4100  -0.2282   3.2306  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -4.689e+00  3.119e-01 -15.033  &lt; 2e-16 ***
## CreditScore      -2.364e-04  3.363e-04  -0.703   0.4821    
## GeographyGermany  8.655e-01  8.110e-02  10.671  &lt; 2e-16 ***
## GeographySpain    4.870e-02  8.348e-02   0.583   0.5596    
## GenderMale       -4.894e-01  6.470e-02  -7.564 3.92e-14 ***
## Age               1.089e-01  3.875e-03  28.116  &lt; 2e-16 ***
## Tenure           -2.209e-02  1.117e-02  -1.977   0.0481 *  
## Balance           1.192e-06  6.189e-07   1.926   0.0541 .  
## NumOfProducts    -3.888e-01  6.105e-02  -6.369 1.91e-10 ***
## HasCrCard1        5.357e-03  7.083e-02   0.076   0.9397    
## IsActiveMember1  -9.543e-01  6.734e-02 -14.172  &lt; 2e-16 ***
## EstimatedSalary   8.136e-07  5.638e-07   1.443   0.1490    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7610.5  on 7653  degrees of freedom
## Residual deviance: 6126.0  on 7642  degrees of freedom
## AIC: 6150
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The coefficients of the fitted model can be seen above. Observe that many of them are not significant (i.e., <em>CreditScore</em>, <em>Tenure</em>, and <em>HasCrCard1</em>, among others). Hence, the model can be further refined.</p>
<div id="a-backward-regression" class="section level4">
<h4>A) Backward Regression</h4>
<p>Here we used step AIC backward regression to find out the best model.</p>
<pre class="r"><code>mod_step_bank_churn&lt;- stepAIC(full_model_bank_churn, direction = &#39;backward&#39;, trace = FALSE)
mod_step_bank_churn</code></pre>
<pre><code>## 
## Call:  glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember + EstimatedSalary, family = &quot;binomial&quot;, 
##     data = bank_churn_training)
## 
## Coefficients:
##      (Intercept)  GeographyGermany    GeographySpain        GenderMale  
##       -4.840e+00         8.644e-01         4.796e-02        -4.892e-01  
##              Age            Tenure           Balance     NumOfProducts  
##        1.090e-01        -2.207e-02         1.196e-06        -3.889e-01  
##  IsActiveMember1   EstimatedSalary  
##       -9.557e-01         8.153e-07  
## 
## Degrees of Freedom: 7653 Total (i.e. Null);  7644 Residual
## Null Deviance:       7611 
## Residual Deviance: 6127  AIC: 6147</code></pre>
<p>Observe that variables <em>CreditScore</em>, <em>Tenure</em>, and <em>HasCrCard1</em>
were discarded from the model, whereas <em>Geography<span class="math inline">\(Germany* *Geography\)</span>France</em>, <em>Gender$Male</em>, and <em>Age</em>, among others, are the best predictors.</p>
</div>
<div id="b-bootstrap-method" class="section level4">
<h4>B) Bootstrap method</h4>
<p>Now, we use bootstrap resampling with replacement method to access the consistency predictors selected with step-wise. The number of bootstrap estimates was set to 50.</p>
<pre class="r"><code>mod_boot_bank_churn&lt;- boot.stepAIC(full_model_bank_churn, bank_churn_training, B =50)</code></pre>
<p><strong>Bootstrap summary</strong></p>
<pre class="r"><code>print(mod_boot_bank_churn)</code></pre>
<pre><code>## 
## Summary of Bootstrapping the &#39;stepAIC()&#39; procedure for
## 
## Call:
## glm(formula = Exited ~ ., family = &quot;binomial&quot;, data = bank_churn_training)
## 
## Bootstrap samples: 50 
## Direction: backward 
## Penalty: 2 * df
## 
## Covariates selected
##                 (%)
## Age             100
## Gender          100
## Geography       100
## IsActiveMember  100
## NumOfProducts   100
## Tenure           72
## Balance          70
## EstimatedSalary  52
## CreditScore      20
## HasCrCard        12
## 
## Coefficients Sign
##                   + (%)  - (%)
## Age              100.00   0.00
## Balance          100.00   0.00
## EstimatedSalary  100.00   0.00
## GeographyGermany 100.00   0.00
## GeographySpain    88.00  12.00
## HasCrCard1        66.67  33.33
## CreditScore       20.00  80.00
## GenderMale         0.00 100.00
## IsActiveMember1    0.00 100.00
## NumOfProducts      0.00 100.00
## Tenure             0.00 100.00
## 
## Stat Significance
##                     (%)
## Age              100.00
## GenderMale       100.00
## GeographyGermany 100.00
## IsActiveMember1  100.00
## NumOfProducts    100.00
## EstimatedSalary   69.23
## Balance           65.71
## Tenure            61.11
## CreditScore       50.00
## GeographySpain    18.00
## HasCrCard1        16.67
## 
## 
## The stepAIC() for the original data-set gave
## 
## Call:  glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember + EstimatedSalary, family = &quot;binomial&quot;, 
##     data = bank_churn_training)
## 
## Coefficients:
##      (Intercept)  GeographyGermany    GeographySpain        GenderMale  
##       -4.840e+00         8.644e-01         4.796e-02        -4.892e-01  
##              Age            Tenure           Balance     NumOfProducts  
##        1.090e-01        -2.207e-02         1.196e-06        -3.889e-01  
##  IsActiveMember1   EstimatedSalary  
##       -9.557e-01         8.153e-07  
## 
## Degrees of Freedom: 7653 Total (i.e. Null);  7644 Residual
## Null Deviance:       7611 
## Residual Deviance: 6127  AIC: 6147
## 
## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## Exited ~ CreditScore + Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + HasCrCard + IsActiveMember + EstimatedSalary
## 
## Final Model:
## Exited ~ Geography + Gender + Age + Tenure + Balance + NumOfProducts + 
##     IsActiveMember + EstimatedSalary
## 
## 
##            Step Df    Deviance Resid. Df Resid. Dev      AIC
## 1                                   7642   6126.009 6150.009
## 2   - HasCrCard  1 0.005721044      7643   6126.015 6148.015
## 3 - CreditScore  1 0.493993964      7644   6126.509 6146.509</code></pre>
<p>From the results above, we can notice that, even after bootstrapping, the same variables were selected as the best predictors.</p>
</div>
<div id="c-best-model-by-backward-and-bootstrap-methods" class="section level4">
<h4>C) Best model by backward and bootstrap methods</h4>
<p>The selected variables by backward and bootstrap methods were joined to fit a new model.</p>
<pre class="r"><code>best_bootmodel_bank_churn&lt;- glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + NumOfProducts + 
    IsActiveMember, family = &quot;binomial&quot;, 
    data = bank_churn_training)
summary(best_bootmodel_bank_churn) </code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember, family = &quot;binomial&quot;, data = bank_churn_training)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1927  -0.6219  -0.4108  -0.2280   3.2184  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -4.761e+00  2.084e-01 -22.847  &lt; 2e-16 ***
## GeographyGermany  8.646e-01  8.105e-02  10.668  &lt; 2e-16 ***
## GeographySpain    4.748e-02  8.345e-02   0.569   0.5694    
## GenderMale       -4.900e-01  6.468e-02  -7.576 3.56e-14 ***
## Age               1.089e-01  3.873e-03  28.127  &lt; 2e-16 ***
## Tenure           -2.173e-02  1.116e-02  -1.947   0.0516 .  
## Balance           1.212e-06  6.187e-07   1.958   0.0502 .  
## NumOfProducts    -3.870e-01  6.102e-02  -6.341 2.28e-10 ***
## IsActiveMember1  -9.570e-01  6.729e-02 -14.222  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7610.5  on 7653  degrees of freedom
## Residual deviance: 6128.6  on 7645  degrees of freedom
## AIC: 6146.6
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>From the results above, we can see that some of the coefficients, such as <em>Geography$Spain</em> and <em>Tenure</em> are not significant. Therefore, to improve our model, these variables were removed.</p>
</div>
<div id="d-final-best-model" class="section level4">
<h4>D) Final Best model</h4>
<p>The final logistic regression model is the following:</p>
<pre class="r"><code>bestmodel&lt;- glm(formula = Exited ~  Gender + Age + Balance + NumOfProducts + 
    IsActiveMember, family = &quot;binomial&quot;, 
    data = bank_churn_training)
summary(bestmodel)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Gender + Age + Balance + NumOfProducts + 
##     IsActiveMember, family = &quot;binomial&quot;, data = bank_churn_training)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0458  -0.6348  -0.4249  -0.2391   3.1870  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -4.952e+00  1.984e-01 -24.955  &lt; 2e-16 ***
## GenderMale      -5.215e-01  6.389e-02  -8.162 3.30e-16 ***
## Age              1.096e-01  3.830e-03  28.602  &lt; 2e-16 ***
## Balance          3.964e-06  5.491e-07   7.219 5.24e-13 ***
## NumOfProducts   -3.053e-01  6.009e-02  -5.081 3.75e-07 ***
## IsActiveMember1 -9.601e-01  6.652e-02 -14.434  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 7610.5  on 7653  degrees of freedom
## Residual deviance: 6257.5  on 7648  degrees of freedom
## AIC: 6269.5
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The best model includes variables <em>Gender$Male</em>, <em>Age</em>, <em>Balance</em>, <em>NumOfproducts</em>, and <em>IsActiveMember1</em>. Notice that all the coefficients are 99.999% significant to predict the response variable <em>Exited</em>.</p>
</div>
<div id="e-cut-off-value-and-test-bestmodel" class="section level4">
<h4>E) Cut-off value and Test bestmodel</h4>
<p>Here, we verify the performance results of the best model on the out-of-sample testing set.</p>
<pre class="r"><code>library(PRROC)
test.pred &lt;- predict(bestmodel,type = &quot;response&quot;, newdata = bank_churn_test)
summary(test.pred)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.00623 0.07534 0.14158 0.20496 0.28871 0.89015</code></pre>
<pre class="r"><code>bank_churn_test$prob &lt;- test.pred
test.pred.default &lt;- factor(ifelse(bank_churn_test$prob&gt;=0.5,&quot;Yes&quot;,&quot;No&quot;))
test.actual.default &lt;- factor(ifelse(bank_churn_test$Exited ==1,&quot;Yes&quot;,&quot;No&quot;))
table(test.actual.default,test.pred.default)</code></pre>
<pre><code>##                    test.pred.default
## test.actual.default   No  Yes
##                 No  1473   63
##                 Yes  269  110</code></pre>
<pre class="r"><code>test.conf &lt;- confusionMatrix(test.pred.default,test.actual.default,positive = &quot;Yes&quot;)
#test.conf
perform.fn &lt;- function(cutoff){
  pred.default &lt;- factor(ifelse(test.pred&gt;=cutoff,&quot;Yes&quot;,&quot;No&quot;))
  conf &lt;- confusionMatrix(pred.default,test.actual.default,positive = &quot;Yes&quot;)
  acc &lt;- conf$overall[1]
  sens &lt;- conf$byClass[1]
  spec &lt;- conf$byClass[2]
  out &lt;- t(as.matrix(c(sens,spec,acc)))
  colnames(out) &lt;- c(&quot;sensitivity&quot;,&quot;specificity&quot;, &quot;accuracy&quot;)
  return(out)
  summary(test.pred)
}
s &lt;- seq(0.01,1, length = 100)
OUT &lt;- matrix(0,100,3)
for(i in 1:100){
  OUT[i,] = perform.fn(s[i])
}

plot(s, OUT[,1], xlab = &quot;Cutoff&quot;, ylab = &quot;Sens/Spec/Acc&quot;, cex.lab = 1.5, cex.aux = 1.5, ylim =c(0,1), type = &quot;l&quot;, lwd =2, axes = FALSE, col =2)
axis(1, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
axis(2, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
lines(s, OUT[,2], col = &quot;darkgreen&quot;, lwd = 2)
lines(s, OUT[,3], col = 4, lwd = 2)
box()
legend(0.5, 1, col = c(2, &quot;darkgreen&quot;, 4, &quot;darkred&quot;), lwd = c(2,2,2,2), c(&quot;Sensitivity&quot;, &quot;Specificity&quot;,&quot;Accuracy&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<pre class="r"><code>diff &lt;- abs(OUT[,1]-OUT[,2])
cutoff &lt;- s[which(diff ==min(diff))]
cutoff </code></pre>
<pre><code>## [1] 0.19</code></pre>
<pre class="r"><code>test.cuttoff.default &lt;- factor(ifelse(test.pred&gt;=cutoff,&quot;Yes&quot;,&quot;No&quot;))
conf.final &lt;- confusionMatrix(test.cuttoff.default,test.actual.default,positive = &quot;Yes&quot;)
conf.final</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1061  111
##        Yes  475  268
##                                           
##                Accuracy : 0.694           
##                  95% CI : (0.6728, 0.7146)
##     No Information Rate : 0.8021          
##     P-Value [Acc &gt; NIR] : 1               
##                                           
##                   Kappa : 0.2922          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.7071          
##             Specificity : 0.6908          
##          Pos Pred Value : 0.3607          
##          Neg Pred Value : 0.9053          
##              Prevalence : 0.1979          
##          Detection Rate : 0.1399          
##    Detection Prevalence : 0.3880          
##       Balanced Accuracy : 0.6989          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<pre class="r"><code>table(test.actual.default, test.cuttoff.default)</code></pre>
<pre><code>##                    test.cuttoff.default
## test.actual.default   No  Yes
##                 No  1061  475
##                 Yes  111  268</code></pre>
<pre class="r"><code>acc &lt;- conf.final$overall[1]
sens &lt;- conf.final$byClass[1]
spec &lt;- conf.final$byClass[2]

acc</code></pre>
<pre><code>##  Accuracy 
## 0.6939948</code></pre>
<pre class="r"><code>sens</code></pre>
<pre><code>## Sensitivity 
##    0.707124</code></pre>
<pre class="r"><code>spec</code></pre>
<pre><code>## Specificity 
##   0.6907552</code></pre>
<pre class="r"><code>roc.glm &lt;- roc.curve(test.actual.default, test.cuttoff.default, curve = TRUE)
plot(roc.glm)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-59-2.png" width="672" /></p>
<p>Observe the confusion matrix shown above. In total, 1405 (out of 1915) customers were predicted correctly if they exited or not from the bank. In general, the performance of the best logistic regression model is acceptable in terms of accuracy (70%), specificity (70%), and sensitivity (70%) with a cut off value of 19% . Unfortunately, in terms of ROC AUC score, the achieved performance is far from ideal, with a score of 41%.</p>
</div>
</div>
<div id="feature-engineering-modelling-with-the-tidymodel-ecosystem" class="section level1">
<h1>9. Feature engineering: Modelling with the Tidymodel Ecosystem</h1>
<p>We used the tidymodel ecosystem to standardized the workflow of all our models, and make easier the hyper-parameter tuning and performance comparison among them.</p>
<p>The first step was to pre-processing of data:</p>
<pre class="r"><code>bank_churn_recipe &lt;- recipe(Exited ~ ., data = bank_churn_training) %&gt;%
  step_corr(all_numeric(), threshold =0.8) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes())
bank_churn_recipe</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         10
## 
## Operations:
## 
## Correlation filter on all_numeric()
## Centering and scaling for all_numeric()
## Dummy variables from all_nominal(), -all_outcomes()</code></pre>
<p>The instructions above create a recipe to check the correlation for all numeric variables and keep the thresholding limit equal to 0.8. We have also instructed the model to normalize all numeric variables and set as dummy all nominal or character variables except the outcome variable, because outcome is in factor. There are 10 predictor variables and 1 outcome variable.</p>
<p><strong>Recipe training</strong></p>
<pre class="r"><code>bank_churn_recipe_prep&lt;- bank_churn_recipe %&gt;%
  prep(training = bank_churn_training)
bank_churn_recipe_prep</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         10
## 
## Training data contained 7654 data points and no missing data.
## 
## Operations:
## 
## Correlation filter removed no terms [trained]
## Centering and scaling for CreditScore, Age, Tenure, Balance, NumOfProduct... [trained]
## Dummy variables from Geography, Gender, HasCrCard, IsActiveMember [trained]</code></pre>
<p>With the instruction above we prepared the training data set.</p>
<p><strong>Preprocess training data</strong></p>
<pre class="r"><code>bank_churn_training_prep &lt;- bank_churn_recipe_prep %&gt;%
  bake (new_data = NULL)
bank_churn_training_prep</code></pre>
<pre><code>## # A tibble: 7,654 x 12
##    CreditScore    Age   Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1      -0.439  0.381 -1.40      0.130        -0.940           0.221 0     
##  2       0.507  0.153 -1.40     -1.21          0.873          -0.105 0     
##  3       2.08   0.609 -1.05      0.797        -0.940          -0.362 0     
##  4      -1.55   0.724 -0.356     1.06          0.873          -0.434 0     
##  5       0.351 -1.22  -1.05      0.943        -0.940          -0.490 0     
##  6      -1.27  -0.761  0.337     0.421         0.873          -0.343 0     
##  7      -1.59  -1.56  -0.703    -1.21          0.873          -0.409 0     
##  8      -1.81  -0.418  1.72     -1.21          0.873          -1.28  0     
##  9      -1.05  -1.45  -0.00951  -1.21          0.873           1.58  0     
## 10      -0.158 -0.304  0.684    -1.21          0.873          -0.591 0     
## # ... with 7,644 more rows, and 5 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;</code></pre>
<p>We have pre-processed the training data set.</p>
<p><strong>Preprocess test data</strong></p>
<pre class="r"><code>bank_churn_test_prep &lt;- bank_churn_recipe_prep %&gt;%
  bake (new_data = bank_churn_test)
bank_churn_test_prep</code></pre>
<pre><code>## # A tibble: 1,915 x 12
##    CreditScore    Age Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.325   0.495 -1.05   -1.21         -0.940          0.0257 1     
##  2     -0.0542  0.724  1.03    0.609         0.873          0.869  1     
##  3      1.79    1.41   0.684  -1.21          0.873         -1.56   0     
##  4      0.0290  2.32  -1.40    0.911        -0.940         -1.65   1     
##  5      0.788  -1.56   0.337  -1.21          0.873         -0.786  0     
##  6      1.10   -0.190 -1.05    0.978        -0.940          1.22   0     
##  7     -0.616   0.153 -0.703  -1.21          2.69           0.707  1     
##  8     -1.01    0.381  1.38    0.551         0.873         -0.313  0     
##  9     -1.35    0.495  0.337  -1.21          0.873         -1.14   0     
## 10     -1.82    0.838 -1.74    0.937        -0.940         -1.25   1     
## # ... with 1,905 more rows, and 5 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;</code></pre>
<p>Finally, we pre-processed the test data. Now we can continue with the development of our prediction models.</p>
</div>
<div id="support-vector-machine-model" class="section level1">
<h1>10. Support Vector Machine Model</h1>
<p>In this section, we build our support vector machine model using the training dataset. This approach considers all independent variables and a linear kernel as parameter.</p>
<pre class="r"><code>bank_churn_svm_model &lt;- svm(Exited ~., data = bank_churn_training, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE)
bank_churn_svm_model</code></pre>
<pre><code>## 
## Call:
## svm(formula = Exited ~ ., data = bank_churn_training, type = &quot;C-classification&quot;, 
##     kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  2211</code></pre>
<p><strong>Training accuracy</strong></p>
<pre class="r"><code>bank_churn_svm_pred_train &lt;- predict(bank_churn_svm_model, bank_churn_training)
mean(bank_churn_svm_pred_train == bank_churn_training_prep$Exited)</code></pre>
<pre><code>## [1] 0.8011497</code></pre>
<p>The SVM model achieved an in-sample accuracy of 80.23%.</p>
<p><strong>Testing accuracy</strong></p>
<pre class="r"><code>bank_churn_svm_pred_test &lt;- predict(bank_churn_svm_model, newdata = bank_churn_test)
mean(bank_churn_svm_pred_test == bank_churn_test_prep$Exited)</code></pre>
<pre><code>## [1] 0.8010444</code></pre>
<p>As expected, the out-of-sample performance of the SVM model is little bit lower than that of the in-sample performance. The average predicted accuracy in the test data was of 0.802.</p>
<p><strong>Confusion Matrix</strong></p>
<pre class="r"><code>confusionMatrix(bank_churn_svm_pred_test, bank_churn_test$Exited )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1497  342
##          1   39   37
##                                           
##                Accuracy : 0.801           
##                  95% CI : (0.7824, 0.8187)
##     No Information Rate : 0.8021          
##     P-Value [Acc &gt; NIR] : 0.5592          
##                                           
##                   Kappa : 0.1034          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.97461         
##             Specificity : 0.09763         
##          Pos Pred Value : 0.81403         
##          Neg Pred Value : 0.48684         
##              Prevalence : 0.80209         
##          Detection Rate : 0.78172         
##    Detection Prevalence : 0.96031         
##       Balanced Accuracy : 0.53612         
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>From the confusion matrix shown above, we can see that accuracy is 79%, sensitivity 97.2%, but very low specificity is 6.5%.</p>
<div id="a-cut-off-value-and-test-bestmodel" class="section level4">
<h4>A) Cut-off value and Test bestmodel</h4>
<pre class="r"><code>library(PRROC)
test.svm.pred &lt;- predict(bank_churn_svm_model,type = &quot;C-classification&quot;, newdata = bank_churn_test)
summary(test.svm.pred)</code></pre>
<pre><code>##    0    1 
## 1839   76</code></pre>
<pre class="r"><code>bank_churn_test$prob &lt;- test.svm.pred
test.svm.pred.default &lt;- factor(ifelse(bank_churn_test$prob&gt;=0.5,&quot;Yes&quot;,&quot;No&quot;))
test.svm.actual.default &lt;- factor(ifelse(bank_churn_test$Exited ==1,&quot;Yes&quot;,&quot;No&quot;))
table(test.svm.pred.default, test.svm.actual.default)</code></pre>
<pre><code>## &lt; table of extent 0 x 2 &gt;</code></pre>
<pre class="r"><code>#test.conf
perform.svm.fn &lt;- function(cutoff){
  pred.svm.default &lt;- factor(ifelse(test.svm.pred&gt;=cutoff,&quot;Yes&quot;,&quot;No&quot;))

  conf&lt;- confusionMatrix(test.svm.actual.default,pred.svm.default,positive = &quot;Yes&quot;)
  acc&lt;- conf$overall[1]
  sens&lt;- conf$byClass[1]
  spec&lt;- conf$byClass[2]
  out_svm&lt;- t(as.matrix(c(sens,spec,acc)))
  colnames(out_svm) &lt;- c(&quot;sensitivity&quot;,&quot;specificity&quot;, &quot;accuracy&quot;)
  return(out_svm)
  summary(test.svm.pred)
}
s&lt;- seq(0.01,1, length = 100)
out_svm&lt;- matrix(0,100,3)
for(i in 1:100){
  out_svm[i,] = perform.fn(s[i])
}

plot(s, out_svm[,1], xlab = &quot;Cutoff&quot;, ylab = &quot;Sens/Spec/Acc&quot;, cex.lab = 1.5, cex.aux = 1.5, ylim =c(0,1), type = &quot;l&quot;, lwd =2, axes = FALSE, col =2)
axis(1, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
axis(2, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
lines(s, out_svm[,2], col = &quot;darkgreen&quot;, lwd = 2)
lines(s, out_svm[,3], col = 4, lwd = 2)
box()
legend(0.5, 1, col = c(2, &quot;darkgreen&quot;, 4, &quot;darkred&quot;), lwd = c(2,2,2,2), c(&quot;Sensitivity&quot;, &quot;Specificity&quot;,&quot;Accuracy&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<pre class="r"><code>diff.svm &lt;- abs(out_svm[,1]-out_svm[,2])
cutoff_svm&lt;- s[which(diff ==min(diff))]
cutoff_svm</code></pre>
<pre><code>## [1] 0.19</code></pre>
<pre class="r"><code>test.svm.cuttoff.default &lt;- factor(ifelse(test.svm.pred&gt;=cutoff,&quot;Yes&quot;,&quot;No&quot;))


table(test.svm.cuttoff.default, test.svm.actual.default)</code></pre>
<pre><code>## &lt; table of extent 0 x 2 &gt;</code></pre>
<p><strong>ROC Curve</strong></p>
<pre class="r"><code>roc.svm &lt;- roc.curve(bank_churn_test_prep$Exited,bank_churn_svm_pred_test, curve=TRUE)
plot(roc.svm)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>Of course, the specificity performance largely affected the ROC AUC score, achieving a total of 0.5989.</p>
</div>
</div>
<div id="logistic-regression-model-tidy-workflow-version" class="section level1">
<h1>11. Logistic Regression Model (Tidy workflow version)</h1>
<p>In this section, we rebuild our logistic regression model
under the tidy ecosystem. The model is fitted using the training dataset, and all independent variables.</p>
<pre class="r"><code>bank_churn_logistic_model&lt;- logistic_reg() %&gt;%
  set_engine(&#39;glm&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
<p><strong>Model fitting</strong></p>
<pre class="r"><code>bank_churn_logistic_fit &lt;- bank_churn_logistic_model %&gt;%
  fit(Exited ~ ., data = bank_churn_training_prep)
bank_churn_logistic_fit</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  50ms 
## 
## Call:  stats::glm(formula = Exited ~ ., family = stats::binomial, data = data)
## 
## Coefficients:
##       (Intercept)        CreditScore                Age             Tenure  
##         -1.270313          -0.022731           0.954115          -0.063722  
##           Balance      NumOfProducts    EstimatedSalary  Geography_Germany  
##          0.074462          -0.214485           0.046716           0.865468  
##   Geography_Spain        Gender_Male       HasCrCard_X1  IsActiveMember_X1  
##          0.048698          -0.489362           0.005357          -0.954322  
## 
## Degrees of Freedom: 7653 Total (i.e. Null);  7642 Residual
## Null Deviance:       7611 
## Residual Deviance: 6126  AIC: 6150</code></pre>
<p>Observed that the selected independent variables are the same to those selected in Section 8.</p>
<p><strong>Predicting outcome categories</strong></p>
<pre class="r"><code>bank_churn_logistic_class_preds &lt;- predict (bank_churn_logistic_fit, new_data = bank_churn_test_prep, type =&quot;class&quot;)
bank_churn_logistic_class_preds</code></pre>
<pre><code>## # A tibble: 1,915 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 0          
##  2 0          
##  3 0          
##  4 1          
##  5 0          
##  6 0          
##  7 0          
##  8 0          
##  9 0          
## 10 0          
## # ... with 1,905 more rows</code></pre>
<p>The above .pred class is the outcome of our results. That is if the customer will churn or not. The number o means churn and 1 means no churn.</p>
<p><strong>Estimated probabilities</strong></p>
<pre class="r"><code>bank_churn_logistic_prob_preds &lt;- predict (bank_churn_logistic_fit, new_data = bank_churn_test_prep, type =&quot;prob&quot;)
bank_churn_logistic_prob_preds</code></pre>
<pre><code>## # A tibble: 1,915 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1   0.826  0.174 
##  2   0.765  0.235 
##  3   0.858  0.142 
##  4   0.167  0.833 
##  5   0.983  0.0169
##  6   0.839  0.161 
##  7   0.838  0.162 
##  8   0.680  0.320 
##  9   0.883  0.117 
## 10   0.525  0.475 
## # ... with 1,905 more rows</code></pre>
<p>The above prediction are the probabilities of outcome occurrences.</p>
<p><strong>Combining results</strong></p>
<pre class="r"><code>bank_churn_logistic_results &lt;- bank_churn_test_prep %&gt;%
  bind_cols(bank_churn_logistic_class_preds, bank_churn_logistic_prob_preds)
bank_churn_logistic_results</code></pre>
<pre><code>## # A tibble: 1,915 x 15
##    CreditScore    Age Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.325   0.495 -1.05   -1.21         -0.940          0.0257 1     
##  2     -0.0542  0.724  1.03    0.609         0.873          0.869  1     
##  3      1.79    1.41   0.684  -1.21          0.873         -1.56   0     
##  4      0.0290  2.32  -1.40    0.911        -0.940         -1.65   1     
##  5      0.788  -1.56   0.337  -1.21          0.873         -0.786  0     
##  6      1.10   -0.190 -1.05    0.978        -0.940          1.22   0     
##  7     -0.616   0.153 -0.703  -1.21          2.69           0.707  1     
##  8     -1.01    0.381  1.38    0.551         0.873         -0.313  0     
##  9     -1.35    0.495  0.337  -1.21          0.873         -1.14   0     
## 10     -1.82    0.838 -1.74    0.937        -0.940         -1.25   1     
## # ... with 1,905 more rows, and 8 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;, .pred_class &lt;fct&gt;, .pred_0 &lt;dbl&gt;, .pred_1 &lt;dbl&gt;</code></pre>
<p>We have combined the predicted outcome and probabilities in to test data.</p>
<p><strong>Assessing the model fit using Confusion matrix</strong></p>
<pre class="r"><code>bank_churn_logistic_results %&gt;%
  conf_mat(truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<p><strong>Correct predictions of test set</strong>
From the confusion matrix above, we observe that 1484 customers, who churned, were correctly classified (True Negatives). The number of True Positives is 120, all of these customers were loyal customer and did not churn.</p>
<p><strong>Classification error of test set</strong>
The number of False Positives is 52 customers, who were predicted as not churned but actually churned. The number of False Negatives is 2259 people, who are predicted as churned but actually not churned and were loyal to bank.</p>
<p><strong>Creating workflow</strong></p>
<pre class="r"><code>bank_churn_logistic_wkfl&lt;- workflow() %&gt;%
add_model(bank_churn_logistic_model) %&gt;%
add_recipe(bank_churn_recipe)
bank_churn_logistic_wkfl</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm</code></pre>
<p>We have created workflow above in the model that to check the correlation for all numeric variables and keep the threshold limit as 0.8. We have also given instruction in the model to normalize all the numeric variables and set dummy to all nominal or character variables except the outcome variables because outcome is a character variable.</p>
<p><strong>Train the workflow</strong></p>
<pre class="r"><code>bank_churn_logistic__wkfl_fit &lt;- bank_churn_logistic_wkfl %&gt;%
  last_fit( split = bank_churn_split)
bank_churn_logistic__wkfl_fit</code></pre>
<pre><code>## # Resampling results
## # Manual resampling 
## # A tibble: 1 x 6
##   splits              id               .metrics  .notes   .predictions .workflow
##   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;    &lt;list&gt;   &lt;list&gt;       &lt;list&gt;   
## 1 &lt;split [7654/1915]&gt; train/test split &lt;tibble ~ &lt;tibble~ &lt;tibble [1,~ &lt;workflo~</code></pre>
<p>Above we trained the corresponding workflow.</p>
<p><strong>Calculating performance metrics</strong></p>
<pre class="r"><code>bank_churn_logistic__wkfl_fit %&gt;%
   collect_metrics() </code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.827 Preprocessor1_Model1
## 2 roc_auc  binary         0.788 Preprocessor1_Model1</code></pre>
<p>The accuracy of the logistic regression model is 84.28% and roc-auc is 80.99%</p>
<p><strong>collecting predictions</strong></p>
<pre class="r"><code>bank_churn_logistic_wkfl_fit_results&lt;- bank_churn_logistic__wkfl_fit %&gt;%
  collect_predictions()
bank_churn_logistic_wkfl_fit_results</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.826  0.174      1 0           1      Preprocessor1_Mode~
##  2 train/test split   0.765  0.235      6 0           1      Preprocessor1_Mode~
##  3 train/test split   0.858  0.142      7 0           0      Preprocessor1_Mode~
##  4 train/test split   0.167  0.833     16 1           1      Preprocessor1_Mode~
##  5 train/test split   0.983  0.0169    19 0           0      Preprocessor1_Mode~
##  6 train/test split   0.839  0.161     26 0           0      Preprocessor1_Mode~
##  7 train/test split   0.838  0.162     30 0           1      Preprocessor1_Mode~
##  8 train/test split   0.680  0.320     32 0           0      Preprocessor1_Mode~
##  9 train/test split   0.883  0.117     33 0           0      Preprocessor1_Mode~
## 10 train/test split   0.525  0.475     35 0           1      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Creating custom metrics</strong></p>
<pre class="r"><code>bank_churn_logistic_metrics &lt;- metric_set(roc_auc,yardstick::sens,yardstick::spec,accuracy)
bank_churn_logistic_metrics</code></pre>
<pre><code>## # A tibble: 4 x 3
##   metric   class        direction
##   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;    
## 1 roc_auc  prob_metric  maximize 
## 2 sens     class_metric maximize 
## 3 spec     class_metric maximize 
## 4 accuracy class_metric maximize</code></pre>
<p>We created a set of metrics to compare our models: ROC AUC score, sensitivity, specificity, and accuracy.</p>
<pre class="r"><code>bank_churn_logistic_wkfl_fit_results %&gt;%
 bank_churn_logistic_metrics(truth = Exited, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.960
## 2 spec     binary         0.290
## 3 accuracy binary         0.827
## 4 roc_auc  binary         0.212</code></pre>
<p>The table above shows the calculated metrics using logistic regression. The model achieved a sensitivity of 0.966 but a very low score in specificity (0.298), which largely affected its ROC AUC score (0.183).</p>
</div>
<div id="random-forest-models" class="section level1">
<h1>12. Random Forest Models</h1>
<pre><code>## ntree      OOB      1      2
##     1:  21.39% 14.68% 49.35%
##     2:  21.25% 13.07% 54.69%
##     3:  20.90% 12.48% 55.15%
##     4:  20.71% 12.27% 55.13%
##     5:  19.81% 11.34% 54.50%
##     6:  19.36% 10.58% 55.37%
##     7:  19.15% 10.24% 55.78%
##     8:  18.75%  9.85% 55.15%
##     9:  18.22%  9.45% 53.94%
##    10:  17.96%  9.03% 54.36%
##    11:  17.59%  8.64% 54.01%
##    12:  17.68%  8.70% 54.22%
##    13:  17.11%  8.05% 53.95%
##    14:  16.78%  7.73% 53.55%
##    15:  17.14%  7.78% 55.17%
##    16:  16.96%  7.53% 55.26%
##    17:  16.78%  7.38% 54.93%
##    18:  16.58%  7.15% 54.83%
##    19:  16.39%  7.05% 54.30%
##    20:  16.47%  7.00% 54.89%
##    21:  16.23%  6.74% 54.73%
##    22:  16.26%  6.73% 54.92%
##    23:  16.02%  6.42% 54.99%
##    24:  16.12%  6.45% 55.39%
##    25:  15.85%  6.14% 55.25%
##    26:  15.80%  6.14% 54.99%
##    27:  15.92%  6.29% 54.99%
##    28:  15.86%  6.22% 54.99%
##    29:  15.90%  6.30% 54.86%
##    30:  15.86%  6.14% 55.32%
##    31:  15.70%  5.88% 55.58%
##    32:  15.73%  6.12% 54.73%
##    33:  15.86%  5.98% 55.98%
##    34:  15.81%  5.96% 55.78%
##    35:  15.68%  5.73% 56.05%
##    36:  15.60%  5.65% 55.98%
##    37:  15.52%  5.63% 55.65%
##    38:  15.48%  5.62% 55.52%
##    39:  15.36%  5.49% 55.45%
##    40:  15.59%  5.50% 56.51%
##    41:  15.44%  5.52% 55.72%
##    42:  15.35%  5.49% 55.39%
##    43:  15.21%  5.37% 55.12%
##    44:  15.29%  5.37% 55.52%
##    45:  15.31%  5.47% 55.25%
##    46:  15.48%  5.63% 55.45%
##    47:  15.33%  5.50% 55.19%
##    48:  15.30%  5.44% 55.32%
##    49:  15.34%  5.42% 55.58%
##    50:  15.29%  5.31% 55.78%
##    51:  15.19%  5.34% 55.19%
##    52:  15.26%  5.26% 55.85%
##    53:  15.18%  5.31% 55.25%
##    54:  15.18%  5.29% 55.32%
##    55:  15.22%  5.31% 55.45%
##    56:  15.21%  5.36% 55.19%
##    57:  15.08%  5.26% 54.92%
##    58:  15.18%  5.28% 55.39%
##    59:  15.16%  5.23% 55.45%
##    60:  15.04%  5.19% 54.99%
##    61:  15.04%  5.11% 55.32%
##    62:  14.99%  5.05% 55.32%
##    63:  14.92%  5.03% 55.06%
##    64:  15.05%  5.11% 55.39%
##    65:  15.06%  5.10% 55.52%
##    66:  14.91%  5.02% 55.06%
##    67:  14.88%  5.02% 54.92%
##    68:  14.83%  5.03% 54.59%
##    69:  14.88%  5.02% 54.92%
##    70:  14.97%  5.06% 55.19%
##    71:  14.89%  5.02% 54.99%
##    72:  14.92%  4.97% 55.32%
##    73:  14.91%  5.03% 54.99%
##    74:  14.80%  4.92% 54.92%
##    75:  14.84%  4.95% 54.99%
##    76:  14.80%  4.90% 54.99%
##    77:  14.79%  4.90% 54.92%
##    78:  14.80%  4.90% 54.99%
##    79:  14.76%  4.87% 54.92%
##    80:  14.85%  4.92% 55.19%
##    81:  14.91%  4.95% 55.32%
##    82:  14.75%  4.84% 54.99%
##    83:  14.87%  4.92% 55.25%
##    84:  14.76%  4.84% 55.06%
##    85:  14.76%  4.90% 54.79%
##    86:  14.80%  4.90% 54.99%
##    87:  14.75%  4.85% 54.92%
##    88:  14.70%  4.80% 54.86%
##    89:  14.67%  4.77% 54.86%
##    90:  14.62%  4.79% 54.53%
##    91:  14.59%  4.71% 54.73%
##    92:  14.70%  4.77% 54.99%
##    93:  14.72%  4.75% 55.19%
##    94:  14.70%  4.75% 55.06%
##    95:  14.67%  4.66% 55.32%
##    96:  14.67%  4.71% 55.12%
##    97:  14.63%  4.71% 54.92%
##    98:  14.62%  4.77% 54.59%
##    99:  14.62%  4.69% 54.92%
##   100:  14.66%  4.82% 54.59%
##   101:  14.69%  4.85% 54.59%
##   102:  14.67%  4.80% 54.73%
##   103:  14.66%  4.72% 54.99%
##   104:  14.70%  4.77% 54.99%
##   105:  14.72%  4.71% 55.39%
##   106:  14.72%  4.69% 55.45%
##   107:  14.70%  4.67% 55.39%
##   108:  14.67%  4.64% 55.39%
##   109:  14.66%  4.61% 55.45%
##   110:  14.63%  4.58% 55.45%
##   111:  14.65%  4.56% 55.58%
##   112:  14.65%  4.58% 55.52%
##   113:  14.59%  4.54% 55.39%
##   114:  14.61%  4.56% 55.39%
##   115:  14.62%  4.58% 55.39%
##   116:  14.72%  4.67% 55.52%
##   117:  14.58%  4.56% 55.25%
##   118:  14.59%  4.56% 55.32%
##   119:  14.63%  4.54% 55.58%
##   120:  14.57%  4.51% 55.39%
##   121:  14.69%  4.62% 55.52%
##   122:  14.63%  4.59% 55.39%
##   123:  14.65%  4.61% 55.39%
##   124:  14.65%  4.58% 55.52%
##   125:  14.65%  4.58% 55.52%
##   126:  14.61%  4.54% 55.45%
##   127:  14.61%  4.54% 55.45%
##   128:  14.65%  4.58% 55.52%
##   129:  14.52%  4.51% 55.12%
##   130:  14.58%  4.58% 55.19%
##   131:  14.62%  4.62% 55.19%
##   132:  14.65%  4.61% 55.39%
##   133:  14.62%  4.59% 55.32%
##   134:  14.55%  4.54% 55.19%
##   135:  14.52%  4.54% 54.99%
##   136:  14.61%  4.56% 55.39%
##   137:  14.63%  4.61% 55.32%
##   138:  14.70%  4.69% 55.32%
##   139:  14.74%  4.66% 55.65%
##   140:  14.76%  4.67% 55.72%
##   141:  14.79%  4.69% 55.78%
##   142:  14.67%  4.64% 55.39%
##   143:  14.71%  4.66% 55.52%
##   144:  14.63%  4.62% 55.25%
##   145:  14.59%  4.56% 55.32%
##   146:  14.72%  4.69% 55.45%
##   147:  14.63%  4.59% 55.39%
##   148:  14.59%  4.59% 55.19%
##   149:  14.70%  4.69% 55.32%
##   150:  14.59%  4.61% 55.12%
##   151:  14.69%  4.67% 55.32%
##   152:  14.55%  4.58% 55.06%
##   153:  14.58%  4.61% 55.06%
##   154:  14.55%  4.56% 55.12%
##   155:  14.58%  4.59% 55.12%
##   156:  14.55%  4.58% 55.06%
##   157:  14.57%  4.54% 55.25%
##   158:  14.57%  4.58% 55.12%
##   159:  14.71%  4.67% 55.45%
##   160:  14.62%  4.61% 55.25%
##   161:  14.57%  4.58% 55.12%
##   162:  14.57%  4.54% 55.25%
##   163:  14.61%  4.58% 55.32%
##   164:  14.58%  4.56% 55.25%
##   165:  14.66%  4.58% 55.58%
##   166:  14.63%  4.58% 55.45%
##   167:  14.65%  4.59% 55.45%
##   168:  14.53%  4.51% 55.19%
##   169:  14.53%  4.46% 55.39%
##   170:  14.55%  4.51% 55.32%
##   171:  14.57%  4.51% 55.39%
##   172:  14.61%  4.53% 55.52%
##   173:  14.57%  4.54% 55.25%
##   174:  14.53%  4.51% 55.19%
##   175:  14.57%  4.56% 55.19%
##   176:  14.53%  4.51% 55.19%
##   177:  14.46%  4.48% 54.99%
##   178:  14.45%  4.46% 54.99%
##   179:  14.52%  4.51% 55.12%
##   180:  14.52%  4.45% 55.39%
##   181:  14.66%  4.54% 55.72%
##   182:  14.59%  4.51% 55.52%
##   183:  14.62%  4.53% 55.58%
##   184:  14.63%  4.56% 55.52%
##   185:  14.61%  4.51% 55.58%
##   186:  14.58%  4.51% 55.45%
##   187:  14.65%  4.53% 55.72%
##   188:  14.61%  4.48% 55.72%
##   189:  14.54%  4.45% 55.52%
##   190:  14.61%  4.54% 55.45%
##   191:  14.67%  4.53% 55.85%
##   192:  14.54%  4.51% 55.25%
##   193:  14.61%  4.56% 55.39%
##   194:  14.58%  4.56% 55.25%
##   195:  14.58%  4.53% 55.39%
##   196:  14.55%  4.53% 55.25%
##   197:  14.58%  4.56% 55.25%
##   198:  14.57%  4.54% 55.25%
##   199:  14.59%  4.56% 55.32%
##   200:  14.53%  4.53% 55.12%
##   201:  14.57%  4.53% 55.32%
##   202:  14.53%  4.53% 55.12%
##   203:  14.59%  4.56% 55.32%
##   204:  14.62%  4.59% 55.32%
##   205:  14.55%  4.56% 55.12%
##   206:  14.52%  4.53% 55.06%
##   207:  14.54%  4.59% 54.92%
##   208:  14.53%  4.56% 54.99%
##   209:  14.57%  4.56% 55.19%
##   210:  14.54%  4.56% 55.06%
##   211:  14.55%  4.59% 54.99%
##   212:  14.58%  4.61% 55.06%
##   213:  14.54%  4.56% 55.06%
##   214:  14.46%  4.51% 54.86%
##   215:  14.54%  4.62% 54.79%
##   216:  14.52%  4.59% 54.79%
##   217:  14.57%  4.62% 54.92%
##   218:  14.54%  4.64% 54.73%
##   219:  14.53%  4.59% 54.86%
##   220:  14.54%  4.59% 54.92%
##   221:  14.50%  4.56% 54.86%
##   222:  14.53%  4.58% 54.92%
##   223:  14.49%  4.51% 54.99%
##   224:  14.55%  4.54% 55.19%
##   225:  14.59%  4.59% 55.19%
##   226:  14.55%  4.53% 55.25%
##   227:  14.52%  4.51% 55.12%
##   228:  14.49%  4.53% 54.92%
##   229:  14.55%  4.58% 55.06%
##   230:  14.62%  4.58% 55.39%
##   231:  14.61%  4.58% 55.32%
##   232:  14.62%  4.59% 55.32%
##   233:  14.57%  4.58% 55.12%
##   234:  14.57%  4.56% 55.19%
##   235:  14.57%  4.51% 55.39%
##   236:  14.59%  4.56% 55.32%
##   237:  14.61%  4.54% 55.45%
##   238:  14.62%  4.56% 55.45%
##   239:  14.67%  4.61% 55.52%
##   240:  14.61%  4.56% 55.39%
##   241:  14.63%  4.54% 55.58%
##   242:  14.62%  4.54% 55.52%
##   243:  14.63%  4.56% 55.52%
##   244:  14.63%  4.56% 55.52%
##   245:  14.59%  4.49% 55.58%
##   246:  14.61%  4.51% 55.58%
##   247:  14.59%  4.53% 55.45%
##   248:  14.59%  4.49% 55.58%
##   249:  14.66%  4.58% 55.58%
##   250:  14.63%  4.53% 55.65%
##   251:  14.58%  4.49% 55.52%
##   252:  14.55%  4.51% 55.32%
##   253:  14.58%  4.53% 55.39%
##   254:  14.59%  4.49% 55.58%
##   255:  14.59%  4.48% 55.65%
##   256:  14.63%  4.54% 55.58%
##   257:  14.62%  4.53% 55.58%
##   258:  14.57%  4.48% 55.52%
##   259:  14.59%  4.51% 55.52%
##   260:  14.62%  4.51% 55.65%
##   261:  14.63%  4.49% 55.78%
##   262:  14.69%  4.54% 55.85%
##   263:  14.62%  4.49% 55.72%
##   264:  14.63%  4.49% 55.78%
##   265:  14.61%  4.46% 55.78%
##   266:  14.62%  4.46% 55.85%
##   267:  14.59%  4.43% 55.85%
##   268:  14.61%  4.46% 55.78%
##   269:  14.62%  4.46% 55.85%
##   270:  14.61%  4.46% 55.78%
##   271:  14.59%  4.41% 55.92%
##   272:  14.63%  4.46% 55.92%
##   273:  14.67%  4.51% 55.92%
##   274:  14.61%  4.45% 55.85%
##   275:  14.61%  4.48% 55.72%
##   276:  14.62%  4.45% 55.92%
##   277:  14.74%  4.54% 56.11%
##   278:  14.70%  4.56% 55.85%
##   279:  14.70%  4.54% 55.92%
##   280:  14.76%  4.56% 56.18%
##   281:  14.71%  4.49% 56.18%
##   282:  14.74%  4.51% 56.25%
##   283:  14.72%  4.51% 56.18%
##   284:  14.74%  4.49% 56.31%
##   285:  14.75%  4.54% 56.18%
##   286:  14.76%  4.56% 56.18%
##   287:  14.74%  4.51% 56.25%
##   288:  14.79%  4.54% 56.38%
##   289:  14.76%  4.54% 56.25%
##   290:  14.74%  4.56% 56.05%
##   291:  14.72%  4.54% 56.05%
##   292:  14.72%  4.59% 55.85%
##   293:  14.72%  4.53% 56.11%
##   294:  14.69%  4.53% 55.92%
##   295:  14.72%  4.56% 55.98%
##   296:  14.71%  4.51% 56.11%
##   297:  14.74%  4.51% 56.25%
##   298:  14.71%  4.54% 55.98%
##   299:  14.71%  4.51% 56.11%
##   300:  14.69%  4.51% 55.98%
##   301:  14.70%  4.49% 56.11%
##   302:  14.66%  4.46% 56.05%
##   303:  14.65%  4.46% 55.98%
##   304:  14.66%  4.51% 55.85%
##   305:  14.66%  4.49% 55.92%
##   306:  14.66%  4.48% 55.98%
##   307:  14.63%  4.48% 55.85%
##   308:  14.67%  4.51% 55.92%
##   309:  14.63%  4.49% 55.78%
##   310:  14.66%  4.48% 55.98%
##   311:  14.69%  4.49% 56.05%
##   312:  14.63%  4.46% 55.92%
##   313:  14.69%  4.46% 56.18%
##   314:  14.62%  4.43% 55.98%
##   315:  14.65%  4.45% 56.05%
##   316:  14.62%  4.40% 56.11%
##   317:  14.65%  4.45% 56.05%
##   318:  14.67%  4.45% 56.18%
##   319:  14.66%  4.45% 56.11%
##   320:  14.62%  4.38% 56.18%
##   321:  14.59%  4.40% 55.98%
##   322:  14.61%  4.38% 56.11%
##   323:  14.63%  4.40% 56.18%
##   324:  14.62%  4.36% 56.25%
##   325:  14.62%  4.40% 56.11%
##   326:  14.65%  4.41% 56.18%
##   327:  14.65%  4.40% 56.25%
##   328:  14.63%  4.40% 56.18%
##   329:  14.61%  4.38% 56.11%
##   330:  14.62%  4.35% 56.31%
##   331:  14.65%  4.40% 56.25%
##   332:  14.67%  4.40% 56.38%
##   333:  14.71%  4.45% 56.38%
##   334:  14.71%  4.43% 56.44%
##   335:  14.70%  4.41% 56.44%
##   336:  14.66%  4.43% 56.18%
##   337:  14.66%  4.41% 56.25%
##   338:  14.63%  4.40% 56.18%
##   339:  14.62%  4.40% 56.11%
##   340:  14.65%  4.45% 56.05%
##   341:  14.58%  4.36% 56.05%
##   342:  14.62%  4.40% 56.11%
##   343:  14.58%  4.38% 55.98%
##   344:  14.63%  4.40% 56.18%
##   345:  14.62%  4.36% 56.25%
##   346:  14.70%  4.43% 56.38%
##   347:  14.72%  4.41% 56.58%
##   348:  14.70%  4.41% 56.44%
##   349:  14.72%  4.43% 56.51%
##   350:  14.63%  4.40% 56.18%
##   351:  14.70%  4.43% 56.38%
##   352:  14.70%  4.45% 56.31%
##   353:  14.67%  4.45% 56.18%
##   354:  14.65%  4.41% 56.18%
##   355:  14.61%  4.38% 56.11%
##   356:  14.58%  4.36% 56.05%
##   357:  14.61%  4.36% 56.18%
##   358:  14.62%  4.36% 56.25%
##   359:  14.58%  4.35% 56.11%
##   360:  14.61%  4.36% 56.18%
##   361:  14.53%  4.32% 55.98%
##   362:  14.55%  4.35% 55.98%
##   363:  14.59%  4.35% 56.18%
##   364:  14.52%  4.33% 55.85%
##   365:  14.58%  4.38% 55.98%
##   366:  14.57%  4.38% 55.92%
##   367:  14.58%  4.38% 55.98%
##   368:  14.53%  4.38% 55.72%
##   369:  14.54%  4.36% 55.85%
##   370:  14.53%  4.36% 55.78%
##   371:  14.58%  4.36% 56.05%
##   372:  14.58%  4.40% 55.92%
##   373:  14.58%  4.38% 55.98%
##   374:  14.54%  4.35% 55.92%
##   375:  14.54%  4.36% 55.85%
##   376:  14.55%  4.38% 55.85%
##   377:  14.58%  4.36% 56.05%
##   378:  14.50%  4.30% 55.92%
##   379:  14.55%  4.33% 56.05%
##   380:  14.55%  4.35% 55.98%
##   381:  14.49%  4.28% 55.92%
##   382:  14.49%  4.28% 55.92%
##   383:  14.50%  4.27% 56.05%
##   384:  14.50%  4.28% 55.98%
##   385:  14.50%  4.30% 55.92%
##   386:  14.49%  4.30% 55.85%
##   387:  14.58%  4.35% 56.11%
##   388:  14.58%  4.35% 56.11%
##   389:  14.57%  4.38% 55.92%
##   390:  14.49%  4.28% 55.92%
##   391:  14.54%  4.35% 55.92%
##   392:  14.55%  4.35% 55.98%
##   393:  14.58%  4.38% 55.98%
##   394:  14.54%  4.35% 55.92%
##   395:  14.55%  4.35% 55.98%
##   396:  14.58%  4.38% 55.98%
##   397:  14.57%  4.38% 55.92%
##   398:  14.54%  4.36% 55.85%
##   399:  14.53%  4.36% 55.78%
##   400:  14.61%  4.43% 55.92%
##   401:  14.57%  4.40% 55.85%
##   402:  14.58%  4.41% 55.85%
##   403:  14.55%  4.41% 55.72%
##   404:  14.55%  4.40% 55.78%
##   405:  14.54%  4.40% 55.72%
##   406:  14.50%  4.38% 55.58%
##   407:  14.53%  4.38% 55.72%
##   408:  14.53%  4.38% 55.72%
##   409:  14.54%  4.41% 55.65%
##   410:  14.57%  4.41% 55.78%
##   411:  14.54%  4.43% 55.58%
##   412:  14.50%  4.40% 55.52%
##   413:  14.49%  4.38% 55.52%
##   414:  14.53%  4.38% 55.72%
##   415:  14.57%  4.43% 55.72%
##   416:  14.54%  4.43% 55.58%
##   417:  14.53%  4.43% 55.52%
##   418:  14.53%  4.41% 55.58%
##   419:  14.54%  4.38% 55.78%
##   420:  14.52%  4.40% 55.58%
##   421:  14.52%  4.38% 55.65%
##   422:  14.57%  4.43% 55.72%
##   423:  14.55%  4.43% 55.65%
##   424:  14.53%  4.41% 55.58%
##   425:  14.55%  4.40% 55.78%
##   426:  14.57%  4.43% 55.72%
##   427:  14.53%  4.41% 55.58%
##   428:  14.57%  4.41% 55.78%
##   429:  14.57%  4.40% 55.85%
##   430:  14.52%  4.33% 55.85%
##   431:  14.50%  4.33% 55.78%
##   432:  14.53%  4.38% 55.72%
##   433:  14.53%  4.36% 55.78%
##   434:  14.54%  4.36% 55.85%
##   435:  14.53%  4.35% 55.85%
##   436:  14.49%  4.32% 55.78%
##   437:  14.52%  4.33% 55.85%
##   438:  14.49%  4.32% 55.78%
##   439:  14.54%  4.35% 55.92%
##   440:  14.57%  4.38% 55.92%
##   441:  14.57%  4.38% 55.92%
##   442:  14.62%  4.41% 56.05%
##   443:  14.65%  4.41% 56.18%
##   444:  14.57%  4.36% 55.98%
##   445:  14.58%  4.40% 55.92%
##   446:  14.58%  4.40% 55.92%
##   447:  14.57%  4.35% 56.05%
##   448:  14.55%  4.35% 55.98%
##   449:  14.49%  4.32% 55.78%
##   450:  14.58%  4.40% 55.92%
##   451:  14.53%  4.36% 55.78%
##   452:  14.58%  4.38% 55.98%
##   453:  14.59%  4.35% 56.18%
##   454:  14.57%  4.36% 55.98%
##   455:  14.55%  4.36% 55.92%
##   456:  14.55%  4.38% 55.85%
##   457:  14.52%  4.30% 55.98%
##   458:  14.53%  4.33% 55.92%
##   459:  14.57%  4.40% 55.85%
##   460:  14.53%  4.36% 55.78%
##   461:  14.49%  4.32% 55.78%
##   462:  14.50%  4.33% 55.78%
##   463:  14.48%  4.30% 55.78%
##   464:  14.49%  4.33% 55.72%
##   465:  14.49%  4.33% 55.72%
##   466:  14.54%  4.38% 55.78%
##   467:  14.54%  4.38% 55.78%
##   468:  14.53%  4.36% 55.78%
##   469:  14.49%  4.36% 55.58%
##   470:  14.46%  4.32% 55.65%
##   471:  14.46%  4.32% 55.65%
##   472:  14.45%  4.30% 55.65%
##   473:  14.48%  4.33% 55.65%
##   474:  14.49%  4.28% 55.92%
##   475:  14.49%  4.32% 55.78%
##   476:  14.52%  4.33% 55.85%
##   477:  14.49%  4.32% 55.78%
##   478:  14.52%  4.33% 55.85%
##   479:  14.53%  4.36% 55.78%
##   480:  14.52%  4.35% 55.78%
##   481:  14.53%  4.35% 55.85%
##   482:  14.54%  4.33% 55.98%
##   483:  14.52%  4.32% 55.92%
##   484:  14.53%  4.33% 55.92%
##   485:  14.50%  4.30% 55.92%
##   486:  14.54%  4.35% 55.92%
##   487:  14.55%  4.35% 55.98%
##   488:  14.53%  4.35% 55.85%
##   489:  14.50%  4.32% 55.85%
##   490:  14.54%  4.32% 56.05%
##   491:  14.53%  4.32% 55.98%
##   492:  14.55%  4.33% 56.05%
##   493:  14.54%  4.30% 56.11%
##   494:  14.50%  4.30% 55.92%
##   495:  14.45%  4.25% 55.85%
##   496:  14.45%  4.23% 55.92%
##   497:  14.45%  4.23% 55.92%
##   498:  14.53%  4.28% 56.11%
##   499:  14.53%  4.28% 56.11%
##   500:  14.54%  4.30% 56.11%</code></pre>
<div id="a-cut-off-value-and-test-bestmodel-1" class="section level4">
<h4>A) Cut-off value and Test bestmodel</h4>
<pre class="r"><code>library(PRROC)
test.rf.pred &lt;- predict(rf,type = &quot;prob&quot;, newdata = bank_churn_test)
test_cutoff_default_rf &lt;- factor(ifelse(test.rf.pred[,2]&gt;=0.5, &quot;Yes&quot;, &quot;No&quot;))


table(bank_churn_test$Exited, test_cutoff_default_rf )</code></pre>
<pre><code>##    test_cutoff_default_rf
##       No  Yes
##   0 1473   63
##   1  206  173</code></pre>
<pre class="r"><code>#test.conf
perform.fn_rf &lt;- function(cutoff){
  predicted_response_rf &lt;- as.factor(ifelse(test.rf.pred[,2]&gt;=cutoff,&quot;Yes&quot;,&quot;No&quot;))
  conf_rf&lt;- confusionMatrix(predicted_response_rf,test_cutoff_default_rf,positive = &quot;Yes&quot;)
  
  acc &lt;- conf_rf$overall[1]
  sens &lt;- conf_rf$byClass[1]
  spec &lt;- conf_rf$byClass[2]
  out_rf &lt;- t(as.matrix(c(sens,spec,acc)))
  colnames(out_rf) &lt;- c(&quot;sensitivity&quot;,&quot;specificity&quot;, &quot;accuracy&quot;)
  return(out_rf)
 
}
s &lt;- seq(0.01,1, length = 100)
out_rf &lt;- matrix(0,100,3)
for(i in 1:100){
  out_rf[i,] = perform.fn_rf(s[i])
}

plot(s, out_rf[,1], xlab = &quot;Cutoff&quot;, ylab = &quot;Sens/Spec/Acc&quot;, cex.lab = 1.5, cex.aux = 1.5, ylim =c(0,1), type = &quot;l&quot;, lwd =2, axes = FALSE, col =2)
axis(1, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
axis(2, seq(0,1, length = 5), seq(0,1, length = 5), cex.lab = 1.5)
lines(s, out_rf[,2], col = &quot;darkgreen&quot;, lwd = 2)
lines(s, out_rf[,3], col = 4, lwd = 2)
box()
legend(0.5, 1, col = c(2, &quot;darkgreen&quot;, 4, &quot;darkred&quot;), lwd = c(2,2,2,2), c(&quot;Sensitivity&quot;, &quot;Specificity&quot;,&quot;Accuracy&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<pre class="r"><code>diff &lt;- abs(out_rf[,1]-out_rf[,2])
cutoff_rf &lt;- s[which(diff ==min(diff))]
cutoff_rf</code></pre>
<pre><code>## [1] 0.5</code></pre>
<pre class="r"><code>predicted_response_rf_new &lt;- factor(ifelse(test.rf.pred[,2]&gt;=cutoff_rf,&quot;Yes&quot;,&quot;No&quot;))
confusion_matrix_rf_new&lt;- confusionMatrix(predicted_response_rf_new,test_cutoff_default_rf,positive = &quot;Yes&quot;)

acc &lt;- confusion_matrix_rf_new$overall[1]
sens &lt;- confusion_matrix_rf_new$byClass[1]
spec &lt;- confusion_matrix_rf_new$byClass[2]

acc</code></pre>
<pre><code>## Accuracy 
##        1</code></pre>
<pre class="r"><code>sens</code></pre>
<pre><code>## Sensitivity 
##           1</code></pre>
<pre class="r"><code>spec</code></pre>
<pre><code>## Specificity 
##           1</code></pre>
<p>The cut-off value is 50%.The accuracy, sensitivity and specificity are 1. This is the best results compared with logistic regression and support vector machine.</p>
<p><strong>Roc curve</strong></p>
<pre class="r"><code>roc.rf1 &lt;- roc.curve(test.actual.default, test_cutoff_default_rf, curve = TRUE)
plot(roc.rf1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-85-1.png" width="672" />
The roc-curve of random forest model is 53.2%</p>
<p>In this section we develop a Random Forest Model for the prediction of customer churn. The parameters used were the following. The number of trees was set to 100, the smallest node size allowed was set to 10, and the number of predictors seen at each node was set to 4.</p>
<pre class="r"><code>bank_churn_rf_model&lt;- rand_forest(mtry =4,trees = 100, min_n =10) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
<p><strong>Training a forest</strong></p>
<pre class="r"><code>bank_churn_fit_rf &lt;- bank_churn_rf_model %&gt;%
  fit (Exited ~ ., data = bank_churn_training_prep)
bank_churn_fit_rf</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  1s 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~100, min.node.size = min_rows(~10, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  100 
## Sample size:                      7654 
## Number of independent variables:  11 
## Mtry:                             4 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.1061698</code></pre>
<p>We considered all independent variables for model fitting.</p>
<p><strong>Predicting outcome variables</strong></p>
<pre class="r"><code>bank_churn_class_preds_rf &lt;- predict(bank_churn_fit_rf, new_data = bank_churn_test_prep, type = &quot;class&quot;)
bank_churn_class_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 0          
##  2 0          
##  3 0          
##  4 1          
##  5 0          
##  6 0          
##  7 1          
##  8 0          
##  9 0          
## 10 0          
## # ... with 1,905 more rows</code></pre>
<p>The table above shows a sample of predicted outcomes using the independent set.</p>
<p><strong>Estimated probabilities</strong></p>
<pre class="r"><code>bank_churn_prob_preds_rf &lt;- predict(bank_churn_fit_rf, new_data = bank_churn_test_prep, type = &quot;prob&quot;)
bank_churn_prob_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1   0.726  0.274 
##  2   0.835  0.165 
##  3   0.870  0.130 
##  4   0.202  0.798 
##  5   0.968  0.0320
##  6   0.701  0.299 
##  7   0.298  0.702 
##  8   0.851  0.149 
##  9   0.964  0.0360
## 10   0.583  0.417 
## # ... with 1,905 more rows</code></pre>
<p>The table above shows the corresponding predicted probabilities using the test data set.</p>
<p><strong>Combining results</strong></p>
<pre class="r"><code>bank_churn_results_rf &lt;- bank_churn_test_prep %&gt;%
  bind_cols(bank_churn_class_preds_rf, bank_churn_prob_preds_rf)
bank_churn_results_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 15
##    CreditScore    Age Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.325   0.495 -1.05   -1.21         -0.940          0.0257 1     
##  2     -0.0542  0.724  1.03    0.609         0.873          0.869  1     
##  3      1.79    1.41   0.684  -1.21          0.873         -1.56   0     
##  4      0.0290  2.32  -1.40    0.911        -0.940         -1.65   1     
##  5      0.788  -1.56   0.337  -1.21          0.873         -0.786  0     
##  6      1.10   -0.190 -1.05    0.978        -0.940          1.22   0     
##  7     -0.616   0.153 -0.703  -1.21          2.69           0.707  1     
##  8     -1.01    0.381  1.38    0.551         0.873         -0.313  0     
##  9     -1.35    0.495  0.337  -1.21          0.873         -1.14   0     
## 10     -1.82    0.838 -1.74    0.937        -0.940         -1.25   1     
## # ... with 1,905 more rows, and 8 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;, .pred_class &lt;fct&gt;, .pred_0 &lt;dbl&gt;, .pred_1 &lt;dbl&gt;</code></pre>
<p>The procedure above combined the predicted outcomes and their probabilities into the preprocessed test data set.</p>
<p><strong>Assessing model fit using confusion matrix</strong></p>
<pre class="r"><code>bank_churn_results_rf %&gt;%
  conf_mat(truth = Exited, estimate = .pred_class) %&gt;%
autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
<p><strong>Correct predictions of test set</strong>
The total of True Negatives is 1463, that is, the number of customers who churned. True positive is 177 people, who was a loyal customer and did not churn.</p>
<p><strong>Classification error of test set</strong>
In total, 73 people were predicted as not churned, but actually churned (False Positives). Also, 204 people were predicted as churned, but actually did not churn and were loyal to bank (False Negatives).</p>
<p><strong>Combining models and recipe</strong></p>
<pre class="r"><code>bank_churn_wkfl_rf&lt;- workflow() %&gt;%
  add_model(bank_churn_rf_model) %&gt;%
  add_recipe(bank_churn_recipe)
bank_churn_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 4
##   trees = 100
##   min_n = 10
## 
## Computational engine: ranger</code></pre>
<p>We combined the model and its recipe.</p>
<p><strong>Model fitting with workflow</strong></p>
<pre class="r"><code>bank_churn_wkfl_fit_rf &lt;- bank_churn_wkfl_rf %&gt;%
  last_fit(split = bank_churn_split)
bank_churn_wkfl_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.863 Preprocessor1_Model1
## 2 roc_auc  binary         0.846 Preprocessor1_Model1</code></pre>
<p><strong>Performance comparison among RF, LG, and SVM models</strong></p>
<p>The table above shows some performance metrics of the RF model on the testing set. RF model achieved an accuracy of 85.37% and a ROC AUC score of 85.48%. Compared to the Logistic Regression Model, which obtained an accuracy of 83.4% and a ROC AUC score of 81.7%, it is clear that RF model slightly outperformed it in both metrics. Also, we can see that RF model achieves a much better performance than SVM model, since the latter model reported an accuracy of 80.21% and a ROC AUC score of 59.89%.
For this reason, we selected RF model for further refinements.</p>
<p><strong>Collecting predictions</strong></p>
<pre class="r"><code>bank_churn_wkfl_preds_rf &lt;- bank_churn_wkfl_fit_rf %&gt;%
  collect_predictions()
bank_churn_wkfl_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.737  0.263      1 0           1      Preprocessor1_Mode~
##  2 train/test split   0.898  0.102      6 0           1      Preprocessor1_Mode~
##  3 train/test split   0.912  0.0882     7 0           0      Preprocessor1_Mode~
##  4 train/test split   0.197  0.803     16 1           1      Preprocessor1_Mode~
##  5 train/test split   0.986  0.0135    19 0           0      Preprocessor1_Mode~
##  6 train/test split   0.744  0.256     26 0           0      Preprocessor1_Mode~
##  7 train/test split   0.321  0.679     30 1           1      Preprocessor1_Mode~
##  8 train/test split   0.858  0.142     32 0           0      Preprocessor1_Mode~
##  9 train/test split   0.977  0.0230    33 0           0      Preprocessor1_Mode~
## 10 train/test split   0.516  0.484     35 0           1      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Confusion matrix</strong></p>
<pre class="r"><code>conf_mat(bank_churn_wkfl_preds_rf, truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-95-1.png" width="672" />
We could observe a slight reduction in both False Positives and False Negatives.</p>
<p><strong>Exploring custom metrics</strong></p>
<pre class="r"><code>bank_churn_metrics_rf &lt;- metric_set(roc_auc, yardstick::sens, yardstick::spec, accuracy)
bank_churn_wkfl_preds_rf %&gt;%
bank_churn_metrics_rf(truth = Exited, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.958
## 2 spec     binary         0.478
## 3 accuracy binary         0.863
## 4 roc_auc  binary         0.154</code></pre>
</div>
</div>
<div id="creating-k-fold-cross-validation" class="section level1">
<h1>13. Creating <em>k</em>-Fold Cross Validation</h1>
<p>We tried to improve the RF model fitting using cross validation with 10 folds. since observations of response variable <em>Exited</em> were imbalanced, we decided to balance its distribution by using the “strata” argument which causes the random sampling to be conducted within the stratification variable. This parameter ensures that the number of data points in the training data is equivalent to the proportions in the original data set. We used 10 folds as input parameter.</p>
<pre class="r"><code>set.seed(222)
bank_churn_folds_rf &lt;- vfold_cv(bank_churn_training, v = 10, strata = Exited)
bank_churn_folds_rf</code></pre>
<pre><code>## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits             id    
##    &lt;list&gt;             &lt;chr&gt; 
##  1 &lt;split [6887/767]&gt; Fold01
##  2 &lt;split [6888/766]&gt; Fold02
##  3 &lt;split [6888/766]&gt; Fold03
##  4 &lt;split [6889/765]&gt; Fold04
##  5 &lt;split [6889/765]&gt; Fold05
##  6 &lt;split [6889/765]&gt; Fold06
##  7 &lt;split [6889/765]&gt; Fold07
##  8 &lt;split [6889/765]&gt; Fold08
##  9 &lt;split [6889/765]&gt; Fold09
## 10 &lt;split [6889/765]&gt; Fold10</code></pre>
<p><strong>Model training with cross validation</strong></p>
<pre class="r"><code>bank_churn_rs_fit_rf &lt;-bank_churn_wkfl_rf %&gt;%
  fit_resamples(resamples = bank_churn_folds_rf, metrics = bank_churn_metrics_rf)
bank_churn_rs_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.862    10 0.00345 Preprocessor1_Model1
## 2 roc_auc  binary     0.852    10 0.00611 Preprocessor1_Model1
## 3 sens     binary     0.963    10 0.00293 Preprocessor1_Model1
## 4 spec     binary     0.452    10 0.00875 Preprocessor1_Model1</code></pre>
<p>The cross-validation training resulted in an average accuracy of 0.861, an average sensitivity of 0.447, an average specificity of 0.447, and an average ROC AUC score of 0.848.</p>
<p><strong>Detailed cross_validation results</strong></p>
<pre class="r"><code>bank_churn_rs_metrics_rf &lt;-bank_churn_rs_fit_rf %&gt;%
  collect_metrics(summarize = FALSE)
bank_churn_rs_metrics_rf </code></pre>
<pre><code>## # A tibble: 40 x 5
##    id     .metric  .estimator .estimate .config             
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
##  1 Fold01 sens     binary         0.971 Preprocessor1_Model1
##  2 Fold01 spec     binary         0.454 Preprocessor1_Model1
##  3 Fold01 accuracy binary         0.868 Preprocessor1_Model1
##  4 Fold01 roc_auc  binary         0.862 Preprocessor1_Model1
##  5 Fold02 sens     binary         0.951 Preprocessor1_Model1
##  6 Fold02 spec     binary         0.414 Preprocessor1_Model1
##  7 Fold02 accuracy binary         0.845 Preprocessor1_Model1
##  8 Fold02 roc_auc  binary         0.832 Preprocessor1_Model1
##  9 Fold03 sens     binary         0.977 Preprocessor1_Model1
## 10 Fold03 spec     binary         0.5   Preprocessor1_Model1
## # ... with 30 more rows</code></pre>
<p><strong>Summarizing cross validation results</strong></p>
<pre class="r"><code>bank_churn_rs_metrics_rf %&gt;%
  group_by(.metric) %&gt;%
  summarize(min= min(.estimate),
            median = median(.estimate),
            max = max(.estimate),
            sd = sd(.estimate))</code></pre>
<pre><code>##         min    median       max        sd
## 1 0.4144737 0.8568627 0.9771987 0.1987805</code></pre>
</div>
<div id="hyper-parameter-tuning" class="section level1">
<h1>14. Hyper Parameter Tuning</h1>
<p>In addition to using a fixed set of Random Forest parameters during the training process, we also performed parameter tuning to find the optimal set. In particular, we considered 3 parameters that aim at controlling the model’s complexity: the number of trees (<em>tree</em>), the smallest node size allowed (<em>min_n</em>), and the number of predictors seen at each node (<em>mtry</em>). Four different metrics were considered: accuracy, sensitivity, specificity, and the area under the ROC curve. The latter was used to select the best model. We show below all the technical pipeline to perform hyper parameter tuning.</p>
<pre class="r"><code>bank_churn_rf_tune_model &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)
bank_churn_rf_tune_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
<p><strong>Creating tuning workflow</strong></p>
<pre class="r"><code>bank_churn_tune_wkfl_rf &lt;- bank_churn_wkfl_rf %&gt;%
  update_model(bank_churn_rf_tune_model)
bank_churn_tune_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
<p><strong>Identifying hyperparameters</strong></p>
<pre class="r"><code>parameters(bank_churn_rf_tune_model)</code></pre>
<pre><code>## Collection of 3 parameters for tuning
## 
##  identifier  type    object
##        mtry  mtry nparam[?]
##       trees trees nparam[+]
##       min_n min_n nparam[+]
## 
## Model parameters needing finalization:
##    # Randomly Selected Predictors (&#39;mtry&#39;)
## 
## See `?dials::finalize` or `?dials::update.parameters` for more information.</code></pre>
<p><strong>Hyper parameter tuning with cross validation</strong></p>
<pre class="r"><code>bank_churn_rf_tuning &lt;- bank_churn_tune_wkfl_rf %&gt;%
  tune_grid(resamples= bank_churn_folds_rf,  metrics = bank_churn_metrics_rf)</code></pre>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<pre class="r"><code>bank_churn_rf_tuning</code></pre>
<pre><code>## # Tuning results
## # 10-fold cross-validation using stratification 
## # A tibble: 10 x 4
##    splits             id     .metrics          .notes          
##    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          
##  1 &lt;split [6887/767]&gt; Fold01 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  2 &lt;split [6888/766]&gt; Fold02 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  3 &lt;split [6888/766]&gt; Fold03 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  4 &lt;split [6889/765]&gt; Fold04 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  5 &lt;split [6889/765]&gt; Fold05 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  6 &lt;split [6889/765]&gt; Fold06 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  7 &lt;split [6889/765]&gt; Fold07 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  8 &lt;split [6889/765]&gt; Fold08 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  9 &lt;split [6889/765]&gt; Fold09 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
## 10 &lt;split [6889/765]&gt; Fold10 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;</code></pre>
<p><strong>Exploring tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 40 x 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config            
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              
##  1     6   374    30 accuracy binary     0.863    10 0.00311 Preprocessor1_Mode~
##  2     6   374    30 roc_auc  binary     0.853    10 0.00626 Preprocessor1_Mode~
##  3     6   374    30 sens     binary     0.962    10 0.00307 Preprocessor1_Mode~
##  4     6   374    30 spec     binary     0.463    10 0.00917 Preprocessor1_Mode~
##  5     6  1474    21 accuracy binary     0.863    10 0.00271 Preprocessor1_Mode~
##  6     6  1474    21 roc_auc  binary     0.853    10 0.00630 Preprocessor1_Mode~
##  7     6  1474    21 sens     binary     0.961    10 0.00236 Preprocessor1_Mode~
##  8     6  1474    21 spec     binary     0.467    10 0.00946 Preprocessor1_Mode~
##  9    10  1621    33 accuracy binary     0.862    10 0.00284 Preprocessor1_Mode~
## 10    10  1621    33 roc_auc  binary     0.849    10 0.00621 Preprocessor1_Mode~
## # ... with 30 more rows</code></pre>
<p>The table above shows a sample of the different combinations tested during the process.</p>
<p><strong>Detailed tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE)</code></pre>
<pre><code>## # A tibble: 400 x 8
##    id      mtry trees min_n .metric  .estimator .estimate .config              
##    &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1 Fold01     6   374    30 sens     binary         0.976 Preprocessor1_Model01
##  2 Fold01     6   374    30 spec     binary         0.474 Preprocessor1_Model01
##  3 Fold01     6   374    30 accuracy binary         0.876 Preprocessor1_Model01
##  4 Fold01     6   374    30 roc_auc  binary         0.866 Preprocessor1_Model01
##  5 Fold02     6   374    30 sens     binary         0.950 Preprocessor1_Model01
##  6 Fold02     6   374    30 spec     binary         0.447 Preprocessor1_Model01
##  7 Fold02     6   374    30 accuracy binary         0.850 Preprocessor1_Model01
##  8 Fold02     6   374    30 roc_auc  binary         0.832 Preprocessor1_Model01
##  9 Fold03     6   374    30 sens     binary         0.977 Preprocessor1_Model01
## 10 Fold03     6   374    30 spec     binary         0.487 Preprocessor1_Model01
## # ... with 390 more rows</code></pre>
<p><strong>Exploring tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE) %&gt;%
  filter(.metric == &#39;roc_auc&#39;) %&gt;%
  group_by (id) %&gt;%
  summarize( min_roc_auc = min(.estimate),
             median_roc_auc = median(.estimate),
             max_roc_auc = max(.estimate))</code></pre>
<pre><code>##   min_roc_auc median_roc_auc max_roc_auc
## 1   0.8059301       0.852347    0.884622</code></pre>
<p>As mentioned earlier, optimal RF parameters were selected based on their ROC AUC score. The minimum ROC AUC score found during the process was of 0.791, the median was of 0.849, and the maximum score was of 0.876.</p>
<p><strong>Viewing the best performing model</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  show_best(metric = &#39;roc_auc&#39;, n =5)</code></pre>
<pre><code>## # A tibble: 5 x 9
##    mtry trees min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1     4   839    37 roc_auc binary     0.856    10 0.00632 Preprocessor1_Model06
## 2     3  1969    11 roc_auc binary     0.856    10 0.00611 Preprocessor1_Model07
## 3     3   427    17 roc_auc binary     0.856    10 0.00615 Preprocessor1_Model05
## 4     6   374    30 roc_auc binary     0.853    10 0.00626 Preprocessor1_Model01
## 5     6  1474    21 roc_auc binary     0.853    10 0.00630 Preprocessor1_Model02</code></pre>
<p>The best parameter set for the RF model was <em>mtry</em> = 4, <em>trees</em> = 839, and <em>min_n</em> = 37, with an average ROC AUC score of 0.852 and a standard deviation of 0.00620. The best model is Preprocessor1_Model06.</p>
</div>
<div id="best-model-selection-with-parameter" class="section level1">
<h1>15. Best Model Selection with Parameter</h1>
<p>In this section we train and test our best model, and compare its performance against the Logistic Regression and Support Vector Machine models.</p>
<pre class="r"><code>bank_churn_best_rf_model &lt;- bank_churn_rf_tuning %&gt;%
  select_best(metric = &#39;roc_auc&#39;)
bank_churn_best_rf_model </code></pre>
<pre><code>## # A tibble: 1 x 4
##    mtry trees min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1     4   839    37 Preprocessor1_Model06</code></pre>
<p>The table above shows the best set of parameters found.</p>
<p><strong>Finalizing the workflow</strong></p>
<pre class="r"><code>final_bank_churn_wkfl_rf &lt;- bank_churn_tune_wkfl_rf %&gt;%
  finalize_workflow(bank_churn_best_rf_model)
final_bank_churn_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 4
##   trees = 839
##   min_n = 37
## 
## Computational engine: ranger</code></pre>
<p><strong>Model fitting</strong></p>
<pre class="r"><code>bank_churn_final_fit_rf &lt;- final_bank_churn_wkfl_rf %&gt;%
  last_fit(split = bank_churn_split)
bank_churn_final_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.864 Preprocessor1_Model1
## 2 roc_auc  binary         0.856 Preprocessor1_Model1</code></pre>
<p>We can see above that accuracy and ROC AUC score (86.3% &amp; 85.3%, respectively) are improved compared to the model without tuning.</p>
<p><strong>collecting predictions</strong></p>
<pre class="r"><code>bank_churn_prediction_rf&lt;- bank_churn_final_fit_rf %&gt;%
  collect_predictions()
bank_churn_prediction_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.733  0.267      1 0           1      Preprocessor1_Mode~
##  2 train/test split   0.865  0.135      6 0           1      Preprocessor1_Mode~
##  3 train/test split   0.893  0.107      7 0           0      Preprocessor1_Mode~
##  4 train/test split   0.138  0.862     16 1           1      Preprocessor1_Mode~
##  5 train/test split   0.976  0.0243    19 0           0      Preprocessor1_Mode~
##  6 train/test split   0.737  0.263     26 0           0      Preprocessor1_Mode~
##  7 train/test split   0.338  0.662     30 1           1      Preprocessor1_Mode~
##  8 train/test split   0.862  0.138     32 0           0      Preprocessor1_Mode~
##  9 train/test split   0.955  0.0446    33 0           0      Preprocessor1_Mode~
## 10 train/test split   0.536  0.464     35 0           1      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Confusion Matrix</strong></p>
<pre class="r"><code>conf_mat(bank_churn_prediction_rf, truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
<p><strong>Correct predictions of test set</strong>
The number of True Negatives is 1482 customers, who churned. Furthermore, the number of True Positives is 171 people, who were loyal customers and did not churn.</p>
<p><strong>Classification error of test set</strong>
The number of False Positives is 54 customers, who were predicted as not churned but actually churned. Also, the number of False Negatives is 208. Such customers were predicted as churned but actually did not churn and remained loyal to the bank.</p>
<p>Almost 80% of the customers churned</p>
<p><strong>ROC Curve</strong></p>
<pre class="r"><code>roc.rf &lt;- roc.curve(bank_churn_test_prep$Exited, bank_churn_prediction_rf$.pred_class, curve=TRUE)
plot(roc.rf)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-114-1.png" width="672" />
The roc-auc has improved and now is 54%</p>
</div>
<div id="model-comparison" class="section level1">
<h1>16. Model Comparison</h1>
<p>The ROC curves for the Random Forest, Support Vector Machine, and Logistic Regression Models are presented below. Among all models, Logistic Regression achieved the worst performance. The performance of Random Forest and SVM resulted similar in this regard, but when considering the other metrics, RF outperforms SVM.</p>
<div id="a-roc-curve-plot-of-random-forest-model" class="section level4">
<h4>A) ROC curve plot of random forest model</h4>
<pre class="r"><code>plot(roc.rf)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-115-1.png" width="672" />
#### B) ROC curve plot of SVM model</p>
<pre class="r"><code>plot(roc.svm)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-116-1.png" width="672" /></p>
</div>
<div id="c-roc-curve-plot-of-logistic-model" class="section level4">
<h4>C) ROC curve plot of logistic model</h4>
<pre class="r"><code>plot(roc.glm)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-117-1.png" width="672" /></p>
</div>
</div>
<div id="observations-and-comments" class="section level1">
<h1>17. Observations and Comments</h1>
<ol style="list-style-type: lower-alpha">
<li><p>The important determinants to predict <em>Exited</em> are <em>Gender</em>, <em>Age</em>, <em>Balance</em>, <em>NumOfProducts</em> and <em>IsActiveMember</em>.</p></li>
<li><p>The character integer variables were assigned as factors.</p></li>
<li><p>stepAIC backward regression and boot strapping were used to select the best model.</p></li>
<li><p>Almost 80% of the customers churned and only 20% of the customers were loyal and stayed with the bank.</p></li>
<li><p>The accuracy, sensitivity and specificity are better in Random Forest model compared to Logistic Regression and Support Vector Machine though ROC AUC score is slightly better in SVM model.</p></li>
<li><p>Variables such as <em>Age</em>, <em>CreditScore</em>, <em>NumOfProducts</em> had some outliers and were removed from the data before further processing in model.</p></li>
<li><p>We performed univariate analysis, bivariate analysis, bootstrapping, stepAIC, feature engineering, cross validation, hyper parameter tuning, random forest model, support vector machine, and logistic regression model on banking churn data set.</p></li>
<li><p>When we did bivariate analysis on independent and dependent variables. Only the best model variables such as <em>Gender</em>, <em>Age</em>, <em>Balance</em>, <em>NumOfProducts</em> and <em>IsActiveMember</em> came as 99.9% significant and other variables are not fully significant. The same significant variables turned to a best model predictors as well for a whole model.</p></li>
<li><p>The stepAIC and bootstrapping selected <em>Geography</em>, <em>Tenure</em>, <em>Gender</em>, <em>Age</em>, <em>Balance</em>, <em>NumOfProducts</em> and <em>IsActiveMember</em> but when we run <em>Geography</em> and <em>Tenure</em> resulted not significant, and therefore were removed from the final model.</p></li>
<li><p>We used future engineering to preprocess the training and testing data such as we normalized all numeric variables and set dummy variables to all character variables.</p></li>
<li><p>Some important things for bank in bivariate analysis are the following:</p>
<ol style="list-style-type: lower-roman">
<li>The group of customers that mostly churned are between 30 and 40 years old. The reason for this might be that the product is not tuned to their expectations.</li>
</ol></li>
</ol>
<ol start="2" style="list-style-type: lower-roman">
<li><p>More customers who churned had a credit score between 600 and 700. The reason could be that other banks might lend money easier to this people, whereas this bank might not give a loan to people with less than 700 in credit score. Since no details are provided, this was our assumption.</p></li>
<li><p>The customers having 3 accounts had less churn rate compared to customers having 1 or 2 accounts with bank. As a recommendation, the bank should try to sell as much product as possible and do better service to retain more customers.</p></li>
<li><p>Though geography did not come as a determinant to predict the customer`s churning rate, France had the highest churn rate of 4798 customers, which is almost half of the churning rate of the bank.</p></li>
<li><p>Churning rate was higher with male customers than with female customers. The Bank should come up with some unique product for male members to retain them.</p></li>
<li><p>It is surprising to notice that actively transacted customers(86%) have high percentage of churn rate against customers who have not transacted actively (75%). This shows the poor service provided by the bank. Hence, bank should look into this immediately and resolve the service problem and give a better service to retain them. Retaining the existing customer is as important as getting a new customer.</p></li>
</ol>
</div>
