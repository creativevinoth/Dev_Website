---
title: Bank_Churn_Data
author: ''
date: '2021-11-30'
slug: bank-churn-data
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><strong>A report to predict the churning of bank customers</strong></p>
<p>We have used logistic regression, support vector machine and randomforest to choose the best model to predict the churning of bank customers</p>
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<p>Executive Summary</p>
<p>Issues and Challenges</p>
<p>Descriptive statistics and pre-processing</p>
<p>Univariate analysis on metric and non metric data</p>
<p>Bivariate analysis</p>
<p>Best model using bootstrap and logistic regression</p>
<p>Data re sampling and Feature engineering</p>
<p>Support vector machine model</p>
<p>Logistic regression model and its workflow</p>
<p>Random forest model and its workflow</p>
<p>Observation and comments</p>
</div>
<div id="executive-summary" class="section level1">
<h1>Executive summary</h1>
<p>A bank was interested to find out variables that was significant to predict the churning of the customers so as to make sure that customers are satisfied with all aspects and convinced to stay with a bank. The bank was shocked to see that the churning rate of customer have increased dramatically. To stop this churning trend, bank has to first find out the determinants of churning rate and initiate some steps to avoid such highest churning rate in future. It is hard to get a new client, so it is better to retain the existing customers.</p>
<p>There are total 12 independent variable such as RowNumber, customerid, credit score, geography, gender, age, tenure, balance, numofproucts, hasCrCard, IsActiveMember and estimated salary and 1 dependent variable, exited. It was veru clear that row number pertains to the counting of the data row wise and customerid was to represent the individual customer details. These 2 variables will not have any influence upon the prediction of exited. So, we removed those 2 variables from our model even before taking for analysis. We have analyzed the individual data in to two categories such as numerical and categorical data. We have found some outliers in some of the numerical variables data such as age, credit score, numOfProducts and removed its outliers using subset function. We have analyzed the categorical data using frequency distribution.</p>
<p>We have done bivariate analysis on the dependent and independent variables to see its relationship and its predicting significance using logistic regression, glm function. We have visualized the same using ggplot.</p>
<p>We have used bootstrapping and used stepAIC backward regression to find out the best predicting model but some of the coefficients are not significant though it was selected by both the methods.Thus, we removed the bootstrap selected geography and tenure variables from our model to increase the prediction power and its significance to 99.9%. Our final model turned out to be gender, age, balance, numofproducts and isactivemember to predict the exited at 99.9% significance level.</p>
<p>We have done data re-sampling using 80:20 split as training and test data. We did feature engineering to preprocess the data. Then, we trained the training the model using, support vector machine, logistic regression workflow model and random forest model. After validating the results using confusion matrix and its accuracy and roc-auc, random forest model turned out to be a best predicting model. Hence, we did cross validation and hyper parameter tuning on our random forest model to further enhance the predicting power. our best selected random forest parameter was mtry (4), trees(839) and min_n (37). To automate the model to predict the new data set, we trained the random forest model, then cross-validated and hyper tuned. We finalized the random forest workflow model using finalize_workflow function for predicting new data set.</p>
<p>There was a evident from all the models that around 80% of customers churned. Bank has to encourage customers to be actively transacting the account by offering some cash back opportunities for transacting. When the customers use the service and get happy, definitely they will stay with them and get more products from the same bank and try to maintain more balance, which all will eventually lead to increase in customers<code>retention rate and reduction in customers</code> churning rate.</p>
</div>
<div id="issues-and-challanges" class="section level1">
<h1>Issues and challanges</h1>
<p>The greatest challenge was to figure out which all the variables play a role in predicting the response variable, exited. The exited, hascrcard and isactivemember were treated as integers automatically but they were a categorical values. We converted them as a factor before processing into a model. The balance and salary were high in numbers and others were small in numbers. We normalized balance and salary before processing to get a better prediction. When we hyper tuned and cross validated random forest model, the processing of file was longer.</p>
</div>
<div id="descriptive-statistics-and-preprocessing" class="section level1">
<h1>Descriptive statistics and preprocessing</h1>
<div id="i-summary" class="section level4">
<h4>(i) Summary</h4>
<pre class="r"><code>summary(Bank_Churn_Data)</code></pre>
<pre><code>##    RowNumber       CustomerId        CreditScore     Geography        
##  Min.   :    1   Min.   :15565701   Min.   :350.0   Length:10000      
##  1st Qu.: 2501   1st Qu.:15628528   1st Qu.:584.0   Class :character  
##  Median : 5000   Median :15690738   Median :652.0   Mode  :character  
##  Mean   : 5000   Mean   :15690941   Mean   :650.5                     
##  3rd Qu.: 7500   3rd Qu.:15753234   3rd Qu.:718.0                     
##  Max.   :10000   Max.   :15815690   Max.   :850.0                     
##     Gender               Age            Tenure          Balance      
##  Length:10000       Min.   :18.00   Min.   : 0.000   Min.   :     0  
##  Class :character   1st Qu.:32.00   1st Qu.: 3.000   1st Qu.:     0  
##  Mode  :character   Median :37.00   Median : 5.000   Median : 97199  
##                     Mean   :38.92   Mean   : 5.013   Mean   : 76486  
##                     3rd Qu.:44.00   3rd Qu.: 7.000   3rd Qu.:127644  
##                     Max.   :92.00   Max.   :10.000   Max.   :250898  
##  NumOfProducts    HasCrCard      IsActiveMember   EstimatedSalary 
##  Min.   :1.00   Min.   :0.0000   Min.   :0.0000   Min.   :    12  
##  1st Qu.:1.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 51002  
##  Median :1.00   Median :1.0000   Median :1.0000   Median :100194  
##  Mean   :1.53   Mean   :0.7055   Mean   :0.5151   Mean   :100090  
##  3rd Qu.:2.00   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:149388  
##  Max.   :4.00   Max.   :1.0000   Max.   :1.0000   Max.   :199992  
##      Exited      
##  Min.   :0.0000  
##  1st Qu.:0.0000  
##  Median :0.0000  
##  Mean   :0.2037  
##  3rd Qu.:0.0000  
##  Max.   :1.0000</code></pre>
<p>We can remove 2 variables rownumber and customerid as they are related to individual and will not have influence to predict the churning of customers.</p>
<p><strong>Removing rownumber and customerid</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, select = - RowNumber)
Bank_Churn_Data &lt;- subset (Bank_Churn_Data, select = - CustomerId)</code></pre>
</div>
<div id="ii-structure-of-data" class="section level4">
<h4>(ii) Structure of data</h4>
<pre class="r"><code>str(Bank_Churn_Data)</code></pre>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  11 variables:
##  $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...
##  $ Geography      : chr  &quot;France&quot; &quot;Spain&quot; &quot;France&quot; &quot;France&quot; ...
##  $ Gender         : chr  &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ...
##  $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...
##  $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...
##  $ Balance        : int  0 83808 159661 0 125511 113756 0 115047 142051 134604 ...
##  $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...
##  $ HasCrCard      : int  1 0 1 0 1 1 1 1 0 1 ...
##  $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...
##  $ EstimatedSalary: int  101349 112543 113932 93827 79084 149757 10063 119347 74941 71726 ...
##  $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...</code></pre>
<p>The exited, hascrcard and isactivemember are categorical value but wrongly taken as integer variable</p>
<p><strong>Changing the variable from int to character</strong></p>
<pre class="r"><code>Bank_Churn_Data$HasCrCard = factor(Bank_Churn_Data$HasCrCard)
Bank_Churn_Data$IsActiveMember = factor(Bank_Churn_Data$IsActiveMember)
Bank_Churn_Data$Exited  = factor(Bank_Churn_Data$Exited )</code></pre>
</div>
<div id="iii-descriptive-statistics" class="section level4">
<h4>(iii) Descriptive statistics</h4>
<pre class="r"><code>descr(Bank_Churn_Data)</code></pre>
<pre><code>## Non-numerical variable(s) ignored: Geography, Gender, HasCrCard, IsActiveMember, Exited</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data  
## N: 10000  
## 
##                          Age     Balance   CreditScore   EstimatedSalary   NumOfProducts     Tenure
## ----------------- ---------- ----------- ------------- ----------------- --------------- ----------
##              Mean      38.92    76485.90        650.53         100090.24            1.53       5.01
##           Std.Dev      10.49    62397.41         96.65          57510.49            0.58       2.89
##               Min      18.00        0.00        350.00             12.00            1.00       0.00
##                Q1      32.00        0.00        584.00          50993.00            1.00       3.00
##            Median      37.00    97198.50        652.00         100193.50            1.00       5.00
##                Q3      44.00   127646.00        718.00         149392.00            2.00       7.00
##               Max      92.00   250898.00        850.00         199992.00            4.00      10.00
##               MAD       8.90    69336.01         99.33          72941.70            0.00       2.97
##               IQR      12.00   127644.00        134.00          98386.00            1.00       4.00
##                CV       0.27        0.82          0.15              0.57            0.38       0.58
##          Skewness       1.01       -0.14         -0.07              0.00            0.75       0.01
##       SE.Skewness       0.02        0.02          0.02              0.02            0.02       0.02
##          Kurtosis       1.39       -1.49         -0.43             -1.18            0.58      -1.17
##           N.Valid   10000.00    10000.00      10000.00          10000.00        10000.00   10000.00
##         Pct.Valid     100.00      100.00        100.00            100.00          100.00     100.00</code></pre>
<p>The above are the descriptive statistics for numerical values. We can find a huge standard deviation for some of the variables such as balance, estimated salary.</p>
</div>
<div id="iv-revised-structure-of-data" class="section level4">
<h4>(iv) Revised structure of data</h4>
<pre class="r"><code>str(Bank_Churn_Data)</code></pre>
<pre><code>## &#39;data.frame&#39;:    10000 obs. of  11 variables:
##  $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...
##  $ Geography      : chr  &quot;France&quot; &quot;Spain&quot; &quot;France&quot; &quot;France&quot; ...
##  $ Gender         : chr  &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ...
##  $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...
##  $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...
##  $ Balance        : int  0 83808 159661 0 125511 113756 0 115047 142051 134604 ...
##  $ NumOfProducts  : int  1 1 3 2 1 2 2 4 2 1 ...
##  $ HasCrCard      : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 2 2 2 2 1 2 ...
##  $ IsActiveMember : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 1 1 2 1 2 1 2 2 ...
##  $ EstimatedSalary: int  101349 112543 113932 93827 79084 149757 10063 119347 74941 71726 ...
##  $ Exited         : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 1 1 2 1 2 1 1 ...</code></pre>
<p>The above are the revised structure of the data after some of the variables have been converted as a factor from the wrongly assigned integer values initially.</p>
</div>
</div>
<div id="univariate-analysis-on-numerical-data" class="section level1">
<h1>Univariate analysis on numerical data</h1>
<p>We analyse here all the numerical data individually to find out its outliers and remove those values.</p>
<div id="univariate-analysis-on-age" class="section level4">
<h4>1. Univariate analysis on Age</h4>
<pre class="r"><code>descr(Bank_Churn_Data$Age)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Age  
## N: 10000  
## 
##                          Age
## ----------------- ----------
##              Mean      38.92
##           Std.Dev      10.49
##               Min      18.00
##                Q1      32.00
##            Median      37.00
##                Q3      44.00
##               Max      92.00
##               MAD       8.90
##               IQR      12.00
##                CV       0.27
##          Skewness       1.01
##       SE.Skewness       0.02
##          Kurtosis       1.39
##           N.Valid   10000.00
##         Pct.Valid     100.00</code></pre>
<p>The standard deviation of age is 10.49. The average age is 38. There is a positive skewneww of 1.01.</p>
<p><strong>Boxplot of age and its outliers</strong></p>
<pre class="r"><code>Age_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Age)$out
boxplot(Bank_Churn_Data$Age, main = &quot;Box plot of age&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Age_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>Age_outliers_churn</code></pre>
<pre><code>##   [1] 66 75 65 73 65 72 67 67 79 80 68 75 66 66 70 63 72 64 64 70 67 82 63 69 65
##  [26] 69 64 65 74 67 66 67 63 70 71 72 67 74 76 66 63 66 68 67 63 71 66 69 73 65
##  [51] 66 64 69 64 77 74 65 70 67 69 67 74 69 74 74 64 63 63 70 74 65 72 77 66 65
##  [76] 74 88 63 71 63 64 67 70 68 72 71 66 75 67 73 69 76 63 85 67 74 76 66 69 66
## [101] 72 63 71 63 74 67 72 72 66 84 71 66 63 74 69 84 67 64 68 66 77 70 67 79 67
## [126] 76 73 66 67 64 73 76 72 64 71 63 70 65 66 65 80 66 63 63 63 63 66 74 69 63
## [151] 64 76 75 68 69 77 64 66 74 71 67 68 64 68 70 64 75 66 64 78 65 74 64 64 71
## [176] 77 79 70 81 64 68 68 63 79 66 64 70 69 71 72 66 68 63 71 72 72 64 78 75 65
## [201] 65 67 63 68 71 73 64 66 71 69 71 66 76 69 73 64 64 75 73 71 72 63 67 68 73
## [226] 67 64 63 92 65 75 67 71 64 66 64 66 67 77 92 67 63 66 66 68 65 72 71 76 63
## [251] 67 67 66 67 63 65 70 72 77 74 72 73 77 67 71 64 72 81 76 69 68 74 64 64 71
## [276] 68 63 67 63 64 76 63 63 68 67 72 70 81 67 73 66 68 71 66 63 75 69 64 69 70
## [301] 71 71 66 70 63 64 65 63 67 71 67 65 66 63 73 66 64 72 71 69 67 64 81 73 63
## [326] 67 74 83 69 71 78 63 70 69 72 70 63 74 80 69 72 67 76 71 67 71 78 63 63 68
## [351] 64 70 78 69 68 64 64 77 77</code></pre>
<p>The above box plot shows that the age above 63 are outliers. so, we remove those values from the dataset</p>
<p><strong>Removal of outliers of age</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, Age &lt; 63)</code></pre>
<p>The outliers have been removed from the data set.</p>
</div>
<div id="univariate-analysis-on-balance" class="section level4">
<h4>2. Univariate analysis on Balance</h4>
<pre class="r"><code>descr(Bank_Churn_Data$Balance)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Balance  
## N: 9641  
## 
##                       Balance
## ----------------- -----------
##              Mean    76560.68
##           Std.Dev    62401.70
##               Min        0.00
##                Q1        0.00
##            Median    97318.00
##                Q3   127660.00
##               Max   250898.00
##               MAD    69044.68
##               IQR   127660.00
##                CV        0.82
##          Skewness       -0.14
##       SE.Skewness        0.02
##          Kurtosis       -1.49
##           N.Valid     9641.00
##         Pct.Valid      100.00</code></pre>
<p>The average balance is $76560 but there is a huge standard deviation of $62401. There is a negative skewness of -0.14, which is almost equal to o.</p>
<p><strong>Boxplot of Balance and its outliers</strong></p>
<pre class="r"><code>Balance_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Balance)$out
boxplot(Bank_Churn_Data$Balance, main = &quot;Box plot of Balance&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Balance_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>Balance_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>There are no outliers</p>
</div>
<div id="univariate-analysis-on-creditscore" class="section level4">
<h4>3. Univariate analysis on CreditScore</h4>
<pre class="r"><code>descr(Bank_Churn_Data$CreditScore)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$CreditScore  
## N: 9641  
## 
##                     CreditScore
## ----------------- -------------
##              Mean        650.29
##           Std.Dev         96.69
##               Min        350.00
##                Q1        583.00
##            Median        652.00
##                Q3        717.00
##               Max        850.00
##               MAD         99.33
##               IQR        134.00
##                CV          0.15
##          Skewness         -0.07
##       SE.Skewness          0.02
##          Kurtosis         -0.43
##           N.Valid       9641.00
##         Pct.Valid        100.00</code></pre>
<p>The average credit score is 650 and the stanard deviation is 96. There is a negative skewness of -0.07, which is almost equal to 0.</p>
<p><strong>Boxplot of CreditScore and its outliers</strong></p>
<pre class="r"><code>CreditScore_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$CreditScore)$out
boxplot(Bank_Churn_Data$CreditScore, main = &quot;Box plot of CreditScore&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(CreditScore_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>CreditScore_outliers_churn</code></pre>
<pre><code>##  [1] 376 376 363 359 350 350 358 351 365 367 350 350 373 350</code></pre>
<p>We can see that less than 376 credit score are outliers. So, we remove those values from the data set.</p>
<p><strong>Removal of outliers of CreditScore</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, CreditScore &gt; 376)</code></pre>
<p>The outliers have been removed.</p>
</div>
<div id="univariate-analysis-on-estimatedsalary" class="section level4">
<h4>4. Univariate analysis on EstimatedSalary</h4>
<pre class="r"><code>descr(Bank_Churn_Data$EstimatedSalary)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$EstimatedSalary  
## N: 9627  
## 
##                     EstimatedSalary
## ----------------- -----------------
##              Mean         100121.62
##           Std.Dev          57524.28
##               Min             12.00
##                Q1          51011.00
##            Median         100187.00
##                Q3         149458.00
##               Max         199992.00
##               MAD          72964.68
##               IQR          98424.50
##                CV              0.57
##          Skewness              0.00
##       SE.Skewness              0.02
##          Kurtosis             -1.18
##           N.Valid           9627.00
##         Pct.Valid            100.00</code></pre>
<p>The average estimated salary is $100121 and the standard deviation is $57524. There is no skewness.</p>
<p><strong>Boxplot of EstimatedSalary and its outliers</strong></p>
<pre class="r"><code>EstimatedSalary_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$EstimatedSalary)$out
boxplot(Bank_Churn_Data$EstimatedSalary, main = &quot;Box plot of EstimatedSalary&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(EstimatedSalary_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>EstimatedSalary_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>There are no outliers</p>
</div>
<div id="univariate-analysis-on-numofproducts" class="section level4">
<h4>5. Univariate analysis on NumOfProducts</h4>
<pre class="r"><code>descr(Bank_Churn_Data$NumOfProducts)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$NumOfProducts  
## N: 9627  
## 
##                     NumOfProducts
## ----------------- ---------------
##              Mean            1.53
##           Std.Dev            0.58
##               Min            1.00
##                Q1            1.00
##            Median            1.00
##                Q3            2.00
##               Max            4.00
##               MAD            0.00
##               IQR            1.00
##                CV            0.38
##          Skewness            0.74
##       SE.Skewness            0.02
##          Kurtosis            0.58
##           N.Valid         9627.00
##         Pct.Valid          100.00</code></pre>
<p>The average number of products held by customers are 1.53. There is a standard deviation of 0.58. There is a positive skewness of 0.74.</p>
<p><strong>Boxplot of NumOfProducts and its outliers</strong></p>
<pre class="r"><code>NumOfProducts_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$NumOfProducts)$out
boxplot(Bank_Churn_Data$NumOfProducts, main = &quot;Box plot of NumOfProducts&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(NumOfProducts_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>NumOfProducts_outliers_churn</code></pre>
<pre><code>##  [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
## [39] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4</code></pre>
<p>We can see that there are few people having 4 products and it is an outliers here. SO, we remove it from the data set.</p>
<p><strong>Removal of outliers of NumOfProducts</strong></p>
<pre class="r"><code>Bank_Churn_Data &lt;- subset (Bank_Churn_Data, NumOfProducts &lt; 4)</code></pre>
<p>The outliers have been removed.</p>
</div>
<div id="univariate-analysis-on-tenure" class="section level4">
<h4>6. Univariate analysis on Tenure</h4>
<pre class="r"><code>descr(Bank_Churn_Data$Tenure)</code></pre>
<pre><code>## Descriptive Statistics  
## Bank_Churn_Data$Tenure  
## N: 9569  
## 
##                      Tenure
## ----------------- ---------
##              Mean      5.01
##           Std.Dev      2.89
##               Min      0.00
##                Q1      3.00
##            Median      5.00
##                Q3      7.00
##               Max     10.00
##               MAD      2.97
##               IQR      4.00
##                CV      0.58
##          Skewness      0.01
##       SE.Skewness      0.03
##          Kurtosis     -1.16
##           N.Valid   9569.00
##         Pct.Valid    100.00</code></pre>
<p>The average tenure is 5.01 and the standard deviation of tenure is 2.89. The skewness is almost 0.</p>
<p><strong>Boxplot of Tenure and its outliers</strong></p>
<pre class="r"><code>Tenure_outliers_churn&lt;-boxplot.stats(Bank_Churn_Data$Tenure)$out
boxplot(Bank_Churn_Data$Tenure, main = &quot;Box plot of Tenure&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(Tenure_outliers_churn, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>Tenure_outliers_churn</code></pre>
<pre><code>## integer(0)</code></pre>
<p>There are no outliers for tenure</p>
</div>
</div>
<div id="univariate-analysis-on-categorical-values" class="section level1">
<h1>Univariate analysis on categorical values</h1>
<p>We analyzed the categorical values using frequency distribution to figure out least distribution among the distributed values.</p>
<div id="frequency-distribution-of-geography" class="section level4">
<h4>1. Frequency distribution of Geography</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Geography, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Geography&quot;, xlab =&quot;Geography&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Geography : 
##         Frequency Percent Cum. percent
## Spain        2373    24.8         24.8
## Germany      2398    25.1         49.9
## France       4798    50.1        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Almost half of the customers are from France and the remaining customers are equally distributed between Spain and Germany.</p>
</div>
<div id="frequency-distribution-of-gender" class="section level4">
<h4>2. Frequency distribution of Gender</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Gender, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Gender&quot;, xlab =&quot;Gender&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Gender : 
##         Frequency Percent Cum. percent
## Female       4332    45.3         45.3
## Male         5237    54.7        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>There are more male customers having account with bank compared to female members</p>
</div>
<div id="frequency-distribution-of-hascrcard" class="section level4">
<h4>3. Frequency distribution of HasCrCard</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$HasCrCard, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of HasCrCard&quot;, xlab =&quot;HasCrCard&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$HasCrCard : 
##         Frequency Percent Cum. percent
## 0            2821    29.5         29.5
## 1            6748    70.5        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Around 70% of customers have a credit card with bank and only 30% of people without credit card</p>
</div>
<div id="frequency-distribution-of-isactivemember" class="section level4">
<h4>4. Frequency distribution of IsActiveMember</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$IsActiveMember, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of IsActiveMember&quot;, xlab =&quot;IsActiveMember&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$IsActiveMember : 
##         Frequency Percent Cum. percent
## 0            4749    49.6         49.6
## 1            4820    50.4        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>Almost equal number of customers are both active and not active customers in bank.</p>
</div>
<div id="frequency-distribution-of-exited" class="section level4">
<h4>5. Frequency distribution of Exited</h4>
<pre class="r"><code>tab1(Bank_Churn_Data$Exited , sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of Exited &quot;, xlab =&quot;Exited &quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre><code>## Bank_Churn_Data$Exited : 
##         Frequency Percent Cum. percent
## 1            1892    19.8         19.8
## 0            7677    80.2        100.0
##   Total      9569   100.0        100.0</code></pre>
<p>We could see from the above frequency distribution that only 20% of customers was a loyal customer and approximately 80% of customers closed the bank account</p>
</div>
</div>
<div id="bivariate-analysis" class="section level1">
<h1>Bivariate analysis</h1>
<p>Here, we will analyze the relationship between response and explanatory variables.</p>
<div id="relationship-between-age-and-exited" class="section level4">
<h4>1. Relationship between Age and Exited</h4>
<pre class="r"><code>Age_Exited = glm(Exited~ Age, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Age_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Age, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5677  -0.6369  -0.4780  -0.3213   2.7153  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -5.519707   0.137290  -40.20   &lt;2e-16 ***
## Age          0.103265   0.003233   31.94   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 8334.7  on 9567  degrees of freedom
## AIC: 8338.7
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>There is a 99.9% significant relationship between age and exited</p>
<pre class="r"><code>discretized.Age = cut(Bank_Churn_Data$Age, c(0, 10, 20, 30, 40, 50, 60,70))
ggplot(Bank_Churn_Data, aes(x = discretized.Age , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="672" />
The more number of customers churned are between the age 30 and 40.</p>
</div>
<div id="relationship-between-balance-and-exited" class="section level4">
<h4>2. Relationship between Balance and Exited</h4>
<pre class="r"><code>Balance_Exited = glm(Exited~ Balance, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Balance_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Balance, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8838  -0.7175  -0.5558  -0.5558   1.9718  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.790e+00  4.535e-02  -39.46   &lt;2e-16 ***
## Balance      4.745e-06  4.260e-07   11.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9387.9  on 9567  degrees of freedom
## AIC: 9391.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.9% significant relationship between balance and exited.</p>
<pre class="r"><code>discretized.Balance = cut(Bank_Churn_Data$Balance, c(0, 50000, 100000, 150000, 200000, 250898),dig.lab = 1000)</code></pre>
<pre><code>## Warning in formatC(0 + breaks, digits = dig, width = 1L): &#39;digits&#39; reduced to 50</code></pre>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = discretized.Balance , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
</div>
<div id="relationship-between-creditscore-and-exited" class="section level4">
<h4>3. Relationship between CreditScore and Exited</h4>
<pre class="r"><code>CreditScore_Exited = glm(Exited~ CreditScore, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Balance_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Balance, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8838  -0.7175  -0.5558  -0.5558   1.9718  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.790e+00  4.535e-02  -39.46   &lt;2e-16 ***
## Balance      4.745e-06  4.260e-07   11.14   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9387.9  on 9567  degrees of freedom
## AIC: 9391.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The credit score is 99.9% significant to predict the exited.</p>
<pre class="r"><code>discretized.CreditScore = cut(Bank_Churn_Data$CreditScore, c(0,400, 500, 600, 700, 800, 900))
ggplot(Bank_Churn_Data, aes(x = discretized.CreditScore , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-34-1.png" width="672" />
We can infer from the above bar chart that more people are churned when the credit score is between 600 and 700.</p>
</div>
<div id="relationship-between-estimatedsalary-and-exited" class="section level4">
<h4>4. Relationship between EstimatedSalary and Exited</h4>
<pre class="r"><code>EstimatedSalary_Exited = glm(Exited~ EstimatedSalary, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(EstimatedSalary_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ EstimatedSalary, family = &quot;binomial&quot;, 
##     data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6753  -0.6680  -0.6610  -0.6537   1.8178  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.440e+00  5.182e-02 -27.779   &lt;2e-16 ***
## EstimatedSalary  3.875e-07  4.463e-07   0.868    0.385    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9515.2  on 9567  degrees of freedom
## AIC: 9519.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficient of the estimated salary are not significant to predict the exited</p>
<pre class="r"><code>discretized.EstimatedSalary = cut(Bank_Churn_Data$EstimatedSalary, c(0,40000, 80000, 120000, 160000, 200000),dig.lab = 1000)</code></pre>
<pre><code>## Warning in formatC(0 + breaks, digits = dig, width = 1L): &#39;digits&#39; reduced to 50</code></pre>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = discretized.EstimatedSalary , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
<div id="relationship-between-numofproducts-and-exited" class="section level4">
<h4>5. Relationship between NumOfProducts and Exited</h4>
<pre class="r"><code>NumOfProducts_Exited = glm(Exited~ NumOfProducts, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(NumOfProducts_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ NumOfProducts, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7401  -0.7401  -0.5848  -0.5848   2.1487  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -0.63043    0.07476  -8.433   &lt;2e-16 ***
## NumOfProducts -0.52446    0.04935 -10.628   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9398.0  on 9567  degrees of freedom
## AIC: 9402
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The number of bank products held by customers has a 99.9% significant relationship to predict the customers exited.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = NumOfProducts , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-38-1.png" width="672" />
The percentage of the churn rate for customer holding 1 and 2 account having more churn rate compared to customer having 3 products.</p>
</div>
<div id="relationship-between-tenure-and-exited" class="section level4">
<h4>6. Relationship between Tenure and Exited</h4>
<pre class="r"><code>Tenure_Exited = glm(Exited~ Tenure, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Tenure_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Tenure, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6826  -0.6712  -0.6600  -0.6489   1.8285  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.337962   0.050962 -26.254   &lt;2e-16 ***
## Tenure      -0.012569   0.008891  -1.414    0.157    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9513.9  on 9567  degrees of freedom
## AIC: 9517.9
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The coefficient of the tenure is not significant to predict the exited.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Tenure , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-40-1.png" width="672" />
It is surprising to see that 828 customer after having 7 years of relationship with bank, that was the highest churn numbers among different tenures.</p>
</div>
<div id="relationship-between-geography-and-exited" class="section level4">
<h4>7. Relationship between Geography and Exited</h4>
<pre class="r"><code>Geography_Exited = glm(Exited~ Geography, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Geography_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Geography, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8717  -0.5950  -0.5826  -0.5826   1.9273  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -1.68749    0.03978  -42.42   &lt;2e-16 ***
## GeographyGermany  0.91572    0.05925   15.45   &lt;2e-16 ***
## GeographySpain    0.04585    0.06843    0.67    0.503    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9253.0  on 9566  degrees of freedom
## AIC: 9259
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is no significant relationship between geography and exited due to some of the coefficients are not significant.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Geography , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-42-1.png" width="672" />
France has the highest churn rate of customers among others. Almost 85% of the customer closed the account.</p>
</div>
<div id="relationship-between-gender-and-exited" class="section level4">
<h4>8. Relationship between Gender and Exited</h4>
<pre class="r"><code>Gender_Exited = glm(Exited~ Gender, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(Gender_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Gender, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7467  -0.7467  -0.5905  -0.5905   1.9144  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.13464    0.03541  -32.04   &lt;2e-16 ***
## GenderMale  -0.52347    0.05172  -10.12   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9412.7  on 9567  degrees of freedom
## AIC: 9416.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.9% significant relationship between gender and exited</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = Gender , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>There is a highest churn rate with male customer with approximate churn percentage of 84% as against female churn rate of 75% approximately.</p>
</div>
<div id="relationship-between-hascrcard-and-exited" class="section level4">
<h4>9. Relationship between HasCrCard and Exited</h4>
<pre class="r"><code>HasCrCard_Exited = glm(Exited~ HasCrCard, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(HasCrCard_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ HasCrCard, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6725  -0.6601  -0.6601  -0.6601   1.8060  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.37130    0.04686 -29.264   &lt;2e-16 ***
## HasCrCard1  -0.04170    0.05601  -0.745    0.457    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9515.4  on 9567  degrees of freedom
## AIC: 9519.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is no significant relationship between the customer having credit card and exited because of its coefficients are not significant.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = HasCrCard , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-46-1.png" width="672" />
Almost 80% of customers closed the account irrespective of customers having credit card or not.</p>
</div>
<div id="relationship-between-isactivemember-and-exited" class="section level4">
<h4>10. Relationship between IsActiveMember and Exited</h4>
<pre class="r"><code>IsActiveMember_Exited = glm(Exited~ IsActiveMember, data = Bank_Churn_Data, family = &quot;binomial&quot;)
summary(IsActiveMember_Exited)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.7688  -0.7688  -0.5502  -0.5502   1.9813  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -1.06769    0.03326   -32.1   &lt;2e-16 ***
## IsActiveMember1 -0.74382    0.05315   -14.0   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 9313.0  on 9567  degrees of freedom
## AIC: 9317
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a 99.9% significant relationship between IsActiveMember and Exited.</p>
<pre class="r"><code>ggplot(Bank_Churn_Data, aes(x = IsActiveMember , fill = Exited )) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>It is surprising to notice that actively transacted customers(86%) have high percentage of churn rate against customers who have not transacted actively (75%).</p>
</div>
</div>
<div id="full-model-logistic-regression" class="section level1">
<h1>Full model logistic regression</h1>
<pre class="r"><code>full_model_bank_churn = glm(Exited ~ ., family = &quot;binomial&quot;, data = Bank_Churn_Data)
summary(full_model_bank_churn)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ ., family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2085  -0.6213  -0.4107  -0.2280   3.2385  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -4.574e+00  2.789e-01 -16.399  &lt; 2e-16 ***
## CreditScore      -3.375e-04  3.002e-04  -1.124  0.26101    
## GeographyGermany  8.210e-01  7.215e-02  11.380  &lt; 2e-16 ***
## GeographySpain    3.217e-02  7.474e-02   0.430  0.66685    
## GenderMale       -5.318e-01  5.785e-02  -9.193  &lt; 2e-16 ***
## Age               1.089e-01  3.454e-03  31.541  &lt; 2e-16 ***
## Tenure           -1.611e-02  9.943e-03  -1.620  0.10520    
## Balance           1.519e-06  5.512e-07   2.756  0.00586 ** 
## NumOfProducts    -4.038e-01  5.478e-02  -7.371 1.69e-13 ***
## HasCrCard1       -2.773e-02  6.295e-02  -0.441  0.65954    
## IsActiveMember1  -9.428e-01  6.006e-02 -15.696  &lt; 2e-16 ***
## EstimatedSalary   4.489e-07  5.029e-07   0.893  0.37210    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 7667.3  on 9557  degrees of freedom
## AIC: 7691.3
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We could see from the above full model by logistic regression, many coefficients are not significant.</p>
</div>
<div id="backward-regression" class="section level1">
<h1>Backward Regression</h1>
<p>We used the stepAIC backward regression to find out the best model</p>
<pre class="r"><code>mod_step_bank_churn&lt;- stepAIC(full_model_bank_churn, direction = &#39;backward&#39;, trace = FALSE)
mod_step_bank_churn</code></pre>
<pre><code>## 
## Call:  glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Coefficients:
##      (Intercept)  GeographyGermany    GeographySpain        GenderMale  
##       -4.771e+00         8.201e-01         3.131e-02        -5.317e-01  
##              Age            Tenure           Balance     NumOfProducts  
##        1.090e-01        -1.599e-02         1.531e-06        -4.029e-01  
##  IsActiveMember1  
##       -9.445e-01  
## 
## Degrees of Freedom: 9568 Total (i.e. Null);  9560 Residual
## Null Deviance:       9516 
## Residual Deviance: 7670  AIC: 7688</code></pre>
<p>Geography. gender, age, tenure, balance, numofproducts, isactivemember are the best predictors.</p>
</div>
<div id="bootstrap-method" class="section level1">
<h1>Bootstrap method</h1>
<p>Using bootstrap re sampling with replacement method to access the consistency predictors selected with step-wise.</p>
<pre class="r"><code>mod_boot_bank_churn&lt;- boot.stepAIC(full_model_bank_churn, Bank_Churn_Data, B =50)</code></pre>
<p><strong>Bootstrap summary</strong></p>
<pre class="r"><code>print(mod_boot_bank_churn)</code></pre>
<pre><code>## 
## Summary of Bootstrapping the &#39;stepAIC()&#39; procedure for
## 
## Call:
## glm(formula = Exited ~ ., family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Bootstrap samples: 50 
## Direction: backward 
## Penalty: 2 * df
## 
## Covariates selected
##                 (%)
## Age             100
## Gender          100
## Geography       100
## IsActiveMember  100
## NumOfProducts   100
## Balance          90
## CreditScore      44
## Tenure           42
## EstimatedSalary  22
## HasCrCard        20
## 
## Coefficients Sign
##                  + (%) - (%)
## Age                100     0
## Balance            100     0
## EstimatedSalary    100     0
## GeographyGermany   100     0
## GeographySpain      58    42
## HasCrCard1          30    70
## CreditScore          0   100
## GenderMale           0   100
## IsActiveMember1      0   100
## NumOfProducts        0   100
## Tenure               0   100
## 
## Stat Significance
##                     (%)
## Age              100.00
## GenderMale       100.00
## GeographyGermany 100.00
## IsActiveMember1  100.00
## NumOfProducts    100.00
## Balance           82.22
## Tenure            52.38
## HasCrCard1        50.00
## CreditScore       40.91
## EstimatedSalary   27.27
## GeographySpain    14.00
## 
## 
## The stepAIC() for the original data-set gave
## 
## Call:  glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Coefficients:
##      (Intercept)  GeographyGermany    GeographySpain        GenderMale  
##       -4.771e+00         8.201e-01         3.131e-02        -5.317e-01  
##              Age            Tenure           Balance     NumOfProducts  
##        1.090e-01        -1.599e-02         1.531e-06        -4.029e-01  
##  IsActiveMember1  
##       -9.445e-01  
## 
## Degrees of Freedom: 9568 Total (i.e. Null);  9560 Residual
## Null Deviance:       9516 
## Residual Deviance: 7670  AIC: 7688
## 
## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## Exited ~ CreditScore + Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + HasCrCard + IsActiveMember + EstimatedSalary
## 
## Final Model:
## Exited ~ Geography + Gender + Age + Tenure + Balance + NumOfProducts + 
##     IsActiveMember
## 
## 
##                Step Df  Deviance Resid. Df Resid. Dev      AIC
## 1                                     9557   7667.318 7691.318
## 2       - HasCrCard  1 0.1937752      9558   7667.511 7689.511
## 3 - EstimatedSalary  1 0.7998999      9559   7668.311 7688.311
## 4     - CreditScore  1 1.2713603      9560   7669.583 7687.583</code></pre>
<p>We can see above that even after bootstrapping, same variables are selected as a best predictors.</p>
</div>
<div id="best-model-by-backward-and-bootstrap-method" class="section level1">
<h1>Best model by backward and bootstrap method</h1>
<pre class="r"><code>best_bootmodel_bank_churn&lt;- glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + NumOfProducts + 
    IsActiveMember, family = &quot;binomial&quot;, 
    data = Bank_Churn_Data)
summary(best_bootmodel_bank_churn) </code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Geography + Gender + Age + Tenure + Balance + 
##     NumOfProducts + IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1967  -0.6226  -0.4118  -0.2283   3.2277  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -4.771e+00  1.855e-01 -25.716  &lt; 2e-16 ***
## GeographyGermany  8.201e-01  7.211e-02  11.373  &lt; 2e-16 ***
## GeographySpain    3.131e-02  7.472e-02   0.419  0.67521    
## GenderMale       -5.317e-01  5.783e-02  -9.193  &lt; 2e-16 ***
## Age               1.090e-01  3.453e-03  31.562  &lt; 2e-16 ***
## Tenure           -1.599e-02  9.938e-03  -1.609  0.10758    
## Balance           1.531e-06  5.510e-07   2.778  0.00547 ** 
## NumOfProducts    -4.029e-01  5.476e-02  -7.357 1.88e-13 ***
## IsActiveMember1  -9.445e-01  6.003e-02 -15.734  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 7669.6  on 9560  degrees of freedom
## AIC: 7687.6
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We could see in the above selected bootstrap and stepwise regression model that some of the coefficients such as geography and tenure are not significant. SO to get better significant model, geography and tenure are to be removed.</p>
</div>
<div id="final-best-model" class="section level1">
<h1>Final Best model</h1>
<pre class="r"><code>bestmodel&lt;- glm(formula = Exited ~  Gender + Age + Balance + NumOfProducts + 
    IsActiveMember, family = &quot;binomial&quot;, 
    data = Bank_Churn_Data)
summary(bestmodel)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Exited ~ Gender + Age + Balance + NumOfProducts + 
##     IsActiveMember, family = &quot;binomial&quot;, data = Bank_Churn_Data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0542  -0.6342  -0.4224  -0.2347   3.1988  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -4.936e+00  1.766e-01 -27.944  &lt; 2e-16 ***
## GenderMale      -5.534e-01  5.721e-02  -9.673  &lt; 2e-16 ***
## Age              1.094e-01  3.417e-03  32.022  &lt; 2e-16 ***
## Balance          4.112e-06  4.921e-07   8.357  &lt; 2e-16 ***
## NumOfProducts   -3.231e-01  5.395e-02  -5.989 2.11e-09 ***
## IsActiveMember1 -9.446e-01  5.942e-02 -15.898  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 9515.9  on 9568  degrees of freedom
## Residual deviance: 7815.6  on 9563  degrees of freedom
## AIC: 7827.6
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The above is the best model of logistic regression after many iterations as all the coefficients are 99.9% significant to predict the response variable - exited.</p>
</div>
<div id="data-resampling" class="section level1">
<h1>Data resampling</h1>
<pre class="r"><code>bank_churn_split &lt;- initial_split (Bank_Churn_Data, prop = 0.80, strata = Exited)
bank_churn_training &lt;- bank_churn_split %&gt;%
  training()
bank_churn_test &lt;- bank_churn_split %&gt;%
  testing()</code></pre>
<p>We have taken 80:20 split for data sampling. We have made training data to fit the model and test the model using test set.</p>
<p><strong>Checking number of rows in training and test data</strong></p>
<pre class="r"><code>nrow(bank_churn_training)</code></pre>
<pre><code>## [1] 7654</code></pre>
<pre class="r"><code>nrow(bank_churn_test)</code></pre>
<pre><code>## [1] 1915</code></pre>
<p>There are total 7654 numbers of row in training data and 1915 numbers of row in test data.</p>
<p><strong>Checking multicollinearity between numerical values in a training bank churn data set</strong></p>
<pre class="r"><code>bank_churn_training %&gt;%
  select_if(is.numeric) %&gt;%
  cor()</code></pre>
<pre><code>##                  CreditScore          Age       Tenure      Balance
## CreditScore      1.000000000 -0.014088776  0.002491385 -0.001784284
## Age             -0.014088776  1.000000000 -0.007772773  0.036899233
## Tenure           0.002491385 -0.007772773  1.000000000 -0.012000353
## Balance         -0.001784284  0.036899233 -0.012000353  1.000000000
## NumOfProducts    0.014926483 -0.063926183  0.013649541 -0.327324471
## EstimatedSalary  0.005077244  0.003088562  0.007141343  0.006671063
##                 NumOfProducts EstimatedSalary
## CreditScore        0.01492648     0.005077244
## Age               -0.06392618     0.003088562
## Tenure             0.01364954     0.007141343
## Balance           -0.32732447     0.006671063
## NumOfProducts      1.00000000     0.015216513
## EstimatedSalary    0.01521651     1.000000000</code></pre>
<p>There is no multicollinearity between the independent numeric variables.</p>
</div>
<div id="future-engineering" class="section level1">
<h1>Future engineering</h1>
<p>This is the step to pre-processing of data</p>
<pre class="r"><code>bank_churn_recipe &lt;- recipe(Exited ~ ., data = bank_churn_training) %&gt;%
  step_corr(all_numeric(), threshold =0.8) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes())
bank_churn_recipe</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         10
## 
## Operations:
## 
## Correlation filter on all_numeric()
## Centering and scaling for all_numeric()
## Dummy variables from all_nominal(), -all_outcomes()</code></pre>
<p>We have given instruction above in the model that to check the correlation for all numeric variables and keep the thresholding limit as 0.8. We have also given instruction in the model to normalize all the numeric variables and set dummy to all nominal or character variables except the outcome variables because outcome is in factor. There are 10 predictor variable and 1 outcome variable.</p>
<p><strong>Recipe training</strong></p>
<pre class="r"><code>bank_churn_recipe_prep&lt;- bank_churn_recipe %&gt;%
  prep(training = bank_churn_training)
bank_churn_recipe_prep</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         10
## 
## Training data contained 7654 data points and no missing data.
## 
## Operations:
## 
## Correlation filter removed no terms [trained]
## Centering and scaling for CreditScore, Age, Tenure, Balance, NumOfProduct... [trained]
## Dummy variables from Geography, Gender, HasCrCard, IsActiveMember [trained]</code></pre>
<p>We have prepared the training data set.</p>
<p><strong>Preprocess training data</strong></p>
<pre class="r"><code>bank_churn_training_prep &lt;- bank_churn_recipe_prep %&gt;%
  bake (new_data = NULL)
bank_churn_training_prep</code></pre>
<pre><code>## # A tibble: 7,654 x 12
##    CreditScore    Age   Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1      -0.442  0.380 -1.39      0.126        -0.935           0.201 0     
##  2       0.501  0.151 -1.39     -1.22          0.880          -0.124 0     
##  3       2.07   0.610 -1.04      0.795        -0.935          -0.380 0     
##  4       1.78   1.41   0.687    -1.22          0.880          -1.58  0     
##  5      -1.55   0.725 -0.351     1.06          0.880          -0.452 0     
##  6       0.346 -1.23  -1.04      0.941        -0.935          -0.507 0     
##  7      -1.27  -0.768  0.341     0.418         0.880          -0.361 0     
##  8      -1.59  -1.57  -0.697    -1.22          0.880          -0.426 0     
##  9      -1.81  -0.424  1.73     -1.22          0.880          -1.30  0     
## 10      -1.05  -1.46  -0.00497  -1.22          0.880           1.56  0     
## # ... with 7,644 more rows, and 5 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;</code></pre>
<p>We have pre-processed the training data set.</p>
<p><strong>Preprocess test data</strong></p>
<pre class="r"><code>bank_churn_test_prep &lt;- bank_churn_recipe_prep %&gt;%
  bake (new_data = bank_churn_test)
bank_churn_test_prep</code></pre>
<pre><code>## # A tibble: 1,915 x 12
##    CreditScore    Age   Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.660   0.839  0.341    -1.22         -0.935           1.00  0     
##  2      0.843   0.380  1.03     -1.22          0.880           1.21  0     
##  3     -0.152  -0.653  1.03     -1.22          0.880           0.653 0     
##  4      0.190   0.954 -0.697    -1.22          0.880          -1.61  0     
##  5     -2.49   -0.998 -1.74     -0.261         0.880          -0.824 0     
##  6     -1.22   -0.194  0.687     0.150        -0.935           0.969 0     
##  7      2.07   -0.194  0.687    -1.22         -0.935          -1.04  0     
##  8     -0.982   2.68  -1.04      0.665        -0.935          -0.118 0     
##  9      0.0968  2.68  -0.00497   1.28         -0.935           0.996 0     
## 10      0.491   0.725  1.73      0.648         0.880           1.69  0     
## # ... with 1,905 more rows, and 5 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;</code></pre>
<p>We pre-processed the test data.</p>
</div>
<div id="support-vector-machine" class="section level1">
<h1>Support vector machine</h1>
<pre class="r"><code>bank_churn_svm_model &lt;- svm(Exited ~., data = bank_churn_training_prep, type = &quot;C-classification&quot;, kernel = &quot;linear&quot;, scale = FALSE)
bank_churn_svm_model</code></pre>
<pre><code>## 
## Call:
## svm(formula = Exited ~ ., data = bank_churn_training_prep, type = &quot;C-classification&quot;, 
##     kernel = &quot;linear&quot;, scale = FALSE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  1 
## 
## Number of Support Vectors:  3209</code></pre>
<p>We build the svm model</p>
<p><strong>Training accuracy</strong></p>
<pre class="r"><code>bank_churn_svm_pred_train &lt;- predict(bank_churn_svm_model, bank_churn_training_prep)
mean(bank_churn_svm_pred_train == bank_churn_training_prep$Exited)</code></pre>
<pre><code>## [1] 0.8023256</code></pre>
<p>We can see the average predicted trained data are 80.23%</p>
<p><strong>Testing accuracy</strong></p>
<pre class="r"><code>bank_churn_svm_pred_test &lt;- predict(bank_churn_svm_model, newdata = bank_churn_test_prep)
mean(bank_churn_svm_pred_test == bank_churn_test_prep$Exited)</code></pre>
<pre><code>## [1] 0.8020888</code></pre>
<p>We can see the average predicted test data</p>
<p><strong>Confusion Matrix</strong></p>
<pre class="r"><code>confusionMatrix(bank_churn_svm_pred_test, bank_churn_test_prep$Exited )</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1536  379
##          1    0    0
##                                           
##                Accuracy : 0.8021          
##                  95% CI : (0.7835, 0.8197)
##     No Information Rate : 0.8021          
##     P-Value [Acc &gt; NIR] : 0.5137          
##                                           
##                   Kappa : 0               
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8021          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.8021          
##          Detection Rate : 0.8021          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>We can see the prediction results above that there are more false negative and there are no true positive. We feel that the model is not predicting properly.</p>
</div>
<div id="logistic-regression-machine-learning-workflow-models" class="section level1">
<h1>Logistic regression Machine learning workflow models</h1>
<p>I specify the logistic regression model here</p>
<pre class="r"><code>bank_churn_logistic_model&lt;- logistic_reg() %&gt;%
  set_engine(&#39;glm&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
<p>I have created the logistic model</p>
<p><strong>Model fitting</strong></p>
<pre class="r"><code>bank_churn_logistic_fit &lt;- bank_churn_logistic_model %&gt;%
  fit(Exited ~ ., data = bank_churn_training_prep)
bank_churn_logistic_fit</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  31ms 
## 
## Call:  stats::glm(formula = Exited ~ ., family = stats::binomial, data = data)
## 
## Coefficients:
##       (Intercept)        CreditScore                Age             Tenure  
##        -1.2095406         -0.0235357          0.9649325         -0.0386376  
##           Balance      NumOfProducts    EstimatedSalary  Geography_Germany  
##         0.1008755         -0.2051002          0.0433014          0.7639729  
##   Geography_Spain        Gender_Male       HasCrCard_X1  IsActiveMember_X1  
##        -0.0006612         -0.5060566         -0.0095352         -0.9718120  
## 
## Degrees of Freedom: 7653 Total (i.e. Null);  7642 Residual
## Null Deviance:       7611 
## Residual Deviance: 6140  AIC: 6164</code></pre>
<p>We have used the fit function to train and fit the model.</p>
<p><strong>Predicting outcome categories</strong></p>
<pre class="r"><code>bank_churn_logistic_class_preds &lt;- predict (bank_churn_logistic_fit, new_data = bank_churn_test_prep, type =&quot;class&quot;)
bank_churn_logistic_class_preds</code></pre>
<pre><code>## # A tibble: 1,915 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 0          
##  2 0          
##  3 0          
##  4 0          
##  5 0          
##  6 0          
##  7 0          
##  8 1          
##  9 1          
## 10 0          
## # ... with 1,905 more rows</code></pre>
<p>The above .pred class is the outcome of our results. That is if the customer will churn or not. The number o means churn and 1 means no churn.</p>
<p><strong>Estimated probabilities</strong></p>
<pre class="r"><code>bank_churn_logistic_prob_preds &lt;- predict (bank_churn_logistic_fit, new_data = bank_churn_test_prep, type =&quot;prob&quot;)
bank_churn_logistic_prob_preds</code></pre>
<pre><code>## # A tibble: 1,915 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1   0.688  0.312 
##  2   0.933  0.0666
##  3   0.897  0.103 
##  4   0.893  0.107 
##  5   0.978  0.0224
##  6   0.932  0.0676
##  7   0.950  0.0504
##  8   0.330  0.670 
##  9   0.320  0.680 
## 10   0.594  0.406 
## # ... with 1,905 more rows</code></pre>
<p>The above prediction are the probabilities of outcome occurrences.</p>
<p><strong>Combining results</strong></p>
<pre class="r"><code>bank_churn_logistic_results &lt;- bank_churn_test_prep %&gt;%
  bind_cols(bank_churn_logistic_class_preds, bank_churn_logistic_prob_preds)
bank_churn_logistic_results</code></pre>
<pre><code>## # A tibble: 1,915 x 15
##    CreditScore    Age   Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.660   0.839  0.341    -1.22         -0.935           1.00  0     
##  2      0.843   0.380  1.03     -1.22          0.880           1.21  0     
##  3     -0.152  -0.653  1.03     -1.22          0.880           0.653 0     
##  4      0.190   0.954 -0.697    -1.22          0.880          -1.61  0     
##  5     -2.49   -0.998 -1.74     -0.261         0.880          -0.824 0     
##  6     -1.22   -0.194  0.687     0.150        -0.935           0.969 0     
##  7      2.07   -0.194  0.687    -1.22         -0.935          -1.04  0     
##  8     -0.982   2.68  -1.04      0.665        -0.935          -0.118 0     
##  9      0.0968  2.68  -0.00497   1.28         -0.935           0.996 0     
## 10      0.491   0.725  1.73      0.648         0.880           1.69  0     
## # ... with 1,905 more rows, and 8 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;, .pred_class &lt;fct&gt;, .pred_0 &lt;dbl&gt;, .pred_1 &lt;dbl&gt;</code></pre>
<p>We have combined the predicted outcome and probabilities in to test data.</p>
<p><strong>Assessing the model fit using Confusion matrix</strong></p>
<pre class="r"><code>bank_churn_logistic_results %&gt;%
  conf_mat(truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-73-1.png" width="672" />
<strong>Correct predictions of test set</strong>
True negative is 1483 customers, who churned. True positive is 120 people, who was a loyal customer and did not churn.</p>
<p><strong>Classification error of test set</strong>
False positive is 53 people, who are predicted as not churned but actually churned. False negative is 2259 people, who are predicted as churned but actually not churned and was loyal to bank.</p>
<p><strong>Creating workflow</strong></p>
<pre class="r"><code>bank_churn_logistic_wkfl&lt;- workflow() %&gt;%
add_model(bank_churn_logistic_model) %&gt;%
add_recipe(bank_churn_recipe)
bank_churn_logistic_wkfl</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm</code></pre>
<p>We have created workflow above in the model that to check the correlation for all numeric variables and keep the threshold limit as 0.8. We have also given instruction in the model to normalize all the numeric variables and set dummy to all nominal or character variables except the outcome variables because outcome is a character variable.</p>
<p><strong>Train the workflow</strong></p>
<pre class="r"><code>bank_churn_logistic__wkfl_fit &lt;- bank_churn_logistic_wkfl %&gt;%
  last_fit( split = bank_churn_split)
bank_churn_logistic__wkfl_fit</code></pre>
<pre><code>## # Resampling results
## # Manual resampling 
## # A tibble: 1 x 6
##   splits              id               .metrics  .notes   .predictions .workflow
##   &lt;list&gt;              &lt;chr&gt;            &lt;list&gt;    &lt;list&gt;   &lt;list&gt;       &lt;list&gt;   
## 1 &lt;split [7654/1915]&gt; train/test split &lt;tibble ~ &lt;tibble~ &lt;tibble [1,~ &lt;workflo~</code></pre>
<p>We have trained the workflow.</p>
<p><strong>Calculating performance metrics</strong></p>
<pre class="r"><code>bank_churn_logistic__wkfl_fit %&gt;%
   collect_metrics() </code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.833 Preprocessor1_Model1
## 2 roc_auc  binary         0.793 Preprocessor1_Model1</code></pre>
<p>The accuracy of the logistic regression model is 84.28% and roc-auc is 80.99%</p>
<p><strong>collecting predictions</strong></p>
<pre class="r"><code>bank_churn_logistic_wkfl_fit_results&lt;- bank_churn_logistic__wkfl_fit %&gt;%
  collect_predictions()
bank_churn_logistic_wkfl_fit_results</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.688  0.312     18 0           0      Preprocessor1_Mode~
##  2 train/test split   0.933  0.0666    20 0           0      Preprocessor1_Mode~
##  3 train/test split   0.897  0.103     21 0           0      Preprocessor1_Mode~
##  4 train/test split   0.893  0.107     23 0           0      Preprocessor1_Mode~
##  5 train/test split   0.978  0.0224    29 0           0      Preprocessor1_Mode~
##  6 train/test split   0.932  0.0676    31 0           0      Preprocessor1_Mode~
##  7 train/test split   0.950  0.0504    38 0           0      Preprocessor1_Mode~
##  8 train/test split   0.330  0.670     42 1           0      Preprocessor1_Mode~
##  9 train/test split   0.320  0.680     44 1           0      Preprocessor1_Mode~
## 10 train/test split   0.594  0.406     50 0           0      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Creating custom metrics</strong></p>
<pre class="r"><code>bank_churn_logistic_metrics &lt;- metric_set(roc_auc,sens,spec,accuracy)
bank_churn_logistic_metrics</code></pre>
<pre><code>## # A tibble: 4 x 3
##   metric   class        direction
##   &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;    
## 1 roc_auc  prob_metric  maximize 
## 2 sens     class_metric maximize 
## 3 spec     class_metric maximize 
## 4 accuracy class_metric maximize</code></pre>
<pre class="r"><code>bank_churn_logistic_wkfl_fit_results %&gt;%
 bank_churn_logistic_metrics(truth = Exited, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.958
## 2 spec     binary         0.327
## 3 accuracy binary         0.833
## 4 roc_auc  binary         0.207</code></pre>
<p>The above are the metrics using logistic regression.</p>
</div>
<div id="random-forest-machine-learning-workflow-models" class="section level1">
<h1>Random forest Machine learning workflow models</h1>
<p>Let us develop random forest model and compare against logistic to see which model is more accurate</p>
<pre class="r"><code>bank_churn_rf_model&lt;- rand_forest(mtry =4,trees = 100, min_n =10) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
<p>We have used 100 trees to create a model using random forest</p>
<p><strong>Training a forest</strong></p>
<pre class="r"><code>bank_churn_fit_rf &lt;- bank_churn_rf_model %&gt;%
  fit (Exited ~ ., data = bank_churn_training_prep)
bank_churn_fit_rf</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  940ms 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~100, min.node.size = min_rows(~10, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  100 
## Sample size:                      7654 
## Number of independent variables:  11 
## Mtry:                             4 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.1071763</code></pre>
<p>We have trained the model</p>
<p><strong>Predicting outcome variables</strong></p>
<pre class="r"><code>bank_churn_class_preds_rf &lt;- predict(bank_churn_fit_rf, new_data = bank_churn_test_prep, type = &quot;class&quot;)
bank_churn_class_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 1          
##  2 0          
##  3 0          
##  4 0          
##  5 0          
##  6 0          
##  7 0          
##  8 0          
##  9 0          
## 10 0          
## # ... with 1,905 more rows</code></pre>
<p>The above are predicted outcomes using test set</p>
<p><strong>Estimated probabilities</strong></p>
<pre class="r"><code>bank_churn_prob_preds_rf &lt;- predict(bank_churn_fit_rf, new_data = bank_churn_test_prep, type = &quot;prob&quot;)
bank_churn_prob_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1   0.415  0.585 
##  2   0.981  0.0192
##  3   0.986  0.0135
##  4   0.943  0.0570
##  5   0.796  0.204 
##  6   0.863  0.137 
##  7   0.683  0.317 
##  8   0.672  0.328 
##  9   0.523  0.477 
## 10   0.751  0.249 
## # ... with 1,905 more rows</code></pre>
<p>The above are the predicted probabilities using test data set.</p>
<p><strong>Combining results</strong></p>
<pre class="r"><code>bank_churn_results_rf &lt;- bank_churn_test_prep %&gt;%
  bind_cols(bank_churn_class_preds_rf, bank_churn_prob_preds_rf)
bank_churn_results_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 15
##    CreditScore    Age   Tenure Balance NumOfProducts EstimatedSalary Exited
##          &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt; 
##  1     -0.660   0.839  0.341    -1.22         -0.935           1.00  0     
##  2      0.843   0.380  1.03     -1.22          0.880           1.21  0     
##  3     -0.152  -0.653  1.03     -1.22          0.880           0.653 0     
##  4      0.190   0.954 -0.697    -1.22          0.880          -1.61  0     
##  5     -2.49   -0.998 -1.74     -0.261         0.880          -0.824 0     
##  6     -1.22   -0.194  0.687     0.150        -0.935           0.969 0     
##  7      2.07   -0.194  0.687    -1.22         -0.935          -1.04  0     
##  8     -0.982   2.68  -1.04      0.665        -0.935          -0.118 0     
##  9      0.0968  2.68  -0.00497   1.28         -0.935           0.996 0     
## 10      0.491   0.725  1.73      0.648         0.880           1.69  0     
## # ... with 1,905 more rows, and 8 more variables: Geography_Germany &lt;dbl&gt;,
## #   Geography_Spain &lt;dbl&gt;, Gender_Male &lt;dbl&gt;, HasCrCard_X1 &lt;dbl&gt;,
## #   IsActiveMember_X1 &lt;dbl&gt;, .pred_class &lt;fct&gt;, .pred_0 &lt;dbl&gt;, .pred_1 &lt;dbl&gt;</code></pre>
<p>We have combined the predicted outcome and probabilities into the preprocessed test data set.</p>
<p><strong>Assessing model fit using confusion matrix</strong></p>
<pre class="r"><code>bank_churn_results_rf %&gt;%
  conf_mat(truth = Exited, estimate = .pred_class) %&gt;%
autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-85-1.png" width="672" />
The above random forest model fit is better than logistic model fit</p>
<p><strong>Correct predictions of test set</strong>
True negative is 1492 customers, who churned. True positive is 180 people, who was a loyal customer and did not churn.</p>
<p><strong>Classification error of test set</strong>
False positive is 44 people, who are predicted as not churned but actually churned. False negative is 199 people, who are predicted as churned but actually not churned and was loyal to bank.</p>
<p><strong>Combining models and recipe</strong></p>
<pre class="r"><code>bank_churn_wkfl_rf&lt;- workflow() %&gt;%
  add_model(bank_churn_rf_model) %&gt;%
  add_recipe(bank_churn_recipe)
bank_churn_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 4
##   trees = 100
##   min_n = 10
## 
## Computational engine: ranger</code></pre>
<p>We combined the model and its receipe.</p>
<p><strong>Model fitting with workflow</strong></p>
<pre class="r"><code>bank_churn_wkfl_fit_rf &lt;- bank_churn_wkfl_rf %&gt;%
  last_fit(split = bank_churn_split)
bank_churn_wkfl_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.863 Preprocessor1_Model1
## 2 roc_auc  binary         0.860 Preprocessor1_Model1</code></pre>
<p>We can see here the accuracy of the random forest model is 87.31% and roc_auc is 85.48%, which are better than logistic<code>model accuracy (84.28%) and roc_auc (80.99%) and support vector machine</code> accuracy 80.21%.</p>
<p><strong>Collecting predictions</strong></p>
<pre class="r"><code>bank_churn_wkfl_preds_rf &lt;- bank_churn_wkfl_fit_rf %&gt;%
  collect_predictions()
bank_churn_wkfl_preds_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.450  0.550     18 1           0      Preprocessor1_Mode~
##  2 train/test split   0.960  0.0403    20 0           0      Preprocessor1_Mode~
##  3 train/test split   0.977  0.0226    21 0           0      Preprocessor1_Mode~
##  4 train/test split   0.912  0.0876    23 0           0      Preprocessor1_Mode~
##  5 train/test split   0.849  0.151     29 0           0      Preprocessor1_Mode~
##  6 train/test split   0.898  0.102     31 0           0      Preprocessor1_Mode~
##  7 train/test split   0.734  0.266     38 0           0      Preprocessor1_Mode~
##  8 train/test split   0.726  0.274     42 0           0      Preprocessor1_Mode~
##  9 train/test split   0.503  0.497     44 0           0      Preprocessor1_Mode~
## 10 train/test split   0.694  0.306     50 0           0      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Confusion matrix</strong></p>
<pre class="r"><code>conf_mat(bank_churn_wkfl_preds_rf, truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-89-1.png" width="672" />
We could that there is a reduction in both false positive and negative.</p>
<p><strong>Exploring custom metrics</strong></p>
<pre class="r"><code>bank_churn_metrics_rf &lt;- metric_set(roc_auc, sens, spec, accuracy)
bank_churn_wkfl_preds_rf %&gt;%
bank_churn_metrics_rf(truth = Exited, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.962
## 2 spec     binary         0.464
## 3 accuracy binary         0.863
## 4 roc_auc  binary         0.140</code></pre>
</div>
<div id="creating-k-fold-cross-validation" class="section level1">
<h1>Creating k-fold cross validation</h1>
<pre class="r"><code>set.seed(222)
bank_churn_folds_rf &lt;- vfold_cv(bank_churn_training, v = 10, strata = Exited)
bank_churn_folds_rf</code></pre>
<pre><code>## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits             id    
##    &lt;list&gt;             &lt;chr&gt; 
##  1 &lt;split [6887/767]&gt; Fold01
##  2 &lt;split [6888/766]&gt; Fold02
##  3 &lt;split [6888/766]&gt; Fold03
##  4 &lt;split [6889/765]&gt; Fold04
##  5 &lt;split [6889/765]&gt; Fold05
##  6 &lt;split [6889/765]&gt; Fold06
##  7 &lt;split [6889/765]&gt; Fold07
##  8 &lt;split [6889/765]&gt; Fold08
##  9 &lt;split [6889/765]&gt; Fold09
## 10 &lt;split [6889/765]&gt; Fold10</code></pre>
<p>Since random forest model is better than logistic model and support vector machine for prediction in terms of its accuracy and roc-auc, We take the random forest model to do cross validation and hyper parameter tuning to further increase the prediction accuracy.</p>
<p>We have used 10 times k-fold cross validation to do re-sampling.</p>
<p><strong>Model training with cross validation</strong></p>
<pre class="r"><code>bank_churn_rs_fit_rf &lt;-bank_churn_wkfl_rf %&gt;%
  fit_resamples(resamples = bank_churn_folds_rf, metrics = bank_churn_metrics_rf)
bank_churn_rs_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.863    10 0.00142 Preprocessor1_Model1
## 2 roc_auc  binary     0.847    10 0.00379 Preprocessor1_Model1
## 3 sens     binary     0.964    10 0.00186 Preprocessor1_Model1
## 4 spec     binary     0.451    10 0.00889 Preprocessor1_Model1</code></pre>
<p><strong>Detailed cross_validation results</strong></p>
<pre class="r"><code>bank_churn_rs_metrics_rf &lt;-bank_churn_rs_fit_rf %&gt;%
  collect_metrics(summarize = FALSE)
bank_churn_rs_metrics_rf </code></pre>
<pre><code>## # A tibble: 40 x 5
##    id     .metric  .estimator .estimate .config             
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
##  1 Fold01 sens     binary         0.961 Preprocessor1_Model1
##  2 Fold01 spec     binary         0.5   Preprocessor1_Model1
##  3 Fold01 accuracy binary         0.870 Preprocessor1_Model1
##  4 Fold01 roc_auc  binary         0.837 Preprocessor1_Model1
##  5 Fold02 sens     binary         0.958 Preprocessor1_Model1
##  6 Fold02 spec     binary         0.467 Preprocessor1_Model1
##  7 Fold02 accuracy binary         0.860 Preprocessor1_Model1
##  8 Fold02 roc_auc  binary         0.834 Preprocessor1_Model1
##  9 Fold03 sens     binary         0.966 Preprocessor1_Model1
## 10 Fold03 spec     binary         0.434 Preprocessor1_Model1
## # ... with 30 more rows</code></pre>
<p><strong>Summarizing cross validation results</strong></p>
<pre class="r"><code>bank_churn_rs_metrics_rf %&gt;%
  group_by(.metric) %&gt;%
  summarize(min= min(.estimate),
            median = median(.estimate),
            max = max(.estimate),
            sd = sd(.estimate))</code></pre>
<pre><code>##        min   median       max        sd
## 1 0.410596 0.860222 0.9739414 0.1987228</code></pre>
</div>
<div id="hyper-parameter-tuning" class="section level1">
<h1>Hyper parameter tuning</h1>
<pre class="r"><code>bank_churn_rf_tune_model &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)
bank_churn_rf_tune_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
<p><strong>Creating tuning workflow</strong></p>
<pre class="r"><code>bank_churn_tune_wkfl_rf &lt;- bank_churn_wkfl_rf %&gt;%
  update_model(bank_churn_rf_tune_model)
bank_churn_tune_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
<p><strong>Identifying hyperparameters</strong></p>
<pre class="r"><code>parameters(bank_churn_rf_tune_model)</code></pre>
<pre><code>## Collection of 3 parameters for tuning
## 
##  identifier  type    object
##        mtry  mtry nparam[?]
##       trees trees nparam[+]
##       min_n min_n nparam[+]
## 
## Model parameters needing finalization:
##    # Randomly Selected Predictors (&#39;mtry&#39;)
## 
## See `?dials::finalize` or `?dials::update.parameters` for more information.</code></pre>
<p><strong>Hyper parameter tuning with cross validation</strong></p>
<pre class="r"><code>bank_churn_rf_tuning &lt;- bank_churn_tune_wkfl_rf %&gt;%
  tune_grid(resamples= bank_churn_folds_rf,  metrics = bank_churn_metrics_rf)</code></pre>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<pre class="r"><code>bank_churn_rf_tuning</code></pre>
<pre><code>## # Tuning results
## # 10-fold cross-validation using stratification 
## # A tibble: 10 x 4
##    splits             id     .metrics          .notes          
##    &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          
##  1 &lt;split [6887/767]&gt; Fold01 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  2 &lt;split [6888/766]&gt; Fold02 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  3 &lt;split [6888/766]&gt; Fold03 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  4 &lt;split [6889/765]&gt; Fold04 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  5 &lt;split [6889/765]&gt; Fold05 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  6 &lt;split [6889/765]&gt; Fold06 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  7 &lt;split [6889/765]&gt; Fold07 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  8 &lt;split [6889/765]&gt; Fold08 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  9 &lt;split [6889/765]&gt; Fold09 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
## 10 &lt;split [6889/765]&gt; Fold10 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;</code></pre>
<p><strong>Exploring tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 40 x 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config            
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              
##  1     6   374    30 accuracy binary     0.863    10 0.00205 Preprocessor1_Mode~
##  2     6   374    30 roc_auc  binary     0.848    10 0.00447 Preprocessor1_Mode~
##  3     6   374    30 sens     binary     0.963    10 0.00173 Preprocessor1_Mode~
##  4     6   374    30 spec     binary     0.456    10 0.0117  Preprocessor1_Mode~
##  5     6  1474    21 accuracy binary     0.862    10 0.00169 Preprocessor1_Mode~
##  6     6  1474    21 roc_auc  binary     0.848    10 0.00414 Preprocessor1_Mode~
##  7     6  1474    21 sens     binary     0.961    10 0.00186 Preprocessor1_Mode~
##  8     6  1474    21 spec     binary     0.457    10 0.0122  Preprocessor1_Mode~
##  9    10  1621    33 accuracy binary     0.862    10 0.00212 Preprocessor1_Mode~
## 10    10  1621    33 roc_auc  binary     0.844    10 0.00448 Preprocessor1_Mode~
## # ... with 30 more rows</code></pre>
<p><strong>Detailed tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE)</code></pre>
<pre><code>## # A tibble: 400 x 8
##    id      mtry trees min_n .metric  .estimator .estimate .config              
##    &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1 Fold01     6   374    30 sens     binary         0.958 Preprocessor1_Model01
##  2 Fold01     6   374    30 spec     binary         0.520 Preprocessor1_Model01
##  3 Fold01     6   374    30 accuracy binary         0.871 Preprocessor1_Model01
##  4 Fold01     6   374    30 roc_auc  binary         0.835 Preprocessor1_Model01
##  5 Fold02     6   374    30 sens     binary         0.956 Preprocessor1_Model01
##  6 Fold02     6   374    30 spec     binary         0.454 Preprocessor1_Model01
##  7 Fold02     6   374    30 accuracy binary         0.856 Preprocessor1_Model01
##  8 Fold02     6   374    30 roc_auc  binary         0.839 Preprocessor1_Model01
##  9 Fold03     6   374    30 sens     binary         0.964 Preprocessor1_Model01
## 10 Fold03     6   374    30 spec     binary         0.454 Preprocessor1_Model01
## # ... with 390 more rows</code></pre>
<p><strong>Exploring tuning results</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE) %&gt;%
  filter(.metric == &#39;roc_auc&#39;) %&gt;%
  group_by (id) %&gt;%
  summarize( min_roc_auc = min(.estimate),
             median_roc_auc = median(.estimate),
             max_roc_auc = max(.estimate))</code></pre>
<pre><code>##   min_roc_auc median_roc_auc max_roc_auc
## 1   0.8148446      0.8426408   0.8776452</code></pre>
<p><strong>Viewing the best performing model</strong></p>
<pre class="r"><code>bank_churn_rf_tuning %&gt;%
  show_best(metric = &#39;roc_auc&#39;, n =5)</code></pre>
<pre><code>## # A tibble: 5 x 9
##    mtry trees min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1     4   839    37 roc_auc binary     0.852    10 0.00418 Preprocessor1_Model06
## 2     3  1969    11 roc_auc binary     0.851    10 0.00428 Preprocessor1_Model07
## 3     3   427    17 roc_auc binary     0.851    10 0.00418 Preprocessor1_Model05
## 4     6   374    30 roc_auc binary     0.848    10 0.00447 Preprocessor1_Model01
## 5     6  1474    21 roc_auc binary     0.848    10 0.00414 Preprocessor1_Model02</code></pre>
<p>Model 6 is the best model</p>
</div>
<div id="best-model-selection-with-parameter" class="section level1">
<h1>Best model selection with parameter</h1>
<pre class="r"><code>bank_churn_best_rf_model &lt;- bank_churn_rf_tuning %&gt;%
  select_best(metric = &#39;roc_auc&#39;)
bank_churn_best_rf_model </code></pre>
<pre><code>## # A tibble: 1 x 4
##    mtry trees min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1     4   839    37 Preprocessor1_Model06</code></pre>
<p>The best parameters are mtry is 4, trees is 839 and min_n is 37 and the best model is Preprocessor1_Model06</p>
<p><strong>Finalizing the workflow</strong></p>
<pre class="r"><code>final_bank_churn_wkfl_rf &lt;- bank_churn_tune_wkfl_rf %&gt;%
  finalize_workflow(bank_churn_best_rf_model)
final_bank_churn_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 4
##   trees = 839
##   min_n = 37
## 
## Computational engine: ranger</code></pre>
<p><strong>Model fitting</strong></p>
<pre class="r"><code>bank_churn_final_fit_rf &lt;- final_bank_churn_wkfl_rf %&gt;%
  last_fit(split = bank_churn_split)
bank_churn_final_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.862 Preprocessor1_Model1
## 2 roc_auc  binary         0.866 Preprocessor1_Model1</code></pre>
<p>We can see above that accuracy and roc-auc (87.72% &amp; 85.97% respectively) are improved compared to the model without tuning.</p>
<p><strong>collecting predictions</strong></p>
<pre class="r"><code>bank_churn_prediction_rf&lt;- bank_churn_final_fit_rf %&gt;%
  collect_predictions()
bank_churn_prediction_rf</code></pre>
<pre><code>## # A tibble: 1,915 x 7
##    id               .pred_0 .pred_1  .row .pred_class Exited .config            
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;  &lt;chr&gt;              
##  1 train/test split   0.413 0.587      18 1           0      Preprocessor1_Mode~
##  2 train/test split   0.972 0.0280     20 0           0      Preprocessor1_Mode~
##  3 train/test split   0.990 0.00988    21 0           0      Preprocessor1_Mode~
##  4 train/test split   0.913 0.0867     23 0           0      Preprocessor1_Mode~
##  5 train/test split   0.829 0.171      29 0           0      Preprocessor1_Mode~
##  6 train/test split   0.932 0.0676     31 0           0      Preprocessor1_Mode~
##  7 train/test split   0.758 0.242      38 0           0      Preprocessor1_Mode~
##  8 train/test split   0.710 0.290      42 0           0      Preprocessor1_Mode~
##  9 train/test split   0.616 0.384      44 0           0      Preprocessor1_Mode~
## 10 train/test split   0.708 0.292      50 0           0      Preprocessor1_Mode~
## # ... with 1,905 more rows</code></pre>
<p><strong>Confusion Matrix</strong></p>
<pre class="r"><code>conf_mat(bank_churn_prediction_rf, truth = Exited, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<p><strong>Correct predictions of test set</strong>
True negative is 1499 customers, who churned. True positive is 181 people, who was a loyal customer and did not churn.</p>
<p><strong>Classification error of test set</strong>
False positive is 37 people, who are predicted as not churned but actually churned. False negative is 198 people, who are predicted as churned but actually not churned and was loyal to bank.</p>
<p>Almost 80% of the customers churned</p>
</div>
<div id="observations-and-comments" class="section level1">
<h1>Observations and comments</h1>
<ol style="list-style-type: decimal">
<li><p>The important determinants to predict the exited are gender, age, balance, numofproducts and isactivemember.</p></li>
<li><p>The character integer variables are assigned as a factor.</p></li>
<li><p>stepAIC backward regression and boot strapping were used to select the best model.</p></li>
<li><p>Almost 80% of the customers churned and only 20% of the customers were loyal and stayed with the bank.</p></li>
<li><p>The accuracy and roc-auc is better in random forest model compared to logistic regression and support vector machine.</p></li>
<li><p>The variables such has age, creditscore, numofproducts had outliers and they were removed from the data before further processing in model.</p></li>
<li><p>We performed univariate analysis, bivariate analysis, bootstrapping, stepAIC, feature engineering, cross validation, hyper parameter tuning, random forest model, support vector machine and logistic regression model on banking churn data set.</p></li>
<li><p>When we did bivariate analysis on independent and dependent variables, only the best model variables such as gender, age, balance, numofproducts and isactivemember came as 99.9% significant and other variables are not fully significant. The same significant variables turned to a best model predictors as well for a whole model.</p></li>
<li><p>The stepAIC and bootstrapping selected geography, tenure, gender, age, balance, numofproducts and isactivemember but when we run geography and tenure were not significant and were removed from the final model.</p></li>
<li><p>We used future engineering to preprocess the training and testing data such as we normalized all numeric variables, we set the dummy variables to all character variables.</p></li>
<li><p>The important things for bank in bivariate analysis are as follows</p></li>
</ol>
<ol style="list-style-type: lower-roman">
<li><p>More number of customers churned are between the age 30 and 40 : the reason may be the product is not tuned to their expectations.</p></li>
<li><p>More number of customers churned had a credit score between 600 and 700 : the reason may be other banks might give a loan whereas this bank might not give a loan to people less than credit score 700. Since no details are provided, this was our assumption</p></li>
<li><p>The customers having 3 accounts had less churn rate compared to customers having 1 or 2 accounts with bank : bank should try to sell as much product as possible and do better service to retain more customers.</p></li>
<li><p>Though geography did not come as a determinant to predict the customer`s churning rate, France had highest churn rate of 4798 customer, which is almost half of the churning rate of a bank : There may some unique problem, France people might face. SO, bank should look into the aspect as well.</p></li>
<li><p>Churning rate was higher with male customer compared to female customers: Bank may come up with some unique product for male members to retain them.</p></li>
<li><p>It is surprising to notice that actively transacted customers(86%) have high percentage of churn rate against customers who have not transacted actively (75%) : This shows the poor service by a bank. Hence, bank should look into this immediately and resolve the service problem and give a better service to retain them. Retaining the existing customer is as important as getting a new customer.</p></li>
</ol>
</div>
