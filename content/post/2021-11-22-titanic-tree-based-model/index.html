---
title: Titanic Tree Based Model
author: ''
date: '2021-11-22'
slug: titanic-tree-based-model
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="a-report-to-predict-survivability-in-titanic" class="section level2">
<h2>A report to predict survivability in Titanic</h2>
<p>I use the decision tree and random forest method to predict the survivability</p>
</div>
<div id="table-of-contents" class="section level2">
<h2>Table of contents</h2>
<p>Executive Summary</p>
<p>Issues and Challenges</p>
<p>Univariate analysis on metric and non metric data</p>
<p>Bivariate analysis</p>
<p>Best model using bootstrap and logistic regression</p>
<p>Decision model</p>
<p>Random forest model</p>
<p>Observation and comments</p>
</div>
<div id="executive-summary" class="section level1">
<h1>Executive summary</h1>
<p>The Titanic sunk in 1912 after sideswiping an iceberg during its maiden voyage. I analysed the titanic3 data using decision tree and random forest to predict the survivability. I removed the variables body, cabin, name, ticket number, boat and home destination as it has more missing values and would not impact the survivability. The survived is the response variable that I try to find out. The variables age, fare, siblings or spouses aboard the Titanic (sibsp), parents and children aboard the Titanic (parcg) are numerical values. Sex is the nominal categorical value. Pclass is the ticket class, which is of ordinal categorical value. Embarked is a nominal categorical values.</p>
<p>The univariate anaysis was performed to remove the outliers. The box plot was used to visualize the outliers. Then, bivariate analysis was done to figure out if there is any correlation between dependant and independent variable and find out its significance. I performed feature engineering using tidy models to preporcess the data and used decision tree and random forest classification models to fit the model. I did re sampling using cross-validation to improve the model prediction and used confusion matric to validate the results.</p>
<p>There was a evident from the both the models that almost 60 percent of people did not survive and approximately only 40 percent of people survived. The accuracy level has been improved from 76% to 80% when we compare the decision tree model with random forest model.</p>
</div>
<div id="issues-and-challanges" class="section level1">
<h1>Issues and Challanges</h1>
<p>The greatest challenge was to figure our which all the variables play a role in predicting the response variable, survived. There were many NA values in the titanic3 data, which i cleared it out before modelling. The pclass and survived was initially treated as integers by a system but it was a categorical values. I converted them as a factor before processing it into a model.</p>
</div>
<div id="summary" class="section level1">
<h1>Summary</h1>
<pre class="r"><code>summary(titanic3)</code></pre>
<pre><code>##      pclass         survived         name               sex           
##  Min.   :1.000   Min.   :0.000   Length:1309        Length:1309       
##  1st Qu.:2.000   1st Qu.:0.000   Class :character   Class :character  
##  Median :3.000   Median :0.000   Mode  :character   Mode  :character  
##  Mean   :2.295   Mean   :0.382                                        
##  3rd Qu.:3.000   3rd Qu.:1.000                                        
##  Max.   :3.000   Max.   :1.000                                        
##                                                                       
##       age            sibsp            parch          ticket         
##  Min.   : 0.17   Min.   :0.0000   Min.   :0.000   Length:1309       
##  1st Qu.:21.00   1st Qu.:0.0000   1st Qu.:0.000   Class :character  
##  Median :28.00   Median :0.0000   Median :0.000   Mode  :character  
##  Mean   :29.88   Mean   :0.4989   Mean   :0.385                     
##  3rd Qu.:39.00   3rd Qu.:1.0000   3rd Qu.:0.000                     
##  Max.   :80.00   Max.   :8.0000   Max.   :9.000                     
##  NA&#39;s   :263                                                        
##       fare            cabin             embarked             boat          
##  Min.   :  0.000   Length:1309        Length:1309        Length:1309       
##  1st Qu.:  7.896   Class :character   Class :character   Class :character  
##  Median : 14.454   Mode  :character   Mode  :character   Mode  :character  
##  Mean   : 33.295                                                           
##  3rd Qu.: 31.275                                                           
##  Max.   :512.329                                                           
##  NA&#39;s   :1                                                                 
##       body        home.dest        
##  Min.   :  1.0   Length:1309       
##  1st Qu.: 72.0   Class :character  
##  Median :155.0   Mode  :character  
##  Mean   :160.8                     
##  3rd Qu.:256.0                     
##  Max.   :328.0                     
##  NA&#39;s   :1188</code></pre>
<p>We can see above the NA values are in age, fare and body but character variable NA values are not given above. Hence, we use misssmap fuction to visualize the NA values.</p>
</div>
<div id="cleaning-and-preparing-the-dataset" class="section level1">
<h1>Cleaning and preparing the dataset</h1>
<pre class="r"><code>missmap(titanic3, col = c(&quot;black&quot;, &quot;grey&quot;))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" />
We can see many NA values for body, cabin, boat, home.dest and age and we can intuitively decide that body, Cabin, boat, home.dest will not impact the survival. Age also has some NA values but it is likely that it will impact the survival and we will remove the NA values. Though there is no NA for name and ticket number, we can intuitively remove those as it will not impact the survival.</p>
</div>
<div id="removing-cabin-body-boat-and-home.dest" class="section level1">
<h1>Removing cabin, body, boat and home.dest</h1>
<pre class="r"><code>titanic3 &lt;- subset (titanic3, select = - cabin)
titanic3 &lt;- subset (titanic3, select = - body)
titanic3 &lt;- subset (titanic3, select = - boat)
titanic3 &lt;- subset (titanic3, select = - home.dest)
titanic3 &lt;- subset (titanic3, select = - name)
titanic3 &lt;- subset (titanic3, select = - ticket)</code></pre>
<p>I removed the cabin, body, boat, name, ticket number and home destination because it had many NA values and it would not help to predict the people who survived.</p>
</div>
<div id="dropping-the-missing-values" class="section level1">
<h1>Dropping the missing values</h1>
<pre class="r"><code>titanic3 = na.omit(titanic3)</code></pre>
<p>The variable age had some missing values but it might play the potential role to predict the survivability. That is why i removed only those rows which had NA values to keep the age variable in our model to predict the response variable.</p>
</div>
<div id="summary-after-removing-na" class="section level1">
<h1>summary after removing NA</h1>
<pre class="r"><code>summary(titanic3)</code></pre>
<pre><code>##      pclass         survived          sex                 age       
##  Min.   :1.000   Min.   :0.0000   Length:1043        Min.   : 0.17  
##  1st Qu.:1.000   1st Qu.:0.0000   Class :character   1st Qu.:21.00  
##  Median :2.000   Median :0.0000   Mode  :character   Median :28.00  
##  Mean   :2.209   Mean   :0.4075                      Mean   :29.81  
##  3rd Qu.:3.000   3rd Qu.:1.0000                      3rd Qu.:39.00  
##  Max.   :3.000   Max.   :1.0000                      Max.   :80.00  
##      sibsp            parch             fare          embarked        
##  Min.   :0.0000   Min.   :0.0000   Min.   :  0.00   Length:1043       
##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:  8.05   Class :character  
##  Median :0.0000   Median :0.0000   Median : 15.75   Mode  :character  
##  Mean   :0.5043   Mean   :0.4219   Mean   : 36.60                     
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.: 35.08                     
##  Max.   :8.0000   Max.   :6.0000   Max.   :512.33</code></pre>
<p>We can see above in the revised summary that there is no NA values. We get the minimum, 1st quartile, median, mean, 3rd quartile and maximum values of numerical values. We get the class type and length for the categorical values.</p>
</div>
<div id="descriptive-statistics" class="section level1">
<h1>Descriptive Statistics</h1>
<pre class="r"><code>descr(titanic3)</code></pre>
<pre><code>## Non-numerical variable(s) ignored: sex, embarked</code></pre>
<pre><code>## Descriptive Statistics  
## titanic3  
## N: 1043  
## 
##                         age      fare     parch    pclass     sibsp   survived
## ----------------- --------- --------- --------- --------- --------- ----------
##              Mean     29.81     36.60      0.42      2.21      0.50       0.41
##           Std.Dev     14.37     55.75      0.84      0.84      0.91       0.49
##               Min      0.17      0.00      0.00      1.00      0.00       0.00
##                Q1     21.00      8.05      0.00      1.00      0.00       0.00
##            Median     28.00     15.75      0.00      2.00      0.00       0.00
##                Q3     39.00     35.50      1.00      3.00      1.00       1.00
##               Max     80.00    512.33      6.00      3.00      8.00       1.00
##               MAD     11.86     12.16      0.00      1.48      0.00       0.00
##               IQR     18.00     27.03      1.00      2.00      1.00       1.00
##                CV      0.48      1.52      1.99      0.38      1.81       1.21
##          Skewness      0.41      4.11      2.65     -0.41      2.80       0.38
##       SE.Skewness      0.08      0.08      0.08      0.08      0.08       0.08
##          Kurtosis      0.15     23.52      9.27     -1.47     10.46      -1.86
##           N.Valid   1043.00   1043.00   1043.00   1043.00   1043.00    1043.00
##         Pct.Valid    100.00    100.00    100.00    100.00    100.00     100.00</code></pre>
<p>The above details gives the descriptive statistics for all the numerical variables.</p>
</div>
<div id="structure-of-the-data" class="section level1">
<h1>Structure of the data</h1>
<pre class="r"><code>str(titanic3)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1043 obs. of  8 variables:
##  $ pclass  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ survived: int  1 1 0 0 0 1 1 0 1 0 ...
##  $ sex     : chr  &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;male&quot; ...
##  $ age     : num  29 0.92 2 30 25 48 63 39 53 71 ...
##  $ sibsp   : int  0 1 1 1 1 0 1 0 2 0 ...
##  $ parch   : int  0 2 2 2 2 0 0 0 0 0 ...
##  $ fare    : num  211 152 152 152 152 ...
##  $ embarked: chr  &quot;S&quot; &quot;S&quot; &quot;S&quot; &quot;S&quot; ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:266] 16 38 41 47 60 70 71 75 81 107 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:266] &quot;16&quot; &quot;38&quot; &quot;41&quot; &quot;47&quot; ...</code></pre>
<p>The above provides the structure of our data. pclass is the ticket class and it is ordinal categorical value but it is taken as integer. Survived is a nominal categorical value but taken as integer. Hence, I change both as a categorical values</p>
</div>
<div id="changing-survived-and-pclass-as-categorical-value" class="section level1">
<h1>Changing survived and pclass as categorical value</h1>
<pre class="r"><code>titanic3$survived = factor(titanic3$survived)
titanic3$pclass = factor(titanic3$pclass)</code></pre>
<p>The survived and pclass data type has been changed as a categorical value</p>
</div>
<div id="revised-structure" class="section level1">
<h1>Revised structure</h1>
<pre class="r"><code>str(titanic3)</code></pre>
<pre><code>## &#39;data.frame&#39;:    1043 obs. of  8 variables:
##  $ pclass  : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ survived: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 1 1 1 2 2 1 2 1 ...
##  $ sex     : chr  &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;male&quot; ...
##  $ age     : num  29 0.92 2 30 25 48 63 39 53 71 ...
##  $ sibsp   : int  0 1 1 1 1 0 1 0 2 0 ...
##  $ parch   : int  0 2 2 2 2 0 0 0 0 0 ...
##  $ fare    : num  211 152 152 152 152 ...
##  $ embarked: chr  &quot;S&quot; &quot;S&quot; &quot;S&quot; &quot;S&quot; ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:266] 16 38 41 47 60 70 71 75 81 107 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:266] &quot;16&quot; &quot;38&quot; &quot;41&quot; &quot;47&quot; ...</code></pre>
<p>I recreated the structure of our data to make sure that all the variables are of correct data type before processing the model.</p>
</div>
<div id="univariate-analysis-on-numerical-data" class="section level1">
<h1>Univariate analysis on numerical data</h1>
<p>Let me analyze the numerical data first individually to find out its outliers and remove it.</p>
</div>
<div id="univariate-analysis-on-age" class="section level1">
<h1>Univariate analysis on age</h1>
<pre class="r"><code>descr(titanic3$age)</code></pre>
<pre><code>## Descriptive Statistics  
## titanic3$age  
## N: 1043  
## 
##                         age
## ----------------- ---------
##              Mean     29.81
##           Std.Dev     14.37
##               Min      0.17
##                Q1     21.00
##            Median     28.00
##                Q3     39.00
##               Max     80.00
##               MAD     11.86
##               IQR     18.00
##                CV      0.48
##          Skewness      0.41
##       SE.Skewness      0.08
##          Kurtosis      0.15
##           N.Valid   1043.00
##         Pct.Valid    100.00</code></pre>
<p>The above descriptive statistics provides an indication that there is a huge standard deviation due to the average age is only 29.81 but the maximum age is 80. Let me visualize its outliers using box plot. The people travelled range between 0.17 and 80 years of age.</p>
</div>
<div id="boxplot-of-age-and-its-outliers" class="section level1">
<h1>Boxplot of age and its outliers</h1>
<pre class="r"><code>age_outliers&lt;-boxplot.stats(titanic3$age)$out
boxplot(titanic3$age, main = &quot;Box plot of age&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(age_outliers, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>age_outliers</code></pre>
<pre><code>## [1] 71.0 80.0 76.0 70.0 71.0 67.0 70.0 70.5 74.0</code></pre>
<p>We can infer from the above box plot that age above 67 are outliers.</p>
</div>
<div id="removal-of-outliers-of-age" class="section level1">
<h1>Removal of outliers of age</h1>
<pre class="r"><code>titanic3 &lt;- subset (titanic3, age &lt; 67)</code></pre>
<p>I removed the age above 67 and updated the data set with the same name.</p>
</div>
<div id="univariate-analysis-on-sibsp" class="section level1">
<h1>Univariate analysis on sibsp</h1>
<pre class="r"><code>descr(titanic3$sibsp)</code></pre>
<pre><code>## Descriptive Statistics  
## titanic3$sibsp  
## N: 1034  
## 
##                       sibsp
## ----------------- ---------
##              Mean      0.51
##           Std.Dev      0.92
##               Min      0.00
##                Q1      0.00
##            Median      0.00
##                Q3      1.00
##               Max      8.00
##               MAD      0.00
##               IQR      1.00
##                CV      1.81
##          Skewness      2.79
##       SE.Skewness      0.08
##          Kurtosis     10.39
##           N.Valid   1034.00
##         Pct.Valid    100.00</code></pre>
<p>It can be seen from the above table that standard deviation is more than mean value due it its few values are outliers that are faw away from the average values.</p>
</div>
<div id="boxplot-of-sibsp-and-its-outliers" class="section level1">
<h1>Boxplot of sibsp and its outliers</h1>
<pre class="r"><code>sibsp_outliers&lt;-boxplot.stats(titanic3$sibsp)$out
boxplot(titanic3$sibsp, main = &quot;Box plot of sibsp&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(sibsp_outliers, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>sibsp_outliers</code></pre>
<pre><code>##  [1] 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 3 5 5 5 5 5 5 3 3 3 3 3 4 4 4 4 4 4 4 4 4
## [39] 4 8 3 3 3 3 3</code></pre>
<p>The above box plot visualize us that actual interquartile ranges between o and 2. The rest values are outliers. The siblings or spouse of the traveled passengers has less than 2 with them mostly.</p>
</div>
<div id="removal-of-outliers-of-sibsp" class="section level1">
<h1>Removal of outliers of sibsp</h1>
<pre class="r"><code>titanic3 &lt;- subset (titanic3, sibsp &lt; 3)</code></pre>
<p>I Removed siblings/ spouses greater than 3.</p>
</div>
<div id="univariate-analysis-on-parch" class="section level1">
<h1>Univariate analysis on parch</h1>
<pre class="r"><code>descr(titanic3$parch)</code></pre>
<pre><code>## Descriptive Statistics  
## titanic3$parch  
## N: 989  
## 
##                      parch
## ----------------- --------
##              Mean     0.37
##           Std.Dev     0.82
##               Min     0.00
##                Q1     0.00
##            Median     0.00
##                Q3     0.00
##               Max     6.00
##               MAD     0.00
##               IQR     0.00
##                CV     2.18
##          Skewness     3.02
##       SE.Skewness     0.08
##          Kurtosis    11.80
##           N.Valid   989.00
##         Pct.Valid   100.00</code></pre>
<p>It looks from the above statistics that people traveled without children or parents maximum except few traveled with them. The average is only 0.37 and we can see the 3rd quartile value is also 0 but the max value is 6. This implies that there is no uniformity in data. The below box plot will explain this visually better.</p>
</div>
<div id="boxplot-of-parch-and-its-outliers" class="section level1">
<h1>Boxplot of parch and its outliers</h1>
<pre class="r"><code>parch_outliers&lt;-boxplot.stats(titanic3$parch)$out
boxplot(titanic3$parch, main = &quot;Box plot of parch&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(parch_outliers, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>parch_outliers</code></pre>
<pre><code>##   [1] 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 1 1 2 2 1 2 1 2 1 1 1 4 4 2 1 1 1 1 1 1 1 1
##  [38] 1 1 1 1 1 1 1 1 2 2 1 1 1 1 2 2 2 3 3 2 1 1 2 1 1 1 2 1 1 1 2 1 1 1 2 1 1
##  [75] 1 1 1 1 3 2 1 1 2 1 1 1 2 2 1 1 1 2 1 1 2 1 1 1 2 1 1 2 1 1 2 2 2 2 1 1 1
## [112] 3 1 1 1 2 2 2 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 3 1 2 1 1 2 2 2 2 2 2 1
## [149] 1 1 1 5 5 5 5 1 1 1 3 1 1 1 1 1 1 1 1 2 1 1 2 1 1 2 2 2 2 2 1 1 2 2 2 3 3
## [186] 2 1 1 6 6 1 1 1 1 2 1 1 2 1 1 1 2 1 1 2 1 1 1 1 4 5 1 1 2 5 1 1 2 1 2 1 4
## [223] 4 1 1 1 1 1 1 2 1 2 2 1 1</code></pre>
<p>Though majority of the data are shown as outliers for parents and children aboard the Titanic, I feel that they might influence the survival, Hence, I did not remove the outliers of the data.</p>
</div>
<div id="univariate-analysis-on-fare" class="section level1">
<h1>Univariate analysis on fare</h1>
<pre class="r"><code>descr(titanic3$fare)</code></pre>
<pre><code>## Descriptive Statistics  
## titanic3$fare  
## N: 989  
## 
##                       fare
## ----------------- --------
##              Mean    35.71
##           Std.Dev    54.98
##               Min     0.00
##                Q1     8.05
##            Median    14.50
##                Q3    33.50
##               Max   512.33
##               MAD    10.26
##               IQR    25.45
##                CV     1.54
##          Skewness     4.25
##       SE.Skewness     0.08
##          Kurtosis    25.55
##           N.Valid   989.00
##         Pct.Valid   100.00</code></pre>
<p>It looks like that majority of people traveled were of economy travelers. The average fare is only $35.71 but the maximum fare is $512.33. That is the reason for high standard deviation of $54.98.</p>
</div>
<div id="boxplot-of-fare-and-its-outliers" class="section level1">
<h1>Boxplot of fare and its outliers</h1>
<pre class="r"><code>fare_outliers&lt;-boxplot.stats(titanic3$fare)$out
boxplot(titanic3$fare, main = &quot;Box plot of fare&quot;, boxwex = 0.6)
mtext(paste(&quot;Outliers: &quot;, paste(fare_outliers, collapse =&quot;, &quot;)), cex = 0.6)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>fare_outliers</code></pre>
<pre><code>##   [1] 211.3375 151.5500 151.5500 151.5500 151.5500  77.9583 227.5250 227.5250
##   [9]  78.8500 247.5208 247.5208  76.2917  75.2417 227.5250 221.7792  91.0792
##  [17]  91.0792 135.6333 164.8667 262.3750  76.2917 134.5000 512.3292 512.3292
##  [25] 120.0000 120.0000 120.0000 120.0000  78.8500 262.3750  86.5000 136.7792
##  [33] 136.7792 151.5500  83.1583  83.1583  83.1583 151.5500  81.8583  81.8583
##  [41]  81.8583 106.4250 247.5208 106.4250  83.1583 227.5250  78.2667 263.0000
##  [49] 263.0000 133.6500  79.2000  79.2000 211.5000  79.2000  89.1042 153.4625
##  [57] 153.4625  79.2000  76.7292  76.7292  83.4750  83.4750  76.7292  83.1583
##  [65]  93.5000  93.5000  77.9583  90.0000  90.0000 211.5000 211.3375 106.4250
##  [73] 512.3292  77.9583 146.5208 211.3375  86.5000  75.2417  82.1708  90.0000
##  [81]  90.0000  90.0000 113.2750 113.2750 113.2750 108.9000  93.5000 108.9000
##  [89] 108.9000  93.5000  83.1583 135.6333 211.3375  79.2000  86.5000 262.3750
##  [97] 262.3750 262.3750 262.3750 262.3750 153.4625  82.2667  82.2667 134.5000
## [105] 134.5000 134.5000 146.5208  78.2667 221.7792  79.6500  79.6500  79.6500
## [113] 110.8833 110.8833 110.8833 512.3292  75.2500  75.2500  77.2875  77.2875
## [121] 135.6333 164.8667 164.8667 164.8667 211.5000 211.5000 211.5000 134.5000
## [129] 135.6333  73.5000  73.5000  73.5000  73.5000  73.5000  73.5000  73.5000</code></pre>
<p>Here, fare also has more outliers but I did not remove the outliers as it will reduce my data set very less. I keep the file as it is and process further.</p>
</div>
<div id="univariate-analysis-on-categorical-value" class="section level1">
<h1>UNivariate analysis on categorical value</h1>
<p>I analyzed the categorical values using frequency distribution to figure out least distribution among the distributed values.</p>
</div>
<div id="frequency-distribution-of-pclass" class="section level1">
<h1>Frequency distribution of pclass</h1>
<pre class="r"><code>tab1(titanic3$pclass, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of pclass&quot;, xlab =&quot;pclass&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre><code>## titanic3$pclass : 
##         Frequency Percent Cum. percent
## 2             259    26.2         26.2
## 1             272    27.5         53.7
## 3             458    46.3        100.0
##   Total       989   100.0        100.0</code></pre>
<p>Almost half the people traveled in Titanic were from third class ticke. The first and second class ticket passengers were almost equal.</p>
</div>
<div id="frequency-distribution-of-survived" class="section level1">
<h1>Frequency distribution of survived</h1>
<pre class="r"><code>tab1(titanic3$survived, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of survived&quot;, xlab =&quot;survived&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>## titanic3$survived : 
##         Frequency Percent Cum. percent
## 1             414    41.9         41.9
## 0             575    58.1        100.0
##   Total       989   100.0        100.0</code></pre>
<p>The above the frequency distribution depicts that almost 60% of people did not survive and only around 40% of people survived.</p>
</div>
<div id="frequency-distribution-of-sex" class="section level1">
<h1>Frequency distribution of sex</h1>
<pre class="r"><code>tab1(titanic3$sex, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of sex&quot;, xlab =&quot;sex&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## titanic3$sex : 
##         Frequency Percent Cum. percent
## female        367    37.1         37.1
## male          622    62.9        100.0
##   Total       989   100.0        100.0</code></pre>
<p>The majority of people traveled in Titanic were male members with more than 60% percent between them. The female travelers were only less than 40%.</p>
</div>
<div id="frequency-distribution-of-embarked" class="section level1">
<h1>Frequency distribution of embarked</h1>
<p>Writing the embarked data in full words</p>
<pre class="r"><code>titanic3$embarked[titanic3$embarked == &quot;S&quot; ] &lt;-&quot;Southampton&quot;
titanic3$embarked[titanic3$embarked == &quot;C&quot; ] &lt;-&quot;Cherbourg&quot;
titanic3$embarked[titanic3$embarked == &quot;Q&quot; ] &lt;-&quot;Queenstown&quot;</code></pre>
<p>I have assigned the full form of the the embarked location to explain better.</p>
<pre class="r"><code>tab1(titanic3$embarked, sort.group = &quot;increasing&quot;, cum.percent = TRUE, main = &quot;Frequency distribution of embarked&quot;, xlab =&quot;embarked&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## titanic3$embarked : 
##             Frequency Percent Cum. percent
## Queenstown         44     4.4          4.4
## Cherbourg         210    21.2         25.7
## Southampton       735    74.3        100.0
##   Total           989   100.0        100.0</code></pre>
<p>The above viz explains that the majority of people embarked from Southampton (735), followed by Cherbourg (210). The people embarked from Queenstown were very minimal (44).</p>
</div>
<div id="bivariate-analysis-to-see-the-relationship-between-dependant-and-independant-variable" class="section level1">
<h1>Bivariate analysis to see the relationship between dependant and independant variable</h1>
<p>Here, I will analyze the relationship between response and explanatory variables.</p>
</div>
<div id="relationship-between-survived-and-pclass" class="section level1">
<h1>Relationship between survived and pclass</h1>
<pre class="r"><code>pclass_survived = glm(survived~ pclass, data = titanic3, family = &quot;binomial&quot;)
summary(pclass_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ pclass, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4289  -0.8022  -0.8022   0.9453   1.6066  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   0.5741     0.1263   4.545 5.48e-06 ***
## pclass2      -0.8146     0.1778  -4.581 4.62e-06 ***
## pclass3      -1.5429     0.1640  -9.408  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1249.7  on 986  degrees of freedom
## AIC: 1255.7
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The equation has significant model to predict the survivability.</p>
<pre class="r"><code>ggplot(titanic3, aes(x = survived, fill = pclass)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-28-1.png" width="672" />
The above data viz explains that among the people did not survive, class 3 passengers were more in numbers (332), followed by class2 (145) and class1. The class1 passengers were only 98 in numbers. When we see the people survived, on the contrary, class 1 people survived a most (174). The class 2 and class 3 people survived were almost same in numbers (114 and 126 respectively) .</p>
</div>
<div id="relationship-between-survived-and-sex" class="section level1">
<h1>Relationship between survived and sex</h1>
<pre class="r"><code>sex_survived = glm(survived~ sex, data = titanic3, family = &quot;binomial&quot;)
summary(sex_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ sex, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7104  -0.6907  -0.6907   0.7259   1.7608  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   1.1993     0.1237   9.692   &lt;2e-16 ***
## sexmale      -2.5109     0.1579 -15.903   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1040.2  on 987  degrees of freedom
## AIC: 1044.2
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>It is clear from the above regression that there is a relationship between survived and sex. The above equation is significant as well.</p>
<pre class="r"><code>ggplot(titanic3, aes(x = survived, fill = sex)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-30-1.png" width="672" />
We can infer from the above bar chart that more number of male members around 490 did not survive compared to only 85 female members. On the other hand, male members (132) survived were only half of the female survived (282).</p>
</div>
<div id="relationship-between-survived-and-embarked" class="section level1">
<h1>Relationship between survived and embarked</h1>
<pre class="r"><code>embarked_survived = glm(survived~ embarked, data = titanic3, family = &quot;binomial&quot;)
summary(embarked_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ embarked, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4074  -0.9547  -0.9547   1.4179   1.5616  
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)           0.5261     0.1428   3.684 0.000230 ***
## embarkedQueenstown   -1.3951     0.3600  -3.876 0.000106 ***
## embarkedSouthampton  -1.0756     0.1620  -6.637 3.19e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1296.0  on 986  degrees of freedom
## AIC: 1302
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>It is evident from the above regression that there is a significant relationship between survived and embarked.</p>
<pre class="r"><code>ggplot(titanic3, aes(x = survived, fill = embarked)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-32-1.png" width="672" />
Between the survived and not survived, the people embarked from Southampton were the highest with people not survived was 466 and people survived were 269. The second most people survived and not survived were those who embarked from Cherbourg with survived numbers at 132 and not survived numbers at 78. The least impact for those who embarked from Queenstown as people did not survive were only 31 and survived people were 13 in numbers.</p>
</div>
<div id="relationship-between-survived-and-age" class="section level1">
<h1>Relationship between survived and age</h1>
<pre class="r"><code>age_survived = glm(survived~ age, data = titanic3, family = &quot;binomial&quot;)
summary(age_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ age, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.220  -1.059  -0.951   1.282   1.539  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.10492    0.15887   0.660  0.50900   
## age         -0.01443    0.00487  -2.964  0.00303 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1335.8  on 987  degrees of freedom
## AIC: 1339.8
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The age has some relationship with survival of Titanic though the intercept is not significant.</p>
<pre class="r"><code>titanic3$discretized.age = cut(titanic3$age, c(0, 10, 20, 30, 40, 50, 60, 70))
ggplot(titanic3, aes(x = discretized.age, fill = survived)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-34-1.png" width="672" />
The highest number of people survived and did not survive were from the age group of between 20 and 30 followed by age group 30 and 40, age group 10 and 20, age group 40 and 50, age group 50 and 60 and age group 60 and 70. While all the age group people have more numbers did not survive, on the contrary, the age group between 0 and 12 have more number of people survived than not survived.</p>
</div>
<div id="relationship-between-survived-and-sibsp" class="section level1">
<h1>Relationship between survived and sibsp</h1>
<pre class="r"><code>sibsp_survived = glm(survived~ sibsp, data = titanic3, family = &quot;binomial&quot;)
summary(sibsp_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ sibsp, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3459  -0.9792  -0.9792   1.3895   1.3895  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.48598    0.07781  -6.246 4.21e-10 ***
## sibsp        0.43694    0.11759   3.716 0.000203 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1330.8  on 987  degrees of freedom
## AIC: 1334.8
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>There is a significant relationship between sibsp and survived</p>
<pre class="r"><code>ggplot(titanic3, aes(x = sibsp, fill = survived)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-36-1.png" width="672" />
We can infer from the above bar chart that those who traveled without siblings or spouse were the highest numbers in both people who survived (424) and who did not survive (252) among others who traveled with different number of siblings. The people who traveled with one siblings or spouse has 131 numbers survived and 146 people not survived. At the last, those who travelled with 2 siblings or spouse had very less number in both survived (20) and not survived (16).</p>
</div>
<div id="relationship-between-survived-and-parch" class="section level1">
<h1>Relationship between survived and parch</h1>
<pre class="r"><code>parch_survived = glm(survived~ parch, data = titanic3, family = &quot;binomial&quot;)
summary(parch_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ parch, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0173  -0.9827  -0.9827   1.3855   1.3855  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.47706    0.07228  -6.600 4.10e-11 ***
## parch        0.39528    0.08608   4.592 4.39e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1321.3  on 987  degrees of freedom
## AIC: 1325.3
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The above equation has significant relationship between those who traveled with parents or children and survived.</p>
<pre class="r"><code>ggplot(titanic3, aes(x = parch, fill = survived)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-38-1.png" width="672" />
The people who traveled alone without their parents or children had more death rate that is around 490 people who did not survive and around 264 people who did survive. On the contrary, those who traveled with one or 2 children and parent had more survival rate and less death rate compared to people traveled alone. Those who traveled with above 2 children or parent had very meagre number of people.</p>
</div>
<div id="relationship-between-survived-and-fare" class="section level1">
<h1>Relationship between survived and fare</h1>
<pre class="r"><code>fare_survived = glm(survived~ fare, data = titanic3, family = &quot;binomial&quot;)
summary(fare_survived)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ fare, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3625  -0.9349  -0.9087   1.3343   1.5188  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -0.774275   0.087123  -8.887  &lt; 2e-16 ***
## fare         0.013314   0.001866   7.136  9.6e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.7  on 988  degrees of freedom
## Residual deviance: 1269.3  on 987  degrees of freedom
## AIC: 1273.3
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The above logistic regression explains that there is a significant relationship between fare and survived.</p>
<pre class="r"><code>titanic3$discretized.fare = cut(titanic3$fare, c(0, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550))
ggplot(titanic3, aes(x = discretized.fare, fill = survived)) + geom_bar(position = position_dodge())+geom_text(stat = &#39;count&#39;, aes(label = stat(count)), position = position_dodge(width =1), vjust = -0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-40-1.png" width="672" />
The highest number of people did not survive when the fare ranged between 0 and $50. Though the numbers were quite less for fare greater than $50 compared to lowest fare, they had more number of people survived against people not survived.</p>
</div>
<div id="full-model-logistic-regression" class="section level1">
<h1>Full model logistic regression</h1>
<pre class="r"><code>full_model_titanic = glm(survived ~ sex+ pclass + age + sibsp + parch + fare + embarked, family = &quot;binomial&quot;, data = titanic3)
summary(full_model_titanic)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ sex + pclass + age + sibsp + parch + 
##     fare + embarked, family = &quot;binomial&quot;, data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6926  -0.6726  -0.4163   0.6360   2.4561  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)          4.338655   0.443907   9.774  &lt; 2e-16 ***
## sexmale             -2.605991   0.184050 -14.159  &lt; 2e-16 ***
## pclass2             -1.155728   0.272573  -4.240 2.23e-05 ***
## pclass3             -2.044953   0.281116  -7.274 3.48e-13 ***
## age                 -0.042651   0.007109  -5.999 1.98e-09 ***
## sibsp               -0.142084   0.159305  -0.892  0.37245    
## parch                0.070881   0.104504   0.678  0.49760    
## fare                 0.000097   0.002023   0.048  0.96175    
## embarkedQueenstown  -1.417620   0.454546  -3.119  0.00182 ** 
## embarkedSouthampton -0.669801   0.219581  -3.050  0.00229 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.72  on 988  degrees of freedom
## Residual deviance:  906.86  on 979  degrees of freedom
## AIC: 926.86
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="backward-regression" class="section level1">
<h1>Backward Regression</h1>
<pre class="r"><code>mod_step_titanic&lt;- stepAIC(full_model_titanic, direction = &#39;backward&#39;, trace = FALSE)
mod_step_titanic</code></pre>
<pre><code>## 
## Call:  glm(formula = survived ~ sex + pclass + age + embarked, family = &quot;binomial&quot;, 
##     data = titanic3)
## 
## Coefficients:
##         (Intercept)              sexmale              pclass2  
##              4.2995              -2.6090              -1.1488  
##             pclass3                  age   embarkedQueenstown  
##             -2.0214              -0.0423              -1.4099  
## embarkedSouthampton  
##             -0.6732  
## 
## Degrees of Freedom: 988 Total (i.e. Null);  982 Residual
## Null Deviance:       1345 
## Residual Deviance: 907.9     AIC: 921.9</code></pre>
</div>
<div id="bootstrap-method" class="section level1">
<h1>Bootstrap method</h1>
<p>Using bootstrap re sampling with replacement method to access the consistency predictors selected with stepwise</p>
<pre class="r"><code>mod_boot_titanic&lt;- boot.stepAIC(full_model_titanic, titanic3, B =50)</code></pre>
<div id="bootstrap-summary" class="section level2">
<h2>Bootstrap summary</h2>
<pre class="r"><code>print(mod_boot_titanic)</code></pre>
<pre><code>## 
## Summary of Bootstrapping the &#39;stepAIC()&#39; procedure for
## 
## Call:
## glm(formula = survived ~ sex + pclass + age + sibsp + parch + 
##     fare + embarked, family = &quot;binomial&quot;, data = titanic3)
## 
## Bootstrap samples: 50 
## Direction: backward 
## Penalty: 2 * df
## 
## Covariates selected
##          (%)
## age      100
## pclass   100
## sex      100
## embarked  96
## sibsp     28
## parch     24
## fare      10
## 
## Coefficients Sign
##                     + (%) - (%)
## parch                 100     0
## fare                   20    80
## age                     0   100
## embarkedQueenstown      0   100
## embarkedSouthampton     0   100
## pclass2                 0   100
## pclass3                 0   100
## sexmale                 0   100
## sibsp                   0   100
## 
## Stat Significance
##                        (%)
## age                 100.00
## pclass2             100.00
## pclass3             100.00
## sexmale             100.00
## embarkedQueenstown   85.42
## embarkedSouthampton  83.33
## sibsp                42.86
## parch                41.67
## fare                  0.00
## 
## 
## The stepAIC() for the original data-set gave
## 
## Call:  glm(formula = survived ~ sex + pclass + age + embarked, family = &quot;binomial&quot;, 
##     data = titanic3)
## 
## Coefficients:
##         (Intercept)              sexmale              pclass2  
##              4.2995              -2.6090              -1.1488  
##             pclass3                  age   embarkedQueenstown  
##             -2.0214              -0.0423              -1.4099  
## embarkedSouthampton  
##             -0.6732  
## 
## Degrees of Freedom: 988 Total (i.e. Null);  982 Residual
## Null Deviance:       1345 
## Residual Deviance: 907.9     AIC: 921.9
## 
## Stepwise Model Path 
## Analysis of Deviance Table
## 
## Initial Model:
## survived ~ sex + pclass + age + sibsp + parch + fare + embarked
## 
## Final Model:
## survived ~ sex + pclass + age + embarked
## 
## 
##      Step Df    Deviance Resid. Df Resid. Dev      AIC
## 1                              979   906.8551 926.8551
## 2  - fare  1 0.002305189       980   906.8574 924.8574
## 3 - parch  1 0.500182867       981   907.3576 923.3576
## 4 - sibsp  1 0.587021105       982   907.9446 921.9446</code></pre>
</div>
</div>
<div id="best-model" class="section level1">
<h1>Best model</h1>
<pre class="r"><code>best_bootmodel_titanic&lt;- glm(formula = survived ~ sex + pclass + age + embarked, family = &quot;binomial&quot;, 
    data = titanic3)
summary(best_bootmodel_titanic)</code></pre>
<pre><code>## 
## Call:
## glm(formula = survived ~ sex + pclass + age + embarked, family = &quot;binomial&quot;, 
##     data = titanic3)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6722  -0.6765  -0.4167   0.6456   2.4567  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)          4.299485   0.379980  11.315  &lt; 2e-16 ***
## sexmale             -2.608988   0.177793 -14.674  &lt; 2e-16 ***
## pclass2             -1.148767   0.247670  -4.638 3.51e-06 ***
## pclass3             -2.021354   0.242631  -8.331  &lt; 2e-16 ***
## age                 -0.042301   0.007068  -5.985 2.17e-09 ***
## embarkedQueenstown  -1.409888   0.449752  -3.135  0.00172 ** 
## embarkedSouthampton -0.673197   0.215988  -3.117  0.00183 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1344.72  on 988  degrees of freedom
## Residual deviance:  907.94  on 982  degrees of freedom
## AIC: 921.94
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The above is the best and significant model to predict the survivability. The sex, pclass, age and embarked are the predictors that predict the survived.</p>
</div>
<div id="data-resampling" class="section level1">
<h1>Data resampling</h1>
<pre class="r"><code>titanic_split &lt;- initial_split (titanic3, prop = 0.80, strata = survived)
titanic_training &lt;- titanic_split %&gt;%
  training()
titanic_test &lt;- titanic_split %&gt;%
  testing()</code></pre>
<p>I have taken 80:20 split for data sampling. I have made training data to fit the model and test the model using test set.</p>
</div>
<div id="checking-number-of-rows-in-training-and-test-data" class="section level1">
<h1>Checking number of rows in training and test data</h1>
<pre class="r"><code>nrow(titanic_training)</code></pre>
<pre><code>## [1] 791</code></pre>
<pre class="r"><code>nrow(titanic_test)</code></pre>
<pre><code>## [1] 198</code></pre>
<p>There are total 791 numbers of row in training data and 198 numbers of row in test data.</p>
</div>
<div id="checking-multicollinearity-between-numerical-values-in-a-training-titanic-data-set" class="section level1">
<h1>Checking multicollinearity between numerical values in a training titanic data set</h1>
<pre class="r"><code>titanic_training %&gt;%
  select_if(is.numeric) %&gt;%
  cor()</code></pre>
<pre><code>##               age       sibsp       parch      fare
## age    1.00000000 -0.06597178 -0.07403691 0.1970428
## sibsp -0.06597178  1.00000000  0.27441038 0.1594575
## parch -0.07403691  0.27441038  1.00000000 0.2064577
## fare   0.19704276  0.15945751  0.20645771 1.0000000</code></pre>
<p>There is no multi-colinearity between the independent variables</p>
</div>
<div id="decision-tree-model" class="section level1">
<h1>Decision tree Model</h1>
</div>
<div id="model-specification" class="section level1">
<h1>Model specification</h1>
<pre class="r"><code>titanic_dt_model&lt;- decision_tree() %&gt;%
  set_engine(&#39;rpart&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
</div>
<div id="future-engineering" class="section level1">
<h1>Future engineering</h1>
<p>This is the step to pre-processing of data</p>
<pre class="r"><code>titanic_recipe_dt &lt;- recipe(survived ~ sex+ pclass + age + sibsp + parch + fare + embarked, data = titanic_training) %&gt;%
  step_corr(all_numeric(), threshold =0.8) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  step_dummy(all_nominal(), -all_outcomes())
titanic_recipe_dt</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          7
## 
## Operations:
## 
## Correlation filter on all_numeric()
## Centering and scaling for all_numeric()
## Dummy variables from all_nominal(), -all_outcomes()</code></pre>
<p>There are 7 predictor variable and 1 outcome variable.</p>
</div>
<div id="recipe-training" class="section level1">
<h1>Recipe training</h1>
<pre class="r"><code>titanic_recipe_prep_dt&lt;- titanic_recipe_dt %&gt;%
  prep(training = titanic_training)
titanic_recipe_prep_dt</code></pre>
<pre><code>## Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          7
## 
## Training data contained 791 data points and no missing data.
## 
## Operations:
## 
## Correlation filter removed no terms [trained]
## Centering and scaling for age, sibsp, parch, fare [trained]
## Dummy variables from sex, pclass, embarked [trained]</code></pre>
<p>I have prepared the training data set.</p>
</div>
<div id="preprocess-training-data" class="section level1">
<h1>Preprocess training data</h1>
<pre class="r"><code>titanic_training_prep_dt &lt;- titanic_recipe_prep_dt %&gt;%
  bake (new_data = NULL)
titanic_training_prep_dt</code></pre>
<pre><code>## # A tibble: 791 x 10
##        age  sibsp  parch   fare survived sex_male pclass_X2 pclass_X3
##      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 -2.10    1.18   1.88   1.99  0               0         0         0
##  2 -0.0469  1.18   1.88   1.99  0               1         0         0
##  3 -0.413   1.18   1.88   1.99  0               0         0         0
##  4  0.612  -0.635 -0.460 -0.633 0               1         0         0
##  5  1.20    1.18  -0.460  3.30  0               1         0         0
##  6  0.392  -0.635 -0.460  0.669 0               1         0         0
##  7 -0.413  -0.635 -0.460 -0.183 0               1         0         0
##  8  0.831  -0.635 -0.460 -0.173 0               1         0         0
##  9  0.758  -0.635 -0.460 -0.105 0               1         0         0
## 10  1.05   -0.635 -0.460 -0.173 0               1         0         0
## # ... with 781 more rows, and 2 more variables: embarked_Queenstown &lt;dbl&gt;,
## #   embarked_Southampton &lt;dbl&gt;</code></pre>
<p>I pre-processed the training data set.</p>
</div>
<div id="preprocess-test-data" class="section level1">
<h1>Preprocess test data</h1>
<pre class="r"><code>titanic_test_prep_dt &lt;- titanic_recipe_prep_dt %&gt;%
  bake (new_data = titanic_test)
titanic_test_prep_dt</code></pre>
<pre><code>## # A tibble: 198 x 10
##       age  sibsp  parch    fare survived sex_male pclass_X2 pclass_X3
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 -0.925  1.18  -0.460  3.30   1               0         0         0
##  2 -0.486 -0.635 -0.460  0.567  1               0         0         0
##  3 -0.486 -0.635  0.710  3.65   0               1         0         0
##  4  1.20   1.18   0.710  0.277  1               0         0         0
##  5 -0.852  1.18  -0.460  0.944  1               0         0         0
##  6 -0.193 -0.635 -0.460 -0.173  1               1         0         0
##  7  1.05  -0.635 -0.460 -0.0184 0               1         0         0
##  8 -0.632 -0.635  0.710  0.319  1               0         0         0
##  9  1.27  -0.635 -0.460  0.241  0               1         0         0
## 10  2.08   2.99  -0.460  0.258  1               0         0         0
## # ... with 188 more rows, and 2 more variables: embarked_Queenstown &lt;dbl&gt;,
## #   embarked_Southampton &lt;dbl&gt;</code></pre>
<p>I pre-processed the test data.</p>
</div>
<div id="model-fitting" class="section level1">
<h1>Model fitting</h1>
<pre class="r"><code>titanic_fit_dt &lt;- titanic_dt_model %&gt;%
  fit(survived ~ ., data = titanic_training_prep_dt)
titanic_fit_dt</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  10ms 
## n= 791 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 791 331 0 (0.58154235 0.41845765)  
##     2) sex_male&gt;=0.5 506 113 0 (0.77667984 0.22332016)  
##       4) age&gt;=-1.254271 480  91 0 (0.81041667 0.18958333) *
##       5) age&lt; -1.254271 26   4 1 (0.15384615 0.84615385) *
##     3) sex_male&lt; 0.5 285  67 1 (0.23508772 0.76491228)  
##       6) pclass_X3&gt;=0.5 108  54 0 (0.50000000 0.50000000)  
##        12) fare&gt;=-0.2331857 10   1 0 (0.90000000 0.10000000) *
##        13) fare&lt; -0.2331857 98  45 1 (0.45918367 0.54081633)  
##          26) age&gt;=-1.034747 77  37 0 (0.51948052 0.48051948)  
##            52) fare&gt;=-0.4989895 67  29 0 (0.56716418 0.43283582)  
##             104) parch&lt; 0.1250288 47  17 0 (0.63829787 0.36170213) *
##             105) parch&gt;=0.1250288 20   8 1 (0.40000000 0.60000000) *
##            53) fare&lt; -0.4989895 10   2 1 (0.20000000 0.80000000) *
##          27) age&lt; -1.034747 21   5 1 (0.23809524 0.76190476) *
##       7) pclass_X3&lt; 0.5 177  13 1 (0.07344633 0.92655367) *</code></pre>
<p>I have trained the training data set.</p>
</div>
<div id="predicting-outcome-variables" class="section level1">
<h1>Predicting outcome variables</h1>
<pre class="r"><code>titanic_class_preds &lt;- predict(titanic_fit_dt, new_data = titanic_test_prep_dt, type = &quot;class&quot;)
titanic_class_preds</code></pre>
<pre><code>## # A tibble: 198 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 1          
##  2 1          
##  3 0          
##  4 1          
##  5 1          
##  6 0          
##  7 0          
##  8 1          
##  9 0          
## 10 1          
## # ... with 188 more rows</code></pre>
<p>I predicted the outcome variable using test data set.</p>
</div>
<div id="estimated-probabilities" class="section level1">
<h1>Estimated probabilities</h1>
<pre class="r"><code>titanic_prob_preds &lt;- predict(titanic_fit_dt, new_data = titanic_test_prep_dt, type = &quot;prob&quot;)
titanic_prob_preds</code></pre>
<pre><code>## # A tibble: 198 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1  0.0734   0.927
##  2  0.0734   0.927
##  3  0.810    0.190
##  4  0.0734   0.927
##  5  0.0734   0.927
##  6  0.810    0.190
##  7  0.810    0.190
##  8  0.0734   0.927
##  9  0.810    0.190
## 10  0.0734   0.927
## # ... with 188 more rows</code></pre>
<p>I predicted the estimated probabilities using test data set.</p>
</div>
<div id="combining-results" class="section level1">
<h1>Combining results</h1>
<pre class="r"><code>titanic_results &lt;- titanic_test_prep_dt %&gt;%
  bind_cols(titanic_class_preds, titanic_prob_preds)
titanic_results</code></pre>
<pre><code>## # A tibble: 198 x 13
##       age  sibsp  parch    fare survived sex_male pclass_X2 pclass_X3
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 -0.925  1.18  -0.460  3.30   1               0         0         0
##  2 -0.486 -0.635 -0.460  0.567  1               0         0         0
##  3 -0.486 -0.635  0.710  3.65   0               1         0         0
##  4  1.20   1.18   0.710  0.277  1               0         0         0
##  5 -0.852  1.18  -0.460  0.944  1               0         0         0
##  6 -0.193 -0.635 -0.460 -0.173  1               1         0         0
##  7  1.05  -0.635 -0.460 -0.0184 0               1         0         0
##  8 -0.632 -0.635  0.710  0.319  1               0         0         0
##  9  1.27  -0.635 -0.460  0.241  0               1         0         0
## 10  2.08   2.99  -0.460  0.258  1               0         0         0
## # ... with 188 more rows, and 5 more variables: embarked_Queenstown &lt;dbl&gt;,
## #   embarked_Southampton &lt;dbl&gt;, .pred_class &lt;fct&gt;, .pred_0 &lt;dbl&gt;, .pred_1 &lt;dbl&gt;</code></pre>
<p>I have combined the outcome and probabilities with test data set.</p>
</div>
<div id="assessing-model-fit-using-confusion-matrix" class="section level1">
<h1>Assessing model fit using confusion matrix</h1>
<pre class="r"><code>titanic_results %&gt;%
  conf_mat(truth = survived, estimate = .pred_class) %&gt;%
autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-59-1.png" width="672" />
The above is the confusion matrix of test data set. We can see that 100 is a true negative, which means that people did not survive. The true positive is only 56, those who survived. The false positive is 15, which means that it was predicted as people survived but actually not survived. The false negative is 27, which means that it was predicted as not survived but actually survived.</p>
</div>
<div id="combining-models-and-recipe" class="section level1">
<h1>Combining models and recipe</h1>
<pre class="r"><code>titanic_wkfl_dt&lt;- workflow() %&gt;%
  add_model(titanic_dt_model) %&gt;%
  add_recipe(titanic_recipe_dt)
titanic_wkfl_dt</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: decision_tree()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Decision Tree Model Specification (classification)
## 
## Computational engine: rpart</code></pre>
<p>I have combined the model and recipe.</p>
</div>
<div id="model-fitting-with-workflow" class="section level1">
<h1>Model fitting with workflow</h1>
<pre class="r"><code>titanic_wkfl_fit_dt &lt;- titanic_wkfl_dt %&gt;%
  last_fit(split = titanic_split)
titanic_wkfl_fit_dt %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.859 Preprocessor1_Model1
## 2 roc_auc  binary         0.880 Preprocessor1_Model1</code></pre>
<p>The accuracy of the model is 78% and the roc_auc is 81%.</p>
</div>
<div id="collecting-predictions" class="section level1">
<h1>Collecting predictions</h1>
<pre class="r"><code>titanic_wkfl_preds_dt &lt;- titanic_wkfl_fit_dt %&gt;%
  collect_predictions()
titanic_wkfl_preds_dt</code></pre>
<pre><code>## # A tibble: 198 x 7
##    id               .pred_0 .pred_1  .row .pred_class survived .config          
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;            
##  1 train/test split  0.0734   0.927    11 1           1        Preprocessor1_Mo~
##  2 train/test split  0.0734   0.927    12 1           1        Preprocessor1_Mo~
##  3 train/test split  0.810    0.190    14 0           0        Preprocessor1_Mo~
##  4 train/test split  0.0734   0.927    19 1           1        Preprocessor1_Mo~
##  5 train/test split  0.0734   0.927    25 1           1        Preprocessor1_Mo~
##  6 train/test split  0.810    0.190    27 0           1        Preprocessor1_Mo~
##  7 train/test split  0.810    0.190    28 0           0        Preprocessor1_Mo~
##  8 train/test split  0.0734   0.927    34 1           1        Preprocessor1_Mo~
##  9 train/test split  0.810    0.190    36 0           0        Preprocessor1_Mo~
## 10 train/test split  0.0734   0.927    38 1           1        Preprocessor1_Mo~
## # ... with 188 more rows</code></pre>
</div>
<div id="confusion-matrix" class="section level1">
<h1>Confusion matrix</h1>
<pre class="r"><code>conf_mat(titanic_wkfl_preds_dt, truth = survived, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-63-1.png" width="672" />
# Correct predictions
True negative is 100 people, who did not survive. True positive is 56 people survived.</p>
</div>
<div id="classification-error" class="section level1">
<h1>Classification error</h1>
<p>False positive is 15 people, who are predicted as survived but actually not survived. False negative is 27 people, who are predicted as dead but actually survived.</p>
<p>More number of people not survived in Titanic</p>
</div>
<div id="exploring-custom-metrics" class="section level1">
<h1>Exploring custom metrics</h1>
<pre class="r"><code>titanic_metrics_dt &lt;- metric_set(roc_auc, sens, spec, accuracy)
titanic_wkfl_preds_dt %&gt;%
titanic_metrics_dt(truth = survived, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.922
## 2 spec     binary         0.771
## 3 accuracy binary         0.859
## 4 roc_auc  binary         0.120</code></pre>
<p>The above are metrics to measure the model. The accuracy of the model is 78.78%</p>
</div>
<div id="creating-k-fold-cross-validation" class="section level1">
<h1>Creating k-fold cross validation</h1>
<pre class="r"><code>set.seed(212)
titanic_folds_dt &lt;- vfold_cv(titanic_training, v = 10, strata = survived)
titanic_folds_dt</code></pre>
<pre><code>## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits           id    
##    &lt;list&gt;           &lt;chr&gt; 
##  1 &lt;split [711/80]&gt; Fold01
##  2 &lt;split [712/79]&gt; Fold02
##  3 &lt;split [712/79]&gt; Fold03
##  4 &lt;split [712/79]&gt; Fold04
##  5 &lt;split [712/79]&gt; Fold05
##  6 &lt;split [712/79]&gt; Fold06
##  7 &lt;split [712/79]&gt; Fold07
##  8 &lt;split [712/79]&gt; Fold08
##  9 &lt;split [712/79]&gt; Fold09
## 10 &lt;split [712/79]&gt; Fold10</code></pre>
<p>I re-sampled using 10 times k-fold cross validation.</p>
</div>
<div id="model-training-with-cross-validation" class="section level1">
<h1>Model training with cross validation</h1>
<pre class="r"><code>titanic_rs_fit_dt &lt;-titanic_wkfl_dt %&gt;%
  fit_resamples(resamples = titanic_folds_dt, metrics = titanic_metrics_dt)
titanic_rs_fit_dt %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.789    10  0.0171 Preprocessor1_Model1
## 2 roc_auc  binary     0.810    10  0.0150 Preprocessor1_Model1
## 3 sens     binary     0.904    10  0.0163 Preprocessor1_Model1
## 4 spec     binary     0.628    10  0.0255 Preprocessor1_Model1</code></pre>
<p>When i trained the model using k-fold cross validation, the accuracy of the model is increased to 81% This accuracy is better than normal model.</p>
</div>
<div id="detailed-cross_validation-results" class="section level1">
<h1>Detailed cross_validation results</h1>
<pre class="r"><code>titanic_rs_metrics_dt &lt;-titanic_rs_fit_dt %&gt;%
  collect_metrics(summarize = FALSE)
titanic_rs_metrics_dt </code></pre>
<pre><code>## # A tibble: 40 x 5
##    id     .metric  .estimator .estimate .config             
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
##  1 Fold01 sens     binary         0.957 Preprocessor1_Model1
##  2 Fold01 spec     binary         0.706 Preprocessor1_Model1
##  3 Fold01 accuracy binary         0.85  Preprocessor1_Model1
##  4 Fold01 roc_auc  binary         0.874 Preprocessor1_Model1
##  5 Fold02 sens     binary         0.913 Preprocessor1_Model1
##  6 Fold02 spec     binary         0.727 Preprocessor1_Model1
##  7 Fold02 accuracy binary         0.835 Preprocessor1_Model1
##  8 Fold02 roc_auc  binary         0.837 Preprocessor1_Model1
##  9 Fold03 sens     binary         0.913 Preprocessor1_Model1
## 10 Fold03 spec     binary         0.667 Preprocessor1_Model1
## # ... with 30 more rows</code></pre>
</div>
<div id="summarizing-cross-validation-results" class="section level1">
<h1>Summarizing cross validation results</h1>
<pre class="r"><code>titanic_rs_metrics_dt %&gt;%
  group_by(.metric) %&gt;%
  summarize(min= min(.estimate),
            median = median(.estimate),
            max = max(.estimate),
            sd = sd(.estimate))</code></pre>
<pre><code>##         min    median       max        sd
## 1 0.4545455 0.8131661 0.9782609 0.1158011</code></pre>
</div>
<div id="hyper-parameter-tuning" class="section level1">
<h1>Hyper parameter tuning</h1>
<pre class="r"><code>titanic_dt_tune_model &lt;- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %&gt;%
  set_engine(&#39;rpart&#39;) %&gt;%
  set_mode(&#39;classification&#39;)
titanic_dt_tune_model</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = tune()
##   tree_depth = tune()
##   min_n = tune()
## 
## Computational engine: rpart</code></pre>
<p>I have tuned the model using hyper parameter.</p>
</div>
<div id="creating-tuning-workflow" class="section level1">
<h1>Creating tuning workflow</h1>
<pre class="r"><code>titanic_tune_wkfl &lt;- titanic_wkfl_dt %&gt;%
  update_model(titanic_dt_tune_model)
titanic_tune_wkfl</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: decision_tree()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = tune()
##   tree_depth = tune()
##   min_n = tune()
## 
## Computational engine: rpart</code></pre>
</div>
<div id="identifying-hyperparameters" class="section level1">
<h1>Identifying hyperparameters</h1>
<pre class="r"><code>parameters(titanic_dt_tune_model)</code></pre>
<pre><code>## Collection of 3 parameters for tuning
## 
##       identifier            type    object
##  cost_complexity cost_complexity nparam[+]
##       tree_depth      tree_depth nparam[+]
##            min_n           min_n nparam[+]</code></pre>
</div>
<div id="generating-random-grid" class="section level1">
<h1>Generating random grid</h1>
<pre class="r"><code>set.seed(224)
titanic_dt_grid &lt;- grid_random(parameters(titanic_dt_tune_model), size = 5)
titanic_dt_grid</code></pre>
<pre><code>## # A tibble: 5 x 3
##   cost_complexity tree_depth min_n
##             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;
## 1     0.000608             8    23
## 2     0.0000191            1     7
## 3     0.000000289          7    22
## 4     0.0343              15    22
## 5     0.0000747           12     8</code></pre>
</div>
<div id="hyperparameter-tuning-with-cross-validation" class="section level1">
<h1>Hyperparameter tuning with cross validation</h1>
<pre class="r"><code>titanic_dt_tuning &lt;- titanic_tune_wkfl %&gt;%
  tune_grid(resamples= titanic_folds_dt, grid = titanic_dt_grid, metrics = titanic_metrics_dt)
titanic_dt_tuning</code></pre>
<pre><code>## # Tuning results
## # 10-fold cross-validation using stratification 
## # A tibble: 10 x 4
##    splits           id     .metrics          .notes          
##    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          
##  1 &lt;split [711/80]&gt; Fold01 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  2 &lt;split [712/79]&gt; Fold02 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  3 &lt;split [712/79]&gt; Fold03 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  4 &lt;split [712/79]&gt; Fold04 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  5 &lt;split [712/79]&gt; Fold05 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  6 &lt;split [712/79]&gt; Fold06 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  7 &lt;split [712/79]&gt; Fold07 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  8 &lt;split [712/79]&gt; Fold08 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  9 &lt;split [712/79]&gt; Fold09 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;
## 10 &lt;split [712/79]&gt; Fold10 &lt;tibble [20 x 7]&gt; &lt;tibble [0 x 1]&gt;</code></pre>
</div>
<div id="exploring-tuning-results" class="section level1">
<h1>Exploring tuning results</h1>
<pre class="r"><code>titanic_dt_tuning %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 20 x 9
##    cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err
##              &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1     0.000608             8    23 accuracy binary     0.788    10  0.0194
##  2     0.000608             8    23 roc_auc  binary     0.839    10  0.0173
##  3     0.000608             8    23 sens     binary     0.846    10  0.0196
##  4     0.000608             8    23 spec     binary     0.707    10  0.0301
##  5     0.0000191            1     7 accuracy binary     0.772    10  0.0164
##  6     0.0000191            1     7 roc_auc  binary     0.756    10  0.0172
##  7     0.0000191            1     7 sens     binary     0.854    10  0.0178
##  8     0.0000191            1     7 spec     binary     0.658    10  0.0261
##  9     0.000000289          7    22 accuracy binary     0.772    10  0.0174
## 10     0.000000289          7    22 roc_auc  binary     0.835    10  0.0169
## 11     0.000000289          7    22 sens     binary     0.841    10  0.0169
## 12     0.000000289          7    22 spec     binary     0.676    10  0.0270
## 13     0.0343              15    22 accuracy binary     0.791    10  0.0168
## 14     0.0343              15    22 roc_auc  binary     0.782    10  0.0173
## 15     0.0343              15    22 sens     binary     0.843    10  0.0177
## 16     0.0343              15    22 spec     binary     0.719    10  0.0224
## 17     0.0000747           12     8 accuracy binary     0.761    10  0.0144
## 18     0.0000747           12     8 roc_auc  binary     0.799    10  0.0173
## 19     0.0000747           12     8 sens     binary     0.824    10  0.0187
## 20     0.0000747           12     8 spec     binary     0.674    10  0.0122
## # ... with 1 more variable: .config &lt;chr&gt;</code></pre>
</div>
<div id="detailed-tuning-results" class="section level1">
<h1>Detailed tuning results</h1>
<pre class="r"><code>titanic_dt_tuning %&gt;%
  collect_metrics(summarize = FALSE)</code></pre>
<pre><code>## # A tibble: 200 x 8
##    id     cost_complexity tree_depth min_n .metric  .estimator .estimate .config
##    &lt;chr&gt;            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;  
##  1 Fold01        0.000608          8    23 sens     binary         0.913 Prepro~
##  2 Fold01        0.000608          8    23 spec     binary         0.765 Prepro~
##  3 Fold01        0.000608          8    23 accuracy binary         0.85  Prepro~
##  4 Fold01        0.000608          8    23 roc_auc  binary         0.866 Prepro~
##  5 Fold02        0.000608          8    23 sens     binary         0.848 Prepro~
##  6 Fold02        0.000608          8    23 spec     binary         0.788 Prepro~
##  7 Fold02        0.000608          8    23 accuracy binary         0.823 Prepro~
##  8 Fold02        0.000608          8    23 roc_auc  binary         0.835 Prepro~
##  9 Fold03        0.000608          8    23 sens     binary         0.804 Prepro~
## 10 Fold03        0.000608          8    23 spec     binary         0.788 Prepro~
## # ... with 190 more rows</code></pre>
</div>
<div id="exploring-tuning-results-1" class="section level1">
<h1>Exploring tuning results</h1>
<pre class="r"><code>titanic_dt_tuning %&gt;%
  collect_metrics(summarize = FALSE) %&gt;%
  filter(.metric == &#39;roc_auc&#39;) %&gt;%
  group_by (id) %&gt;%
  summarize( min_roc_auc = min(.estimate),
             median_roc_auc = median(.estimate),
             max_roc_auc = max(.estimate))</code></pre>
<pre><code>##   min_roc_auc median_roc_auc max_roc_auc
## 1   0.6683136      0.8182642   0.8969038</code></pre>
</div>
<div id="viewing-the-best-performing-model" class="section level1">
<h1>Viewing the best performing model</h1>
<pre class="r"><code>titanic_dt_tuning %&gt;%
  show_best(metric = &#39;roc_auc&#39;, n =5)</code></pre>
<pre><code>## # A tibble: 5 x 9
##   cost_complexity tree_depth min_n .metric .estimator  mean     n std_err
##             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
## 1     0.000608             8    23 roc_auc binary     0.839    10  0.0173
## 2     0.000000289          7    22 roc_auc binary     0.835    10  0.0169
## 3     0.0000747           12     8 roc_auc binary     0.799    10  0.0173
## 4     0.0343              15    22 roc_auc binary     0.782    10  0.0173
## 5     0.0000191            1     7 roc_auc binary     0.756    10  0.0172
## # ... with 1 more variable: .config &lt;chr&gt;</code></pre>
<p>Model 1 is the best model</p>
</div>
<div id="selecting-the-best-model" class="section level1">
<h1>Selecting the best model</h1>
<pre class="r"><code>titanic_best_dt_model &lt;- titanic_dt_tuning %&gt;%
  select_best(metric = &#39;roc_auc&#39;)
titanic_best_dt_model </code></pre>
<pre><code>## # A tibble: 1 x 4
##   cost_complexity tree_depth min_n .config             
##             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;               
## 1        0.000608          8    23 Preprocessor1_Model1</code></pre>
<p>The model 1 are best performing model and hyper parameter values</p>
</div>
<div id="finalizing-the-workflow" class="section level1">
<h1>Finalizing the workflow</h1>
<pre class="r"><code>final_titanic_wkfl_dt &lt;- titanic_tune_wkfl %&gt;%
  finalize_workflow(titanic_best_dt_model)
final_titanic_wkfl_dt</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: decision_tree()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = 0.000608424596386352
##   tree_depth = 8
##   min_n = 23
## 
## Computational engine: rpart</code></pre>
</div>
<div id="model-fitting-1" class="section level1">
<h1>Model fitting</h1>
<pre class="r"><code>titanic_final_fit_dt &lt;- final_titanic_wkfl_dt %&gt;%
  last_fit(split = titanic_split)
titanic_final_fit_dt %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.848 Preprocessor1_Model1
## 2 roc_auc  binary         0.884 Preprocessor1_Model1</code></pre>
<p>The accuracy of the model is 76,77 %</p>
<pre class="r"><code>titanic_prediction&lt;- titanic_final_fit_dt %&gt;%
  collect_predictions()
titanic_prediction</code></pre>
<pre><code>## # A tibble: 198 x 7
##    id               .pred_0 .pred_1  .row .pred_class survived .config          
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;            
##  1 train/test split  0.0734  0.927     11 1           1        Preprocessor1_Mo~
##  2 train/test split  0.0734  0.927     12 1           1        Preprocessor1_Mo~
##  3 train/test split  0.605   0.395     14 0           0        Preprocessor1_Mo~
##  4 train/test split  0.0734  0.927     19 1           1        Preprocessor1_Mo~
##  5 train/test split  0.0734  0.927     25 1           1        Preprocessor1_Mo~
##  6 train/test split  0.385   0.615     27 1           1        Preprocessor1_Mo~
##  7 train/test split  0.947   0.0526    28 0           0        Preprocessor1_Mo~
##  8 train/test split  0.0734  0.927     34 1           1        Preprocessor1_Mo~
##  9 train/test split  0.947   0.0526    36 0           0        Preprocessor1_Mo~
## 10 train/test split  0.0734  0.927     38 1           1        Preprocessor1_Mo~
## # ... with 188 more rows</code></pre>
<pre class="r"><code>conf_mat(titanic_prediction, truth = survived, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-82-1.png" width="672" />
# Correct predictions
True negative is 98 people, who did not survive. True positive is 54 people survived.</p>
</div>
<div id="classification-error-1" class="section level1">
<h1>Classification error</h1>
<p>False positive is 17 people, who are predicted as survived but actually not survived. False negative is 29 people, who are predicted as dead but actually survived.</p>
<p>More number of people not survived in Titanic</p>
</div>
<div id="random-forest-model" class="section level1">
<h1>Random forest model</h1>
<p>I used random forest model here</p>
</div>
<div id="model-specification-using-random-forest" class="section level1">
<h1>Model specification using random forest</h1>
<pre class="r"><code>titanic_rf_model&lt;- rand_forest(mtry =4,trees = 100, min_n =10) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)</code></pre>
<p>I have used 100 trees to create a model using random forest</p>
</div>
<div id="training-a-forest" class="section level1">
<h1>Training a forest</h1>
<pre class="r"><code>titanic_fit_rf &lt;- titanic_rf_model %&gt;%
  fit (survived ~ sex+ pclass + age + sibsp + parch + fare + embarked, data = titanic_training)
titanic_fit_rf</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  60ms 
## Ranger result
## 
## Call:
##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), num.trees = ~100, min.node.size = min_rows(~10, x), num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) 
## 
## Type:                             Probability estimation 
## Number of trees:                  100 
## Sample size:                      791 
## Number of independent variables:  7 
## Mtry:                             4 
## Target node size:                 10 
## Variable importance mode:         none 
## Splitrule:                        gini 
## OOB prediction error (Brier s.):  0.150061</code></pre>
<p>I have trained the model using random forest</p>
</div>
<div id="predicting-outcome-variables-1" class="section level1">
<h1>Predicting outcome variables</h1>
<pre class="r"><code>titanic_class_preds_rf &lt;- predict(titanic_fit_rf, new_data = titanic_test, type = &quot;class&quot;)
titanic_class_preds_rf</code></pre>
<pre><code>## # A tibble: 198 x 1
##    .pred_class
##    &lt;fct&gt;      
##  1 1          
##  2 1          
##  3 0          
##  4 1          
##  5 1          
##  6 0          
##  7 0          
##  8 1          
##  9 0          
## 10 1          
## # ... with 188 more rows</code></pre>
<p>The above are predicted outcomes using test set</p>
</div>
<div id="estimated-probabilities-1" class="section level1">
<h1>Estimated probabilities</h1>
<pre class="r"><code>titanic_prob_preds_rf &lt;- predict(titanic_fit_rf, new_data = titanic_test, type = &quot;prob&quot;)
titanic_prob_preds_rf</code></pre>
<pre><code>## # A tibble: 198 x 2
##    .pred_0 .pred_1
##      &lt;dbl&gt;   &lt;dbl&gt;
##  1  0.0741   0.926
##  2  0.0035   0.996
##  3  0.522    0.478
##  4  0.011    0.989
##  5  0.0107   0.989
##  6  0.566    0.434
##  7  0.758    0.242
##  8  0        1    
##  9  0.550    0.450
## 10  0.0604   0.940
## # ... with 188 more rows</code></pre>
<p>The above are the predicted probabilities using test data set.</p>
</div>
<div id="combining-results-1" class="section level1">
<h1>Combining results</h1>
<pre class="r"><code>titanic_results_rf &lt;- titanic_test %&gt;%
  bind_cols(titanic_class_preds_rf, titanic_prob_preds_rf)
titanic_results_rf</code></pre>
<pre><code>##     pclass survived    sex   age sibsp parch     fare    embarked
## 1        1        1 female 18.00     1     0 227.5250   Cherbourg
## 2        1        1 female 24.00     0     0  69.3000   Cherbourg
## 3        1        0   male 24.00     0     1 247.5208   Cherbourg
## 4        1        1 female 47.00     1     1  52.5542 Southampton
## 5        1        1 female 19.00     1     0  91.0792   Cherbourg
## 6        1        1   male 28.00     0     0  26.5500 Southampton
## 7        1        0   male 45.00     0     0  35.5000 Southampton
## 8        1        1 female 22.00     0     1  55.0000 Southampton
## 9        1        0   male 48.00     0     0  50.4958   Cherbourg
## 10       1        1 female 59.00     2     0  51.4792 Southampton
## 11       1        1 female 41.00     0     0 134.5000   Cherbourg
## 12       1        1 female 47.00     1     0  61.1750 Southampton
## 13       1        1 female 33.00     1     0  53.1000 Southampton
## 14       1        1 female 30.00     0     0  86.5000 Southampton
## 15       1        0   male 27.00     1     0 136.7792   Cherbourg
## 16       1        1 female 26.00     1     0 136.7792   Cherbourg
## 17       1        1   male 31.00     1     0  57.0000 Southampton
## 18       1        1 female 48.00     1     0 106.4250   Cherbourg
## 19       1        0   male 47.00     0     0  38.5000 Southampton
## 20       1        1 female 19.00     0     0  30.0000 Southampton
## 21       1        0   male 38.00     0     1 153.4625 Southampton
## 22       1        1 female 49.00     1     0  76.7292   Cherbourg
## 23       1        1 female 24.00     0     0  83.1583   Cherbourg
## 24       1        1 female 52.00     1     1  93.5000 Southampton
## 25       1        1 female 16.00     0     1  57.9792   Cherbourg
## 26       1        0 female 50.00     0     0  28.7125   Cherbourg
## 27       1        0   male 58.00     0     0  29.7000   Cherbourg
## 28       1        0   male 29.00     0     0  30.0000 Southampton
## 29       1        1 female 58.00     0     0 146.5208   Cherbourg
## 30       1        0   male 37.00     0     1  29.7000   Cherbourg
## 31       1        1 female 31.00     1     0 113.2750   Cherbourg
## 32       1        1 female 19.00     0     2  26.2833 Southampton
## 33       1        0   male 18.00     1     0 108.9000   Cherbourg
## 34       1        1   male 52.00     0     0  30.5000 Southampton
## 35       1        1 female 56.00     0     1  83.1583   Cherbourg
## 36       1        0   male 38.00     0     0   0.0000 Southampton
## 37       1        1 female 33.00     0     0  27.7208   Cherbourg
## 38       1        1 female 54.00     1     0  59.4000   Cherbourg
## 39       1        1 female 21.00     2     2 262.3750   Cherbourg
## 40       1        1 female 35.00     1     0  57.7500   Cherbourg
## 41       1        1 female 30.00     0     0  31.0000   Cherbourg
## 42       1        0   male 50.00     1     0  55.9000 Southampton
## 43       1        0   male 56.00     0     0  30.6958   Cherbourg
## 44       1        0   male 24.00     1     0  60.0000 Southampton
## 45       1        1 female 23.00     1     0  82.2667 Southampton
## 46       1        1   male 32.00     0     0  30.5000   Cherbourg
## 47       1        1 female 18.00     0     2  79.6500 Southampton
## 48       1        0   male 52.00     1     1  79.6500 Southampton
## 49       1        1 female 39.00     1     1  79.6500 Southampton
## 50       1        1   male 48.00     1     0  52.0000 Southampton
## 51       1        0   male 40.00     0     0  27.7208   Cherbourg
## 52       1        0   male 54.00     0     1  77.2875 Southampton
## 53       1        0   male 21.00     0     1  77.2875 Southampton
## 54       1        1 female 55.00     0     0 135.6333   Cherbourg
## 55       1        1 female 31.00     0     2 164.8667 Southampton
## 56       2        1 female 19.00     1     0  26.0000 Southampton
## 57       2        1   male 34.00     0     0  13.0000 Southampton
## 58       2        0   male 26.00     0     0  13.0000 Southampton
## 59       2        1 female 24.00     0     0  13.0000 Southampton
## 60       2        1 female 42.00     0     0  13.0000 Southampton
## 61       2        0   male 37.00     1     0  26.0000 Southampton
## 62       2        0   male 29.00     1     0  26.0000 Southampton
## 63       2        1 female 28.00     1     0  26.0000 Southampton
## 64       2        1 female 31.00     1     1  26.2500 Southampton
## 65       2        1 female 24.00     1     0  27.7208   Cherbourg
## 66       2        1 female 18.00     0     1  23.0000 Southampton
## 67       2        0   male 38.00     1     0  21.0000 Southampton
## 68       2        0   male 34.00     1     0  21.0000 Southampton
## 69       2        0   male 26.00     0     0  10.5000 Southampton
## 70       2        0   male 21.00     1     0  11.5000 Southampton
## 71       2        1   male  0.67     1     1  14.5000 Southampton
## 72       2        1 female  7.00     0     2  26.2500 Southampton
## 73       2        1 female 45.00     1     1  26.2500 Southampton
## 74       2        0   male 49.00     1     2  65.0000 Southampton
## 75       2        0   male 24.00     2     0  73.5000 Southampton
## 76       2        0   male 21.00     2     0  73.5000 Southampton
## 77       2        1 female 54.00     1     3  23.0000 Southampton
## 78       2        0   male 44.00     1     0  26.0000 Southampton
## 79       2        1 female 17.00     0     0  10.5000 Southampton
## 80       2        0   male 42.00     1     0  27.0000 Southampton
## 81       2        1 female 23.00     0     0  13.7917   Cherbourg
## 82       2        1 female 24.00     1     0  26.0000 Southampton
## 83       2        0 female 22.00     0     0  21.0000 Southampton
## 84       2        1 female 22.00     1     2  41.5792   Cherbourg
## 85       2        0   male 40.00     0     0  16.0000 Southampton
## 86       2        1 female 41.00     0     1  19.5000 Southampton
## 87       2        0   male 19.00     1     1  36.7500 Southampton
## 88       2        1   male 20.00     0     0  13.8625   Cherbourg
## 89       2        1 female 21.00     0     1  21.0000 Southampton
## 90       2        1   male 30.00     0     0  12.7375   Cherbourg
## 91       2        0   male 27.00     0     0  15.0333   Cherbourg
## 92       2        0   male 23.00     0     0  15.0458   Cherbourg
## 93       2        1 female 50.00     0     0  10.5000 Southampton
## 94       2        0   male 27.00     0     0  26.0000 Southampton
## 95       2        1 female 25.00     0     1  26.0000 Southampton
## 96       2        0   male 25.00     0     0  10.5000 Southampton
## 97       2        0   male 23.00     0     0  13.0000 Southampton
## 98       2        0 female 27.00     1     0  21.0000 Southampton
## 99       2        0   male 40.00     0     0  13.0000 Southampton
## 100      3        0   male 13.00     0     2  20.2500 Southampton
## 101      3        1 female 18.00     0     0   7.2292   Cherbourg
## 102      3        1 female 18.00     0     1   9.3500 Southampton
## 103      3        0   male 20.00     0     0   7.9250 Southampton
## 104      3        0   male 26.00     0     0   7.8958 Southampton
## 105      3        0   male 25.00     1     0  17.8000 Southampton
## 106      3        0 female 45.00     0     1  14.4542   Cherbourg
## 107      3        0   male 22.00     0     0   8.0500 Southampton
## 108      3        0   male 26.00     0     0   7.8958 Southampton
## 109      3        0   male 40.00     1     1  15.5000  Queenstown
## 110      3        0   male 21.00     0     0  16.1000 Southampton
## 111      3        0   male 19.00     0     0   6.7500  Queenstown
## 112      3        0 female 21.00     0     0   8.6625 Southampton
## 113      3        0 female 30.00     0     0   8.6625 Southampton
## 114      3        0   male 17.00     0     0   8.6625 Southampton
## 115      3        0   male 21.00     0     0   8.0500 Southampton
## 116      3        0   male 28.00     0     0   7.2500 Southampton
## 117      3        0   male 24.00     0     0   8.0500 Southampton
## 118      3        0   male 21.00     0     0   7.7333  Queenstown
## 119      3        0   male 26.00     1     0  14.4542   Cherbourg
## 120      3        0   male 36.00     0     0   7.4958 Southampton
## 121      3        0   male 31.00     0     0   7.7500  Queenstown
## 122      3        0   male 17.00     0     0   8.6625 Southampton
## 123      3        1   male 30.00     0     0   9.5000 Southampton
## 124      3        0   male 16.00     0     0   9.5000 Southampton
## 125      3        1   male  1.00     1     2  20.5750 Southampton
## 126      3        1 female  0.17     1     2  20.5750 Southampton
## 127      3        0   male 22.00     0     0   7.2500 Southampton
## 128      3        1   male 19.00     0     0   8.0500 Southampton
## 129      3        0   male 33.00     0     0   7.8958   Cherbourg
## 130      3        1   male 24.00     0     0   7.5500 Southampton
## 131      3        1 female 22.00     1     0  13.9000 Southampton
## 132      3        0   male 47.00     0     0   7.2500 Southampton
## 133      3        1 female  5.00     0     0  12.4750 Southampton
## 134      3        0   male 18.00     0     0   7.7958 Southampton
## 135      3        1 female 16.00     0     0   7.7333  Queenstown
## 136      3        1   male  9.00     0     2  20.5250 Southampton
## 137      3        0   male 38.00     0     0   7.0500 Southampton
## 138      3        0   male 28.00     1     0  15.8500 Southampton
## 139      3        1 female 24.00     1     0  15.8500 Southampton
## 140      3        0   male 21.00     0     0   7.8542 Southampton
## 141      3        0 female 18.00     0     0   6.7500  Queenstown
## 142      3        1 female 22.00     0     0   8.9625 Southampton
## 143      3        0 female 28.00     0     0   7.7750 Southampton
## 144      3        0   male 43.00     0     0   6.4500 Southampton
## 145      3        1 female 27.00     0     0   7.9250 Southampton
## 146      3        0   male 30.00     0     0   7.2292   Cherbourg
## 147      3        0   male 17.00     1     0   7.0542 Southampton
## 148      3        0   male 34.00     0     0   6.4958 Southampton
## 149      3        0   male 33.00     0     0   8.6542 Southampton
## 150      3        0   male 31.00     0     0   7.7750 Southampton
## 151      3        1 female  1.00     1     1  11.1333 Southampton
## 152      3        0   male 49.00     0     0   0.0000 Southampton
## 153      3        0 female 20.00     1     0   9.8250 Southampton
## 154      3        0   male 33.00     0     0   7.8542 Southampton
## 155      3        0   male 22.00     0     0   7.5208 Southampton
## 156      3        0   male 26.00     2     0   8.6625 Southampton
## 157      3        0   male 29.00     0     0   9.4833 Southampton
## 158      3        0   male 29.00     0     0   7.7750 Southampton
## 159      3        0   male 39.00     0     0  24.1500 Southampton
## 160      3        0 female 26.00     1     0  16.1000 Southampton
## 161      3        0   male 22.00     0     0   7.1250 Southampton
## 162      3        0 female 30.50     0     0   7.7500  Queenstown
## 163      3        0   male 21.00     0     0   7.8958 Southampton
## 164      3        0   male 25.00     0     0   7.6500 Southampton
## 165      3        1 female 27.00     0     1  12.4750 Southampton
## 166      3        0   male 18.00     0     0   7.7500 Southampton
## 167      3        0   male 22.00     0     0   7.8958 Southampton
## 168      3        1 female 15.00     0     0   7.2250   Cherbourg
## 169      3        1   male 12.00     1     0  11.2417   Cherbourg
## 170      3        0 female 29.00     0     0   7.9250 Southampton
## 171      3        0   male 61.00     0     0   6.2375 Southampton
## 172      3        0   male 28.00     0     0  22.5250 Southampton
## 173      3        0   male 20.00     0     0   9.2250 Southampton
## 174      3        0   male 20.00     0     0   8.6625 Southampton
## 175      3        0 female  3.00     1     1  13.7750 Southampton
## 176      3        0   male 19.00     0     0   7.8958 Southampton
## 177      3        0 female 18.00     0     0   7.7750 Southampton
## 178      3        0   male 21.00     0     0   8.0500 Southampton
## 179      3        0 female 22.00     0     0  39.6875 Southampton
## 180      3        0 female 47.00     1     0  14.5000 Southampton
## 181      3        0   male 24.00     0     0   9.3250 Southampton
## 182      3        1 female  1.00     1     1  16.7000 Southampton
## 183      3        1   male 29.00     0     0   9.5000 Southampton
## 184      3        0 female 45.00     1     4  27.9000 Southampton
## 185      3        0   male 33.00     0     0   8.6625   Cherbourg
## 186      3        1 female 23.00     0     0   7.5500 Southampton
## 187      3        1   male 31.00     0     0   7.9250 Southampton
## 188      3        1   male 25.00     0     0   0.0000 Southampton
## 189      3        1   male  7.00     1     1  15.2458   Cherbourg
## 190      3        1 female 18.00     0     0   9.8417 Southampton
## 191      3        0   male 11.50     1     1  14.5000 Southampton
## 192      3        0   male 36.00     1     1  24.1500 Southampton
## 193      3        0   male 28.00     0     0   9.5000 Southampton
## 194      3        0   male 16.00     2     0  18.0000 Southampton
## 195      3        0 female 14.00     0     0   7.8542 Southampton
## 196      3        0   male 21.00     0     0   7.2500 Southampton
## 197      3        0   male 26.50     0     0   7.2250   Cherbourg
## 198      3        0   male 27.00     0     0   7.2250   Cherbourg
##     discretized.age discretized.fare .pred_class     .pred_0    .pred_1
## 1           (10,20]        (200,250]           1 0.074063492 0.92593651
## 2           (20,30]         (50,100]           1 0.003500000 0.99650000
## 3           (20,30]        (200,250]           0 0.521892857 0.47810714
## 4           (40,50]         (50,100]           1 0.011000000 0.98900000
## 5           (10,20]         (50,100]           1 0.010722222 0.98927778
## 6           (20,30]           (0,50]           0 0.566413763 0.43358624
## 7           (40,50]           (0,50]           0 0.757715313 0.24228469
## 8           (20,30]         (50,100]           1 0.000000000 1.00000000
## 9           (40,50]         (50,100]           0 0.549540936 0.45045906
## 10          (50,60]         (50,100]           1 0.060428571 0.93957143
## 11          (40,50]        (100,150]           1 0.000000000 1.00000000
## 12          (40,50]         (50,100]           1 0.012500000 0.98750000
## 13          (30,40]         (50,100]           1 0.001000000 0.99900000
## 14          (20,30]         (50,100]           1 0.002222222 0.99777778
## 15          (20,30]        (100,150]           0 0.543949361 0.45605064
## 16          (20,30]        (100,150]           1 0.052309524 0.94769048
## 17          (30,40]         (50,100]           0 0.576635488 0.42336451
## 18          (40,50]        (100,150]           1 0.001111111 0.99888889
## 19          (40,50]           (0,50]           0 0.907791367 0.09220863
## 20          (10,20]           (0,50]           1 0.066535851 0.93346415
## 21          (30,40]        (150,200]           0 0.545215962 0.45478404
## 22          (40,50]         (50,100]           1 0.001111111 0.99888889
## 23          (20,30]         (50,100]           1 0.007944444 0.99205556
## 24          (50,60]         (50,100]           1 0.011611111 0.98838889
## 25          (10,20]         (50,100]           1 0.013000000 0.98700000
## 26          (40,50]           (0,50]           1 0.066071429 0.93392857
## 27          (50,60]           (0,50]           0 0.523798341 0.47620166
## 28          (20,30]           (0,50]           0 0.564444454 0.43555555
## 29          (50,60]        (100,150]           1 0.053436508 0.94656349
## 30          (30,40]           (0,50]           1 0.435568182 0.56443182
## 31          (30,40]        (100,150]           1 0.002666667 0.99733333
## 32          (10,20]           (0,50]           1 0.031869048 0.96813095
## 33          (10,20]        (100,150]           1 0.381044599 0.61895540
## 34          (50,60]           (0,50]           0 0.547327442 0.45267256
## 35          (50,60]         (50,100]           1 0.001111111 0.99888889
## 36          (30,40]             &lt;NA&gt;           0 0.803940846 0.19605915
## 37          (30,40]           (0,50]           1 0.104603175 0.89539683
## 38          (50,60]         (50,100]           1 0.000000000 1.00000000
## 39          (20,30]        (250,300]           1 0.201543651 0.79845635
## 40          (30,40]         (50,100]           1 0.001000000 0.99900000
## 41          (20,30]           (0,50]           1 0.322603175 0.67739683
## 42          (40,50]         (50,100]           1 0.438844813 0.56115519
## 43          (50,60]           (0,50]           1 0.452196419 0.54780358
## 44          (20,30]         (50,100]           0 0.541091270 0.45890873
## 45          (20,30]         (50,100]           1 0.004444444 0.99555556
## 46          (30,40]           (0,50]           1 0.354644527 0.64535547
## 47          (10,20]         (50,100]           1 0.036325397 0.96367460
## 48          (50,60]         (50,100]           1 0.439806122 0.56019388
## 49          (30,40]         (50,100]           1 0.004444444 0.99555556
## 50          (40,50]         (50,100]           0 0.732840845 0.26715916
## 51          (30,40]           (0,50]           1 0.447231828 0.55276817
## 52          (50,60]         (50,100]           0 0.637070497 0.36292950
## 53          (20,30]         (50,100]           1 0.442366522 0.55763348
## 54          (50,60]        (100,150]           1 0.011111111 0.98888889
## 55          (30,40]        (150,200]           1 0.122440476 0.87755952
## 56          (10,20]           (0,50]           1 0.093770563 0.90622944
## 57          (30,40]           (0,50]           0 0.972895890 0.02710411
## 58          (20,30]           (0,50]           0 0.984567600 0.01543240
## 59          (20,30]           (0,50]           1 0.335544095 0.66445591
## 60          (40,50]           (0,50]           1 0.252121656 0.74787834
## 61          (30,40]           (0,50]           0 0.888865690 0.11113431
## 62          (20,30]           (0,50]           0 0.929941087 0.07005891
## 63          (20,30]           (0,50]           1 0.176186924 0.82381308
## 64          (30,40]           (0,50]           1 0.105634921 0.89436508
## 65          (20,30]           (0,50]           1 0.082802309 0.91719769
## 66          (10,20]           (0,50]           1 0.032325397 0.96767460
## 67          (30,40]           (0,50]           0 0.953880952 0.04611905
## 68          (30,40]           (0,50]           0 0.961702381 0.03829762
## 69          (20,30]           (0,50]           0 0.854793953 0.14520605
## 70          (20,30]           (0,50]           0 0.950205706 0.04979429
## 71           (0,10]           (0,50]           1 0.423471579 0.57652842
## 72           (0,10]           (0,50]           1 0.009662698 0.99033730
## 73          (40,50]           (0,50]           1 0.202365079 0.79763492
## 74          (40,50]         (50,100]           0 0.716150794 0.28384921
## 75          (20,30]         (50,100]           0 0.858654762 0.14134524
## 76          (20,30]         (50,100]           0 0.898424603 0.10157540
## 77          (50,60]           (0,50]           1 0.179638889 0.82036111
## 78          (40,50]           (0,50]           0 0.958310134 0.04168987
## 79          (10,20]           (0,50]           1 0.156800144 0.84319986
## 80          (40,50]           (0,50]           0 0.879281746 0.12071825
## 81          (20,30]           (0,50]           1 0.126002525 0.87399747
## 82          (20,30]           (0,50]           1 0.186305972 0.81369403
## 83          (20,30]           (0,50]           1 0.026486652 0.97351335
## 84          (20,30]           (0,50]           1 0.014496032 0.98550397
## 85          (30,40]           (0,50]           0 0.936182540 0.06381746
## 86          (40,50]           (0,50]           1 0.074039683 0.92596032
## 87          (10,20]           (0,50]           0 0.776769841 0.22323016
## 88          (10,20]           (0,50]           0 0.565527544 0.43447246
## 89          (20,30]           (0,50]           1 0.023396825 0.97660317
## 90          (20,30]           (0,50]           0 0.630287440 0.36971256
## 91          (20,30]           (0,50]           0 0.611027544 0.38897246
## 92          (20,30]           (0,50]           0 0.643117288 0.35688271
## 93          (40,50]           (0,50]           1 0.237081890 0.76291811
## 94          (20,30]           (0,50]           0 0.959869963 0.04013004
## 95          (20,30]           (0,50]           1 0.114960317 0.88503968
## 96          (20,30]           (0,50]           0 0.830282048 0.16971795
## 97          (20,30]           (0,50]           0 0.989246172 0.01075383
## 98          (20,30]           (0,50]           1 0.144984848 0.85501515
## 99          (30,40]           (0,50]           0 0.962423668 0.03757633
## 100         (10,20]           (0,50]           0 0.517420635 0.48257937
## 101         (10,20]           (0,50]           1 0.193062468 0.80693753
## 102         (10,20]           (0,50]           0 0.624801587 0.37519841
## 103         (10,20]           (0,50]           0 0.643180380 0.35681962
## 104         (20,30]           (0,50]           0 0.985971726 0.01402827
## 105         (20,30]           (0,50]           0 0.764755952 0.23524405
## 106         (40,50]           (0,50]           1 0.419503968 0.58049603
## 107         (20,30]           (0,50]           0 0.956303483 0.04369652
## 108         (20,30]           (0,50]           0 0.985971726 0.01402827
## 109         (30,40]           (0,50]           0 0.865500000 0.13450000
## 110         (20,30]           (0,50]           0 0.906380786 0.09361921
## 111         (10,20]           (0,50]           0 0.855018531 0.14498147
## 112         (20,30]           (0,50]           0 0.821007576 0.17899242
## 113         (20,30]           (0,50]           0 0.636432171 0.36356783
## 114         (10,20]           (0,50]           0 0.929103210 0.07089679
## 115         (20,30]           (0,50]           0 0.935761622 0.06423838
## 116         (20,30]           (0,50]           0 0.813948452 0.18605155
## 117         (20,30]           (0,50]           0 0.965722676 0.03427732
## 118         (20,30]           (0,50]           0 0.759286849 0.24071315
## 119         (20,30]           (0,50]           0 0.664226190 0.33577381
## 120         (30,40]           (0,50]           0 0.970989899 0.02901010
## 121         (30,40]           (0,50]           0 0.630055164 0.36994484
## 122         (10,20]           (0,50]           0 0.929103210 0.07089679
## 123         (20,30]           (0,50]           0 0.923107943 0.07689206
## 124         (10,20]           (0,50]           0 0.792577782 0.20742222
## 125          (0,10]           (0,50]           1 0.187238095 0.81276190
## 126          (0,10]           (0,50]           1 0.347896825 0.65210317
## 127         (20,30]           (0,50]           0 0.921834956 0.07816504
## 128         (10,20]           (0,50]           0 0.839415403 0.16058460
## 129         (30,40]           (0,50]           0 0.769338702 0.23066130
## 130         (20,30]           (0,50]           0 0.769367737 0.23063226
## 131         (20,30]           (0,50]           0 0.649753968 0.35024603
## 132         (40,50]           (0,50]           0 0.968561328 0.03143867
## 133          (0,10]           (0,50]           1 0.369694444 0.63030556
## 134         (10,20]           (0,50]           0 0.667348203 0.33265180
## 135         (10,20]           (0,50]           1 0.272751140 0.72724886
## 136          (0,10]           (0,50]           1 0.220412698 0.77958730
## 137         (30,40]           (0,50]           0 0.981739899 0.01826010
## 138         (20,30]           (0,50]           0 0.899228175 0.10077183
## 139         (20,30]           (0,50]           0 0.677289683 0.32271032
## 140         (20,30]           (0,50]           0 0.950207737 0.04979226
## 141         (10,20]           (0,50]           1 0.255586278 0.74441372
## 142         (20,30]           (0,50]           0 0.790361217 0.20963878
## 143         (20,30]           (0,50]           0 0.587006544 0.41299346
## 144         (40,50]           (0,50]           0 0.952783550 0.04721645
## 145         (20,30]           (0,50]           0 0.652914913 0.34708509
## 146         (20,30]           (0,50]           0 0.815814893 0.18418511
## 147         (10,20]           (0,50]           0 0.951533705 0.04846630
## 148         (30,40]           (0,50]           0 0.963116883 0.03688312
## 149         (30,40]           (0,50]           0 0.963175980 0.03682402
## 150         (30,40]           (0,50]           0 0.561763454 0.43823655
## 151          (0,10]           (0,50]           0 0.636039683 0.36396032
## 152         (40,50]             &lt;NA&gt;           0 0.851743867 0.14825613
## 153         (10,20]           (0,50]           0 0.918793651 0.08120635
## 154         (30,40]           (0,50]           0 0.871318835 0.12868116
## 155         (20,30]           (0,50]           0 0.876954004 0.12304600
## 156         (20,30]           (0,50]           0 0.883365873 0.11663413
## 157         (20,30]           (0,50]           0 0.923107943 0.07689206
## 158         (20,30]           (0,50]           0 0.582203015 0.41779699
## 159         (30,40]           (0,50]           0 0.902805556 0.09719444
## 160         (20,30]           (0,50]           0 0.631575397 0.36842460
## 161         (20,30]           (0,50]           0 0.855690268 0.14430973
## 162         (30,40]           (0,50]           0 0.714140857 0.28585914
## 163         (20,30]           (0,50]           0 0.961707737 0.03829226
## 164         (20,30]           (0,50]           1 0.484051627 0.51594837
## 165         (20,30]           (0,50]           1 0.404863883 0.59513612
## 166         (10,20]           (0,50]           0 0.904964478 0.09503552
## 167         (20,30]           (0,50]           0 0.983660312 0.01633969
## 168         (10,20]           (0,50]           1 0.111574373 0.88842563
## 169         (10,20]           (0,50]           0 0.553929972 0.44607003
## 170         (20,30]           (0,50]           0 0.701668882 0.29833112
## 171         (60,70]           (0,50]           0 0.883410534 0.11658947
## 172         (20,30]           (0,50]           0 0.961443071 0.03855693
## 173         (10,20]           (0,50]           0 0.916248815 0.08375119
## 174         (10,20]           (0,50]           0 0.906943259 0.09305674
## 175          (0,10]           (0,50]           0 0.580305556 0.41969444
## 176         (10,20]           (0,50]           0 0.960697445 0.03930255
## 177         (10,20]           (0,50]           1 0.255165643 0.74483436
## 178         (20,30]           (0,50]           0 0.935761622 0.06423838
## 179         (20,30]           (0,50]           0 0.684206349 0.31579365
## 180         (40,50]           (0,50]           1 0.444658730 0.55534127
## 181         (20,30]           (0,50]           0 0.945664884 0.05433512
## 182          (0,10]           (0,50]           1 0.382507937 0.61749206
## 183         (20,30]           (0,50]           0 0.923107943 0.07689206
## 184         (40,50]           (0,50]           0 0.753234127 0.24676587
## 185         (30,40]           (0,50]           0 0.743592670 0.25640733
## 186         (20,30]           (0,50]           1 0.176645801 0.82335420
## 187         (30,40]           (0,50]           0 0.810356462 0.18964354
## 188         (20,30]             &lt;NA&gt;           0 0.690957481 0.30904252
## 189          (0,10]           (0,50]           0 0.515103175 0.48489683
## 190         (10,20]           (0,50]           0 0.699246032 0.30075397
## 191         (10,20]           (0,50]           1 0.461559524 0.53844048
## 192         (30,40]           (0,50]           0 0.942103175 0.05789683
## 193         (20,30]           (0,50]           0 0.924357943 0.07564206
## 194         (10,20]           (0,50]           0 0.774845238 0.22515476
## 195         (10,20]           (0,50]           1 0.227423579 0.77257642
## 196         (20,30]           (0,50]           0 0.844878840 0.15512116
## 197         (20,30]           (0,50]           0 0.718434314 0.28156569
## 198         (20,30]           (0,50]           0 0.767698179 0.23230182</code></pre>
<p>I have combined the predicted outcome and probabilities into the test data set.</p>
</div>
<div id="assessing-model-fit-using-confusion-matrix-1" class="section level1">
<h1>Assessing model fit using confusion matrix</h1>
<pre class="r"><code>titanic_results_rf %&gt;%
  conf_mat(truth = survived, estimate = .pred_class) %&gt;%
autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-88-1.png" width="672" />
# Correct predictions of test set
True negative is 98 people, who did not survive. True positive is 60 people survived.</p>
</div>
<div id="classification-error-of-test-set" class="section level1">
<h1>Classification error of test set</h1>
<p>False positive is 17 people, who are predicted as survived but actually not survived. False negative is 23 people, who are predicted as dead but actually survived.</p>
<p>More number of people not survived in Titanic</p>
</div>
<div id="combining-models-and-recipe-1" class="section level1">
<h1>Combining models and recipe</h1>
<pre class="r"><code>titanic_wkfl_rf&lt;- workflow() %&gt;%
  add_model(titanic_rf_model) %&gt;%
  add_recipe(titanic_recipe_dt)
titanic_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 4
##   trees = 100
##   min_n = 10
## 
## Computational engine: ranger</code></pre>
</div>
<div id="model-fitting-with-workflow-1" class="section level1">
<h1>Model fitting with workflow</h1>
<pre class="r"><code>titanic_wkfl_fit_rf &lt;- titanic_wkfl_rf %&gt;%
  last_fit(split = titanic_split)
titanic_wkfl_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.823 Preprocessor1_Model1
## 2 roc_auc  binary         0.883 Preprocessor1_Model1</code></pre>
<p>We can see here the accuracy of the random forest model is 80.30%, which is higher than accuracy of decision tree model.</p>
</div>
<div id="collecting-predictions-1" class="section level1">
<h1>Collecting predictions</h1>
<pre class="r"><code>titanic_wkfl_preds_rf &lt;- titanic_wkfl_fit_rf %&gt;%
  collect_predictions()
titanic_wkfl_preds_rf</code></pre>
<pre><code>## # A tibble: 198 x 7
##    id               .pred_0 .pred_1  .row .pred_class survived .config          
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;            
##  1 train/test split 0.0495    0.951    11 1           1        Preprocessor1_Mo~
##  2 train/test split 0.00649   0.994    12 1           1        Preprocessor1_Mo~
##  3 train/test split 0.397     0.603    14 1           0        Preprocessor1_Mo~
##  4 train/test split 0.00798   0.992    19 1           1        Preprocessor1_Mo~
##  5 train/test split 0.014     0.986    25 1           1        Preprocessor1_Mo~
##  6 train/test split 0.553     0.447    27 0           1        Preprocessor1_Mo~
##  7 train/test split 0.756     0.244    28 0           0        Preprocessor1_Mo~
##  8 train/test split 0.0136    0.986    34 1           1        Preprocessor1_Mo~
##  9 train/test split 0.520     0.480    36 0           0        Preprocessor1_Mo~
## 10 train/test split 0.0911    0.909    38 1           1        Preprocessor1_Mo~
## # ... with 188 more rows</code></pre>
</div>
<div id="confusion-matrix-1" class="section level1">
<h1>Confusion matrix</h1>
<pre class="r"><code>conf_mat(titanic_wkfl_preds_rf, truth = survived, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-92-1.png" width="672" />
# Correct predictions on full model
True negative is 100 people, who did not survive. True positive is 59 people survived.</p>
</div>
<div id="classification-error-on-full-model" class="section level1">
<h1>Classification error on full model</h1>
<p>False positive is 15 people, who are predicted as survived but actually not survived. False negative is 24 people, who are predicted as not survived but actually survived.</p>
<p>More number of people not survived in Titanic</p>
</div>
<div id="exploring-custom-metrics-1" class="section level1">
<h1>Exploring custom metrics</h1>
<pre class="r"><code>titanic_metrics_rf &lt;- metric_set(roc_auc, sens, spec, accuracy)
titanic_wkfl_preds_rf %&gt;%
titanic_metrics_rf(truth = survived, estimate = .pred_class, .pred_1)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 sens     binary         0.870
## 2 spec     binary         0.759
## 3 accuracy binary         0.823
## 4 roc_auc  binary         0.117</code></pre>
</div>
<div id="creating-k-fold-cross-validation-1" class="section level1">
<h1>Creating k-fold cross validation</h1>
<pre class="r"><code>set.seed(222)
titanic_folds_rf &lt;- vfold_cv(titanic_training, v = 10, strata = survived)
titanic_folds_rf</code></pre>
<pre><code>## #  10-fold cross-validation using stratification 
## # A tibble: 10 x 2
##    splits           id    
##    &lt;list&gt;           &lt;chr&gt; 
##  1 &lt;split [711/80]&gt; Fold01
##  2 &lt;split [712/79]&gt; Fold02
##  3 &lt;split [712/79]&gt; Fold03
##  4 &lt;split [712/79]&gt; Fold04
##  5 &lt;split [712/79]&gt; Fold05
##  6 &lt;split [712/79]&gt; Fold06
##  7 &lt;split [712/79]&gt; Fold07
##  8 &lt;split [712/79]&gt; Fold08
##  9 &lt;split [712/79]&gt; Fold09
## 10 &lt;split [712/79]&gt; Fold10</code></pre>
<p>I have used 10 times k-fold cross validation to do re-sampling.</p>
</div>
<div id="model-training-with-cross-validation-1" class="section level1">
<h1>Model training with cross validation</h1>
<pre class="r"><code>titanic_rs_fit_rf &lt;-titanic_wkfl_rf %&gt;%
  fit_resamples(resamples = titanic_folds_rf, metrics = titanic_metrics_rf)
titanic_rs_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.801    10 0.00934 Preprocessor1_Model1
## 2 roc_auc  binary     0.855    10 0.0139  Preprocessor1_Model1
## 3 sens     binary     0.880    10 0.00873 Preprocessor1_Model1
## 4 spec     binary     0.692    10 0.0164  Preprocessor1_Model1</code></pre>
</div>
<div id="detailed-cross_validation-results-1" class="section level1">
<h1>Detailed cross_validation results</h1>
<pre class="r"><code>titanic_rs_metrics_rf &lt;-titanic_rs_fit_rf %&gt;%
  collect_metrics(summarize = FALSE)
titanic_rs_metrics_rf </code></pre>
<pre><code>## # A tibble: 40 x 5
##    id     .metric  .estimator .estimate .config             
##    &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
##  1 Fold01 sens     binary         0.913 Preprocessor1_Model1
##  2 Fold01 spec     binary         0.735 Preprocessor1_Model1
##  3 Fold01 accuracy binary         0.838 Preprocessor1_Model1
##  4 Fold01 roc_auc  binary         0.861 Preprocessor1_Model1
##  5 Fold02 sens     binary         0.891 Preprocessor1_Model1
##  6 Fold02 spec     binary         0.667 Preprocessor1_Model1
##  7 Fold02 accuracy binary         0.797 Preprocessor1_Model1
##  8 Fold02 roc_auc  binary         0.835 Preprocessor1_Model1
##  9 Fold03 sens     binary         0.913 Preprocessor1_Model1
## 10 Fold03 spec     binary         0.788 Preprocessor1_Model1
## # ... with 30 more rows</code></pre>
</div>
<div id="summarizing-cross-validation-results-1" class="section level1">
<h1>Summarizing cross validation results</h1>
<pre class="r"><code>titanic_rs_metrics_rf %&gt;%
  group_by(.metric) %&gt;%
  summarize(min= min(.estimate),
            median = median(.estimate),
            max = max(.estimate),
            sd = sd(.estimate))</code></pre>
<pre><code>##         min    median       max         sd
## 1 0.6060606 0.8290472 0.9189723 0.08259255</code></pre>
</div>
<div id="hyper-parameter-tuning-1" class="section level1">
<h1>Hyper parameter tuning</h1>
<pre class="r"><code>titanic_rf_tune_model &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;%
  set_engine(&#39;ranger&#39;) %&gt;%
  set_mode(&#39;classification&#39;)
titanic_rf_tune_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
</div>
<div id="creating-tuning-workflow-1" class="section level1">
<h1>Creating tuning workflow</h1>
<pre class="r"><code>titanic_tune_wkfl_rf &lt;- titanic_wkfl_rf %&gt;%
  update_model(titanic_rf_tune_model)
titanic_tune_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = tune()
##   trees = tune()
##   min_n = tune()
## 
## Computational engine: ranger</code></pre>
</div>
<div id="identifying-hyperparameters-1" class="section level1">
<h1>Identifying hyperparameters</h1>
<pre class="r"><code>parameters(titanic_rf_tune_model)</code></pre>
<pre><code>## Collection of 3 parameters for tuning
## 
##  identifier  type    object
##        mtry  mtry nparam[?]
##       trees trees nparam[+]
##       min_n min_n nparam[+]
## 
## Model parameters needing finalization:
##    # Randomly Selected Predictors (&#39;mtry&#39;)
## 
## See `?dials::finalize` or `?dials::update.parameters` for more information.</code></pre>
<p>`</p>
</div>
<div id="hyperparameter-tuning-with-cross-validation-1" class="section level1">
<h1>Hyperparameter tuning with cross validation</h1>
<pre class="r"><code>titanic_rf_tuning &lt;- titanic_tune_wkfl_rf %&gt;%
  tune_grid(resamples= titanic_folds_rf,  metrics = titanic_metrics_rf)</code></pre>
<pre><code>## i Creating pre-processing data to finalize unknown parameter: mtry</code></pre>
<pre class="r"><code>titanic_rf_tuning</code></pre>
<pre><code>## # Tuning results
## # 10-fold cross-validation using stratification 
## # A tibble: 10 x 4
##    splits           id     .metrics          .notes          
##    &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          
##  1 &lt;split [711/80]&gt; Fold01 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  2 &lt;split [712/79]&gt; Fold02 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  3 &lt;split [712/79]&gt; Fold03 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  4 &lt;split [712/79]&gt; Fold04 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  5 &lt;split [712/79]&gt; Fold05 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  6 &lt;split [712/79]&gt; Fold06 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  7 &lt;split [712/79]&gt; Fold07 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  8 &lt;split [712/79]&gt; Fold08 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
##  9 &lt;split [712/79]&gt; Fold09 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;
## 10 &lt;split [712/79]&gt; Fold10 &lt;tibble [40 x 7]&gt; &lt;tibble [0 x 1]&gt;</code></pre>
</div>
<div id="exploring-tuning-results-2" class="section level1">
<h1>Exploring tuning results</h1>
<pre class="r"><code>titanic_rf_tuning %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 40 x 9
##     mtry trees min_n .metric  .estimator  mean     n std_err .config            
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;              
##  1     7  1067    18 accuracy binary     0.793    10 0.0123  Preprocessor1_Mode~
##  2     7  1067    18 roc_auc  binary     0.856    10 0.0128  Preprocessor1_Mode~
##  3     7  1067    18 sens     binary     0.865    10 0.0129  Preprocessor1_Mode~
##  4     7  1067    18 spec     binary     0.692    10 0.0196  Preprocessor1_Mode~
##  5     2   385    10 accuracy binary     0.795    10 0.0109  Preprocessor1_Mode~
##  6     2   385    10 roc_auc  binary     0.855    10 0.0113  Preprocessor1_Mode~
##  7     2   385    10 sens     binary     0.887    10 0.0141  Preprocessor1_Mode~
##  8     2   385    10 spec     binary     0.668    10 0.0213  Preprocessor1_Mode~
##  9     2   774    29 accuracy binary     0.795    10 0.00775 Preprocessor1_Mode~
## 10     2   774    29 roc_auc  binary     0.857    10 0.0107  Preprocessor1_Mode~
## # ... with 30 more rows</code></pre>
</div>
<div id="detailed-tuning-results-1" class="section level1">
<h1>Detailed tuning results</h1>
<pre class="r"><code>titanic_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE)</code></pre>
<pre><code>## # A tibble: 400 x 8
##    id      mtry trees min_n .metric  .estimator .estimate .config              
##    &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                
##  1 Fold01     7  1067    18 sens     binary         0.913 Preprocessor1_Model01
##  2 Fold01     7  1067    18 spec     binary         0.706 Preprocessor1_Model01
##  3 Fold01     7  1067    18 accuracy binary         0.825 Preprocessor1_Model01
##  4 Fold01     7  1067    18 roc_auc  binary         0.866 Preprocessor1_Model01
##  5 Fold02     7  1067    18 sens     binary         0.826 Preprocessor1_Model01
##  6 Fold02     7  1067    18 spec     binary         0.667 Preprocessor1_Model01
##  7 Fold02     7  1067    18 accuracy binary         0.759 Preprocessor1_Model01
##  8 Fold02     7  1067    18 roc_auc  binary         0.831 Preprocessor1_Model01
##  9 Fold03     7  1067    18 sens     binary         0.913 Preprocessor1_Model01
## 10 Fold03     7  1067    18 spec     binary         0.818 Preprocessor1_Model01
## # ... with 390 more rows</code></pre>
</div>
<div id="exploring-tuning-results-3" class="section level1">
<h1>Exploring tuning results</h1>
<pre class="r"><code>titanic_rf_tuning %&gt;%
  collect_metrics(summarize = FALSE) %&gt;%
  filter(.metric == &#39;roc_auc&#39;) %&gt;%
  group_by (id) %&gt;%
  summarize( min_roc_auc = min(.estimate),
             median_roc_auc = median(.estimate),
             max_roc_auc = max(.estimate))</code></pre>
<pre><code>##   min_roc_auc median_roc_auc max_roc_auc
## 1   0.7654809       0.859025   0.9262187</code></pre>
</div>
<div id="viewing-the-best-performing-model-1" class="section level1">
<h1>Viewing the best performing model</h1>
<pre class="r"><code>titanic_rf_tuning %&gt;%
  show_best(metric = &#39;roc_auc&#39;, n =5)</code></pre>
<pre><code>## # A tibble: 5 x 9
##    mtry trees min_n .metric .estimator  mean     n std_err .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
## 1     5  1699    35 roc_auc binary     0.862    10  0.0116 Preprocessor1_Model07
## 2     3  1207    37 roc_auc binary     0.860    10  0.0110 Preprocessor1_Model08
## 3     6   985    25 roc_auc binary     0.859    10  0.0122 Preprocessor1_Model04
## 4     2   774    29 roc_auc binary     0.857    10  0.0107 Preprocessor1_Model03
## 5     9  1839    24 roc_auc binary     0.856    10  0.0120 Preprocessor1_Model10</code></pre>
<p>Model 6 is the best model</p>
</div>
<div id="selecting-the-best-model-1" class="section level1">
<h1>Selecting the best model</h1>
<pre class="r"><code>titanic_best_rf_model &lt;- titanic_rf_tuning %&gt;%
  select_best(metric = &#39;roc_auc&#39;)
titanic_best_rf_model </code></pre>
<pre><code>## # A tibble: 1 x 4
##    mtry trees min_n .config              
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1     5  1699    35 Preprocessor1_Model07</code></pre>
<p>The model 6 are best performing model and hyper parameter values</p>
</div>
<div id="finalizing-the-workflow-1" class="section level1">
<h1>Finalizing the workflow</h1>
<pre class="r"><code>final_titanic_wkfl_rf &lt;- titanic_tune_wkfl_rf %&gt;%
  finalize_workflow(titanic_best_rf_model)
final_titanic_wkfl_rf</code></pre>
<pre><code>## == Workflow ====================================================================
## Preprocessor: Recipe
## Model: rand_forest()
## 
## -- Preprocessor ----------------------------------------------------------------
## 3 Recipe Steps
## 
## * step_corr()
## * step_normalize()
## * step_dummy()
## 
## -- Model -----------------------------------------------------------------------
## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   mtry = 5
##   trees = 1699
##   min_n = 35
## 
## Computational engine: ranger</code></pre>
</div>
<div id="model-fitting-2" class="section level1">
<h1>Model fitting</h1>
<pre class="r"><code>titanic_final_fit_rf &lt;- final_titanic_wkfl_rf %&gt;%
  last_fit(split = titanic_split)
titanic_final_fit_rf %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary         0.843 Preprocessor1_Model1
## 2 roc_auc  binary         0.895 Preprocessor1_Model1</code></pre>
<p>We can see that there is some improvement compared to what we got from without tuning model</p>
<p>`</p>
<pre class="r"><code>titanic_prediction_rf&lt;- titanic_final_fit_rf %&gt;%
  collect_predictions()
titanic_prediction_rf</code></pre>
<pre><code>## # A tibble: 198 x 7
##    id               .pred_0 .pred_1  .row .pred_class survived .config          
##    &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;chr&gt;            
##  1 train/test split 0.0649    0.935    11 1           1        Preprocessor1_Mo~
##  2 train/test split 0.00911   0.991    12 1           1        Preprocessor1_Mo~
##  3 train/test split 0.504     0.496    14 0           0        Preprocessor1_Mo~
##  4 train/test split 0.0183    0.982    19 1           1        Preprocessor1_Mo~
##  5 train/test split 0.00983   0.990    25 1           1        Preprocessor1_Mo~
##  6 train/test split 0.594     0.406    27 0           1        Preprocessor1_Mo~
##  7 train/test split 0.772     0.228    28 0           0        Preprocessor1_Mo~
##  8 train/test split 0.00892   0.991    34 1           1        Preprocessor1_Mo~
##  9 train/test split 0.580     0.420    36 0           0        Preprocessor1_Mo~
## 10 train/test split 0.0857    0.914    38 1           1        Preprocessor1_Mo~
## # ... with 188 more rows</code></pre>
<pre class="r"><code>conf_mat(titanic_prediction_rf, truth = survived, estimate = .pred_class) %&gt;%
  autoplot(type = &#39;heatmap&#39;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-110-1.png" width="672" />
# Correct predictions on full model
True negative is 101 people, who did not survive. True positive is 58 people survived.</p>
</div>
<div id="classification-error-on-full-model-1" class="section level1">
<h1>Classification error on full model</h1>
<p>False positive is 14 people, who are predicted as survived but actually not survived. False negative is 25 people, who are predicted as not survived but actually survived.</p>
<p>More number of people not survived in Titanic</p>
</div>
<div id="observations-and-comments" class="section level1">
<h1>Observations and Comments</h1>
<ol style="list-style-type: decimal">
<li><p>The important determinants to predict the survived in Titanic data set are sex, pclass, age and embarked.</p></li>
<li><p>Though the variables sibsp, parch and fare were significant in predicting survived individually, those failed to get significance as a full model.</p></li>
<li><p>The character variables are assigned as a factor</p></li>
<li><p>Cross validation and boot strapping are used for re-sampling technique.</p></li>
<li><p>The accuracy and roc_auc is better in random forest model compared to decision tree model.</p></li>
<li><p>Almost 60% of the people did not survive and approximately 40% of people only survived.</p></li>
<li><p>The first class travelers, female, children less than 10 years old, people aged between 20 and 30 and people embarked from Southampton were the major survivors in Titanic.</p></li>
</ol>
</div>
