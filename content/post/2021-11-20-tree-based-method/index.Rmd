---
title: Tree based method
author: ''
date: '2021-11-20'
slug: tree-based-method
categories: []
tags: []
---
```{r message = FALSE, warning = FALSE, echo = FALSE}

library(ggplot2)
library(dplyr)
library(data.table)
library(tidyr)
library(GGally)
library(gapminder)
library(tidyverse)
library(pastecs)
library(ggpubr)
library(fastDummies)
library(QuantPsyc)
library(MASS)
library(caret)
library(leaps)
library(summarytools)
library(epiDisplay)
library(tidymodels)
library(Rmisc)
```
```{r message = FALSE, warning = FALSE, echo = FALSE, include = FALSE}
bank <- read.csv("D:/Vinoth peronal/UNB/STUDY/Fall 2021/Business analytics/Dev_Website/content/post/2021-11-20-tree-based-method/bank.csv")
```
# Structure of the data set
```{r}
str(bank)
```
# Descriptive statistics of numerical independent values

```{r}
descr(bank)
```
# Summary of dataset
```{r}
summary(bank)
```
# Data table of data set

```{r}
data.table(bank)
```
## Univariate analysis on numerical data

# Univariate analysis on age

```{r}
descr(bank$age)
```
The minimum age of the customer is 19 and maximum age is 87. There is a standard deviation of 10.58, which is high in numbers. Let me dwar a box plot to find out the outliers so that data can be streamlined and standard deviation can be reduced

# Box plot of age and its outliers

```{r}
age_outliers<-boxplot.stats(bank$age)$out
boxplot(bank$age, main = "Box plot of age", boxwex = 0.6)
mtext(paste("Outliers: ", paste(age_outliers, collapse =", ")), cex = 0.6)
age_outliers
```

We can see the outliers of the age lie between 74 and 87.

# Minimum value of age outliers

```{r}
min(age_outliers)
```
# Maximum value of age outliers

```{r}
max(age_outliers)
```
I used subset function to add the age less than 74 so that outliers can be removed

```{r}
bank<- subset(bank, age < 74)
```

# Boxplot of age after removal of outliers

```{r}
age_withoutoutliers<-boxplot.stats(bank$age)$out
boxplot(bank$age, main = "Box plot of age after reomval of outliers", boxwex = 0.6)
mtext(paste("Outliers: ", paste(age_withoutoutliers, collapse =", ")), cex = 0.6)
age_withoutoutliers
```


# Univariate analysis on balance

```{r}
descr(bank$balance)
```
The minimum balance of the customer is -3313, which is a negative balance because of non maintenance charges or some other charges. The maximum balance is 71188. The standard deviation is 2987 and median is 440.

# Box plot of balance and its outliers

```{r}
balance_outliers<-boxplot.stats(bank$balance)$out
boxplot(bank$balance, main = "Box plot of balance", boxwex = 0.6)
mtext(paste("Outliers: ", paste(balance_outliers, collapse =", ")), cex = 0.6)
balance_outliers
```
We can see more outliers in balance. The outliers ranges between -3313 and 0 and between 267 and 71188


# Minimum negative value of balance outlier

```{r}
min(balance_outliers)
```
# Minimum positive value of balance outlier
```{r}
which.min(balance_outliers>0)
```
# Maximum value of balance outlier

```{r}
max(balance_outliers)
```
The outliers are removed by choosing the value between 0 and 273
```{r}
bank<- subset(bank, balance > 0 & balance < 273)
```

# Boxplot of balance after removal of outliers
```{r}
balance_withoutoutliers<-boxplot.stats(bank$balance)$out
boxplot(bank$balance, main = "Box plot of balance after removal of outliers", boxwex = 0.6)
mtext(paste("Outliers: ", paste(balance_withoutoutliers, collapse =", ")), cex = 0.6)
balance_withoutoutliers
```


# Unvariate analysis on day

```{r}
descr(bank$day)
```

The minimum contact day is 1 and maximum is 31. There is a standard deviation of 8.41


# Boxplot of day after removal of outliers

```{r}
day_outliers<-boxplot.stats(bank$day)$out
boxplot(bank$day, main = "Box plot of day", boxwex = 0.6)
mtext(paste("Outliers: ", paste(day_outliers, collapse =", ")), cex = 0.6)
day_outliers
```
There are no outliers in day


# Univariate analysis on duration

```{r}
descr(bank$duration)
```

The minimum contact duration was 5 seconds and maximum duration was 2456 seconds. There is a standard deviation of 234.81 seconds

# Box plot of duration and its outliers

```{r}
duration_outliers<-boxplot.stats(bank$duration)$out
boxplot(bank$duration, main = "Box plot of duration", boxwex = 0.6)
mtext(paste("Outliers: ", paste(duration_outliers, collapse =", ")), cex = 0.6)
duration_outliers
```
There are some outliers in duration, ranging between 597 and 2456 seconds


# Minimum value of duration outlier

```{r}
min(duration_outliers)
```
# Maximum value of duration outlier

```{r}
max(duration_outliers)
```

The outliers are removed greater then 597 using subset
```{r}
bank<- subset(bank, duration < 597)
```

# Boxplot of duration after removal of outliers

```{r}
duration_withoutoutliers<-boxplot.stats(bank$duration)$out
boxplot(bank$duration, main = "Box plot of duration after outliers are removed", boxwex = 0.6)
mtext(paste("Outliers: ", paste(duration_withoutoutliers, collapse =", ")), cex = 0.6)
duration_withoutoutliers
```



# UNivariate analysis on campaign

```{r}
descr(bank$campaign)
```
# Box plot of Campaign

```{r}
campaign_outliers<-boxplot.stats(bank$campaign)$out
boxplot(bank$campaign, main = "Box plot of campaign", boxwex = 0.6)
mtext(paste("Outliers: ", paste(campaign_outliers, collapse =", ")), cex = 0.6)
campaign_outliers
```
There are many outliers in campaign ranging between 7 and 31

# Minimum of campaign outliers

```{r}
min(campaign_outliers)
```
# Maximum of campaign outliers

```{r}
max(campaign_outliers)
```
The outliers are removed and data are taken less than 7

```{r}
bank<- subset(bank, campaign < 7)
```

# Box plot of campaign after removal of outliers

```{r}
campaign_withoutoutliers<-boxplot.stats(bank$campaign)$out
boxplot(bank$campaign, main = "Box plot of campaign after removal of outliers", boxwex = 0.6)
mtext(paste("Outliers: ", paste(campaign_withoutoutliers, collapse =", ")), cex = 0.6)
campaign_withoutoutliers
```

# Box plot of pdays

```{r}
pdays_outliers<-boxplot.stats(bank$pdays)$out
boxplot(bank$pdays, main = "Box plot of pdays", boxwex = 0.6)
mtext(paste("Outliers: ", paste(pdays_outliers, collapse =", ")), cex = 0.6)
pdays_outliers
```
It is surprising to see that whole data are outliers.

# Minimum of pdays outliers

```{r}
min(pdays_outliers)
```
# Maximum of pdays outliers

```{r}
max(pdays_outliers)
```
The whole coloum pdays is removed from dataset because of outliers high in numbers

```{r}
bank<- subset(bank, select = -pdays)
```



# Box plot of previous

```{r}
previous_outliers<-boxplot.stats(bank$previous)$out
boxplot(bank$previous, main = "Box plot of previous", boxwex = 0.6)
mtext(paste("Outliers: ", paste(previous_outliers, collapse =", ")), cex = 0.6)
previous_outliers
```
Even the previous data has full outliers on data. Thus, I removed it from the dataset

# Minimum of previous outliers

```{r}
min(previous_outliers)
```
# Maximum outliers

```{r}
max(previous_outliers)
```
The whole column of previous is removed from the bank dataset due to more outliers

```{r}
bank<- subset(bank, select = -previous)
```


## Univariate analysis on categorical value

# Frequency distribution of job


```{r}
tab1(bank$job, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of job", xlab ="Job")
```
The frequency distribution of unknown, student, unemployed, housemaid, retired, entrepreneur, self-employed, services, admin are less in numbers. Thus they are are clubbed together as others so as to reduce the number of variables and improve the prediction accuracy

```{r}
bank$job<-as.character(bank$job)
bank$job[bank$job %in% c("unknown", "student", "housemaid", "unemployed", "entrepreneur", "self-employed", "retired", "services", "admin.")]<- "Others"
bank$job<-as.factor(bank$job)
```

# The frequency distribution after clubing together

```{r}
freq_age_updated <-tab1(bank$job, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of job after merging", xlab ="Job")
freq_age_updated
```



# Frequency distribution of marital

```{r}
tab1(bank$marital, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of marital", xlab ="Marital")
```
The frequency distribution of all items are greater than 10 percent. So, we are not doing any changes

# frequency distribution of Education

```{r}
tab1(bank$education, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Education", xlab ="Education")
```
The frequency distribution of unknown in education is less than5 percent. Thus, they are merged with primary.


```{r}
bank$education<-as.character(bank$education)
bank$education[bank$education %in% c("unknown", "primary")]<- "primary and others"
bank$education<-as.factor(bank$education)

```

# Frequency distribution of education after merging

```{r}
freq_education_updated<- tab1(bank$education, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Education after merging", xlab ="Education")
freq_education_updated
```


# Frequency distribution of Contact

```{r}
tab1(bank$contact, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Contact", xlab ="Contact")
```
The frequency of the telephone is only 5.5%, Thus, I add those values with unknowna and renamed it as telephone and others

```{r}
bank$contact<-as.character(bank$contact)
bank$contact[bank$contact %in% c("telephone", "unknown")]<- "telephone and others"
bank$contact<-as.factor(bank$contact)
```

# Frequency distribution of contact after merging

```{r}
freq_contact_updated <- tab1(bank$contact, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Contact after merging", xlab ="Contact")
freq_contact_updated
```


# Frequency distribution of poutcome

```{r}
tab1(bank$poutcome, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of poutcome", xlab ="poutcome")
```
The frequency distribution of success and others are less in percentage. So, I combined both into one category and renamed it as success and others

```{r}
bank$poutcome<-as.character(bank$poutcome)
bank$poutcome[bank$poutcome %in% c("success", "other")]<- "success and others"
bank$poutcome<-as.factor(bank$poutcome)
```

# Frequenct distribution of poutcome after merging

```{r}
freq_poutcome_updated<-tab1(bank$poutcome, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of poutcome after merging", xlab ="poutcome")
freq_poutcome_updated
```


considering the below as a Binary variables and binary numbers o and 1 are assigned

Assigning binary numbers to default

```{r}
bank$default[bank$default == "no" ] <-"0"
```

```{r}
bank$default[bank$default == "yes" ] <-"1"
```

Assigning binary numbers to Housing

```{r}
bank$housing[bank$housing == "no" ] <-"0"
bank$housing[bank$housing == "yes" ] <-"1"
```

Assigning binary numbers to loan

```{r}
bank$loan[bank$loan == "no" ] <-"0"
bank$loan[bank$loan == "yes" ] <-"1"
```

Assigning binary numbers to y

```{r}
bank$y[bank$y == "no" ] <-"0"
bank$y[bank$y == "yes" ] <-"1"
```


COnsidering the month as a ordinal categorical variable, I assign the numbers in a sequantial order

```{r}
bank$month[bank$month == "jan" ] <-"1"
bank$month[bank$month == "feb" ] <-"2"
bank$month[bank$month == "mar" ] <-"3"
bank$month[bank$month == "apr" ] <-"4"
bank$month[bank$month == "may" ] <-"5"
bank$month[bank$month == "jun" ] <-"6"
bank$month[bank$month == "jul" ] <-"7"
bank$month[bank$month == "aug" ] <-"8"
bank$month[bank$month == "sep" ] <-"9"
bank$month[bank$month == "oct" ] <-"10"
bank$month[bank$month == "nov" ] <-"11"
bank$month[bank$month == "dec" ] <-"12"
```

Converting binary and ordinal categorical variable to numeric to perform univariate analysis as a numerical variable

```{r}
bank$default <- as.numeric (bank$default)
bank$housing <- as.numeric (bank$housing)
bank$loan <- as.numeric (bank$loan)
bank$y <- as.numeric (bank$y)
bank$month <- as.numeric (bank$month)

```


## Univariate analysis on the binary variables

# Frequency distribution of Default


```{r}
tab1(bank$default, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Default", xlab ="Default")
```
There are only 2 percentage of customers did default on loan and the rest 98 percent of customers are not defaulters

# Frequency distribution of housing

```{r}
tab1(bank$housing, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Housing", xlab ="Housing")
```
The customers who have taken housing loan are 53.8 percentage and the other 46.2 percentage of customers did not take any housing loan


# Frequency distribution of Loan

```{r}
tab1(bank$loan, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Loan", xlab ="Loan")
```
Only 16.7 % of customer took a personal loan and the remaining 83.3 % of customer did not avail any personal loan


# Frequency distribution of y


```{r}
tab1(bank$y, sort.group = "increasing", cum.percent = TRUE, main = "Frequency distribution of Y", xlab ="Y")
```
The majority of the customer, around 93.2 %, said that they would not subscribe to term deposit. Only 6.8 % of customers confirmed that they would subscribe it.


# Box plot of Month


```{r}
month_outliers<-boxplot.stats(bank$month)$out
boxplot(bank$month, main = "Box plot of Month", boxwex = 0.6)
mtext(paste("Outliers: ", paste(month_outliers, collapse =", ")), cex = 0.6)
month_outliers
```



## Bivariate analysis

Let us see the relationship between indendent and dependent variable


# Relationship between y and age

# Correlation test
```{r}
cor.test(bank$y, bank$age)
```
There is a negative correlation between y and age. It means more the age, less the people will subscribe to fixed deposit and less the age of the customer and there is a more likely that they will subscribe to term deposit. The correlation coefficient is -0.051

# Logistic regression on y and age

```{r}
age_y = glm(y~ age, data = bank, family = "binomial")
summary(age_y)
```
The above equation is not significant

# Jitter and box plot of y and age

```{r}
bank %>%
  ggplot(aes(x= age, y= "subscribe or not", color = age))+geom_jitter()+geom_boxplot()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and age")
```

# Relationship between y and job

Logistic regression on y and job

```{r}
job_y = glm(y~ job, data = bank, family = "binomial")
summary(job_y)
```
The above equation is not good enough to predict the y by job as some of the coefficients are not significant

# Bar chart of y and job

```{r}
bank %>%
ggplot(aes(x= job, y= "subscribe or not", color = job))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and job")+coord_flip()
```

## Relationship between y and marital

Logistic regression on y and marital

```{r}
marital_y = glm(y~ marital, data = bank, family = "binomial")
summary(marital_y)
```
the above equation is not good as well because of all the coefficients are not significant

# Bar chart of y and marital

```{r}
bank %>%
ggplot(aes(x= marital, y= "subscribe or not", color = marital))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and marital")
```

# Relationship between y and Education

Logistic regression on y and education

```{r}
education_y = glm(y~ education, data = bank, family = "binomial")
summary(education_y)
```
The intercept is only significant but all the coefficients are not significant. Thus, the above equation is not good to predict the y

# Bar chart of y and education

```{r}
bank %>%
ggplot(aes(x= education, y= "subscribe or not", color =education))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and education")
```


# Relationship between y and default

Correlation between y and default

```{r}
cor.test(bank$y, bank$default)
```
There is some correlation between y and default. The correlation coefficient is 0.056

# Logistic regression on y and default

```{r}
default_y = glm(y~ default, data = bank, family = "binomial")
summary(default_y)
```
Though there is some correlation between y and default, the coefficient of the above equation is not significant.

# Bar chart of y and default

```{r}
bank %>%
ggplot(aes(x= default, y= "subscribe or not", colour= default))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial", maxit =100))+labs(title ="Relationship between y and default")
```

# Relationship between y and balance

```{r}
cor.test(bank$y, bank$balance)
```
There is some positive correlation between y and balance. The correlation coefficient is 0.016

# Logistice regression on y and balance

```{r}
balance_y = glm(y~ balance, data = bank, family = "binomial")
summary(balance_y)
```
Even though there is a positive correlation between y and balance, that above equation is not good as the coefficient is not significant

# JItter and box plot of y and balance

```{r}
bank %>%
ggplot(aes(x= balance, y= "subscribe or not", color =balance))+geom_jitter()+geom_boxplot()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and balance")
```


## Relationship between y and housing

Correlation test between y and housing

```{r}
cor.test(bank$y, bank$housing)
```
There is a negative correlation between y and housing. The correlation coefficient is -0.083

# Logistic regression of y and housing

```{r}
housing_y = glm(y~ housing, data = bank, family = "binomial")
summary(housing_y)
```
The above equation is significant to predict y but it is a negative correlation. If the customer takes more housing loan, it is less likely that he will subscribe to term  deposit because he would have already a commitment to oay a monthly EMI. On the contrary, if the customer does not have any housing loan, it is more likely that he will subscribe to term deposit

# Bar chart of y and housing

```{r}
bank %>%
ggplot(aes(x= housing, y= "subscribe or not", colour = housing))+geom_col()+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial", maxit =100))+labs(title ="Relationship between y and housing")
```
In the above chart, o represents no housing loan and 1 represents customer having housing loan. 


# Relationship between y and loan

Correlation test of y and loan

```{r}
cor.test(bank$y, bank$loan)
```
There is a negative correlation between y and loan. It is just like the case of housing loan. When the customers have an obligation towards personal loan, they will not have the sufficients funds to subscribe to term deposit. That is the reason, there is a negative correlation. When the customer does not have personal loan, there is a likely chance that they will subscribe to term deposit.

# Logistic regression of y and loan
```{r}
loan_y = glm(y~ loan, data = bank, family = "binomial")
summary(loan_y)
```
The above equation is to significant to predict the y. The AIC is 457.62



# Bar chart of y and loan
```{r}
bank %>%
ggplot(aes(x= loan, y= "subscribe or not", colour =loan))+geom_col()+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial", maxit =100))+labs(title ="Relationship between y and loan")
```

# Relationship between y and contact

Logistic regression of y and contact

```{r}
contact_y = glm(y~ contact, data = bank, family = "binomial")
summary(contact_y)
```
The above equation is significant to predict y. The AIC is 443.3

# Bar chart of y and contact
```{r}
bank %>%
ggplot(aes(x= contact, y= "subscribe or not", colour =contact))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and contact")
```

# Relationship between y and day

Correlation test of y and day

```{r}
cor.test(bank$y, bank$day)
```
There is a positive correaltion between y and day

# Logistic regression of y and day
```{r}
day_y = glm(y~ day, data = bank, family = "binomial")
summary(day_y)
```
Though there is a correlation between y and day, the above equation is not significant.

# Jitter and box plot of y and day

```{r}
bank %>%
ggplot(aes(x= day, y= "subscribe or not", color =day))+geom_jitter()+geom_boxplot()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and day")
```

# Relationship between y and month

Correlation between y and month

```{r}
cor.test(bank$y, bank$month)
```
There is a negative correlation. The correlation coefficient is - 0.004

# Logistic regression on y and month

```{r}
month_y = glm(y~ month, data = bank, family = "binomial")
summary(month_y)
```
The above equation is not good as the coefficient is not significant

# Bar chart of y and month

```{r}
bank %>%
ggplot(aes(x= month, y= "subscribe or not", color =month))+geom_col()+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial",maxit =100))+labs(title ="Relationship between y and month")
```


## Relationship between y and duration

Correlation test of y and duration

```{r}
cor.test(bank$y, bank$duration)
```
There is a positive correlation between y and duration. The correlation coefficient is 0.23

# Logistic regression of y and duration
```{r}
duration_y = glm(y~ duration, data = bank, family = "binomial")
summary(duration_y)
```
The above equation is significant to predict y. The AIC is 423.13

# Box plot of y and duration

```{r}
bank %>%
ggplot(aes(x= duration, y= "subscribe or not", color =duration))+geom_boxplot()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and duration")
```

# Relationship between y and campaign

Correlation test between y and campaign

```{r}
cor.test(bank$y, bank$campaign)
```
It is surprising to see that there is a negative correlation between y and campaign. Generally, more the campaign, more should be a new subscription of term deposit but it is opposite here


# Logistic regression between y and campaign
```{r}
campaign_y = glm(y~ campaign, data = bank, family = "binomial")
summary(campaign_y)
```
The above equation is significant to predict the y because both the intercept and coefficient are significant. The AIC is 458.53

# box plot of Campaign

```{r}
bank %>%
ggplot(aes(x= campaign, y= "subscribe or not", color =campaign))+geom_boxplot()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and campaign")
```




# Relationship between y and poutcome

Logistic regression of y and poutcome

```{r}
poutcome_y = glm(y~ poutcome, data = bank, family = "binomial")
summary(poutcome_y)
```
In the above equation, one of the coefficient is not significant. We can take this in to our model since unknow result are more compared to known factors such as success or failure. There is a great chance to convert those customers in to subscribe the term deposit

# Bar chart of y and poutcome

```{r}
bank %>%
ggplot(aes(x= poutcome, y= "subscribe or not", color =poutcome))+geom_col()+expand_limits(y=0)+geom_smooth(method ="glm", se =FALSE, method.args = list(family = "binomial"))+labs(title ="Relationship between y and poutcome")
```

First, I would use tidymodels to build a logistic regression. In order to do, I have to convert the following as a character variable

```{r}
bank$default <- as.character (bank$default)
bank$housing <-  as.character (bank$housing)
bank$loan <-  as.character (bank$loan)
bank$y <-  as.character (bank$y)
bank$month <-  as.character (bank$month)
```
Re-assigning character 

```{r}
bank$y[bank$y == "0" ] <-"no"
bank$y[bank$y == "1" ] <-"yes"
```

# Data resampling
I have split the bank data in to trainig and test data with a split of 75:25
```{r}
bank_split<- initial_split(bank, prop = 0.75, strata = y)
bank_training <- bank_split %>%
  training()
bank_test<- bank_split %>%
  testing()
```

# Checking number of rows in training and test data

```{r}
nrow(bank_training)
nrow(bank_test)
```
# Checking multicollinearity between numerical values in a training bank data set

```{r}
bank_training %>%
  select_if(is.numeric) %>%
  cor()
```
We can notice that there is no much corellation the independent variable


# Model specification
I specify the decision tree based model here
```{r}
dt_model<- decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')
```

# Feature engineering

This is step to do pre-processing of data
```{r}
bank_recipe_dt <- recipe(y ~., data = bank_training) %>%
  step_corr(all_numeric(), threshold =0.8) %>%
  step_normalize(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes())
bank_recipe_dt
```
I have given instruction above in the model that to check the correlation for all numeric variables and keep the thresholding limit as 0.8. I have also given instruction in the model to normalize all the numeric variables and set dummy to all nominal or character variables except the outcome variables because outcome is in factor

# Combining models and recipe
```{r}
bank_wkfl_dt<- workflow() %>%
  add_model(dt_model) %>%
  add_recipe(bank_recipe_dt)
bank_wkfl_dt
```
# Model fitting with workflows
```{r}
bank_wkfl_fit_dt <- bank_wkfl_dt %>%
  last_fit(split = bank_split)
bank_wkfl_fit_dt %>%
  collect_metrics()
```

# Collecting Predictions

```{r}
bank_wkfl_preds_dt <- bank_wkfl_fit_dt %>%
  collect_predictions()
bank_wkfl_preds_dt
```
# Confusion matrix
```{r}
conf_mat(bank_wkfl_preds_dt, truth = y, estimate = .pred_class) %>%
  autoplot(type = 'heatmap')
```
# Correct predictions
True negative is 210, which is the customer did not subscribe to term deposit True positive is 4, which is the customers subscribe to term deposit

# Classification error
False negative is 7, which is the customer did subscribe to term deposit but wrongly predicted as not subscribed. False positive is 15, which is the customer not subscribed to term deposit but wrongly predicted as subscribed

It is clear that the customers will not subscribe to the term deposit

# Exploring custom metrics
```{r}
bank_metrics_dt <- metric_set(roc_auc, sens, spec)
bank_wkfl_preds_dt %>%
 bank_metrics_dt(truth = y, estimate = .pred_class, .pred_yes)
```
# Creating k-fold cross validation
```{r}
set.seed(212)
bank_folds_dt <- vfold_cv(bank_training, v = 10, strata = y)
bank_folds_dt
```
# Model training with cross validation
```{r}
bank_rs_fit_dt <-bank_wkfl_dt %>%
  fit_resamples(resamples = bank_folds_dt, metrics = bank_metrics_dt)
bank_rs_fit_dt %>%
  collect_metrics()
```
# Detailed cross validation results

```{r}
rs_metrics_dt <- bank_rs_fit_dt %>%
  collect_metrics(summarize = FALSE)
rs_metrics_dt
```
# Summarizing cross validation results
```{r}
rs_metrics_dt %>%
  group_by(.metric) %>%
  summarize(min= min(.estimate),
            median = median(.estimate),
            max = max(.estimate),
            sd = sd(.estimate))

```

# Hyper parameter tuning
```{r}
dt_tune_model <- decision_tree(cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')
dt_tune_model
```
# Creating tuning workflow
```{r}
bank_tune_wkfl <- bank_wkfl_dt %>%
  update_model(dt_tune_model)
bank_tune_wkfl
```
# Identifying hyperparameters
```{r}
parameters(dt_tune_model )
```

# Generating random grid
```{r}
set.seed(224)
dt_grid <- grid_random(parameters(dt_tune_model), size = 5)
dt_grid
```

# Hyperparameter tuning with cross validation
```{r}
dt_tuning <- bank_tune_wkfl %>%
  tune_grid(resamples= bank_folds_dt, grid = dt_grid, metrics = bank_metrics_dt)
dt_tuning
```
# Exploring tuning results
```{r}
dt_tuning %>%
  collect_metrics()
```
# Detailed tuning results
```{r}
dt_tuning %>%
  collect_metrics(summarize = FALSE)
```

# Exploring tuning results
```{r}
dt_tuning %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == 'roc_auc') %>%
  group_by (id) %>%
  summarize( min_roc_auc = min(.estimate),
             median_roc_auc = median(.estimate),
             max_roc_auc = max(.estimate))
```

# Viewing the best performing model
```{r}
dt_tuning %>%
  show_best(metric = 'roc_auc', n =5)
```
Model 5 is the best model

# Selecting the best model
```{r}
best_dt_model <- dt_tuning %>%
  select_best(metric = 'roc_auc')
best_dt_model
```
The above are best performing model and hyper parameter values

# Finalizing the workflow
```{r}
final_bank_wkfl_dt <- bank_tune_wkfl %>%
  finalize_workflow(best_dt_model)
final_bank_wkfl_dt
```
# Model fitting
```{r}
bank_final_fit_dt <- final_bank_wkfl_dt %>%
  last_fit(split = bank_split)
bank_final_fit_dt %>%
  collect_metrics()
```

